{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyelq","title":"pyELQ","text":"<p>This repository contains the Python Emission Localization and Quantification software we call pyELQ. It is code used for gas dispersion modelling, in particular methane emissions detection, localization and quantification.</p>"},{"location":"#background","title":"Background","text":"<p>The python Emission Localization and Quantification (pyELQ) code aims to maximize effective use of existing measurement data, especially from continuous monitoring solutions. The code has been developed to detect, localize, and quantify methane emissions from concentration and wind measurements. It can be used in combination with point or beam sensors that are placed strategically on an area of interest.</p> <p>The algorithms in the pyELQ code are based a Bayesian statistics framework. pyELQ can ingest long-term concentration and wind data, and it performs an inversion to predict the likely strengths and locations of persistent methane sources. The goal is to arrive at a plausible estimate of methane emissions from an area of interest that matches the measured data. The predictions from pyELQ come with uncertainty ranges that are representative of probability density functions sampled by a Markov Chain Monte Carlo method. Time series of varying length can be processed by pyELQ: in general, the Bayesian inversion leads to a more constrained solution if more high-precision measurement data is available. We have tested our code under controlled conditions as well as in operating oil and gas facilities.</p> <p>The information on the strength and the approximate location of methane emission sources provided by pyELQ can help operators with more efficient identification and quantification of (unexpected) methane sources, in order to start appropriate mitigating actions accordingly. The pyELQ code is being made available in an open-source environment, to support various assets in their quest to reduce methane emissions.</p> <p>Use cases where the pyELQ code has been applied are described in the following papers:</p> <ul> <li> <p>IJzermans, R., Jones, M., Weidmann, D. et al. \"Long-term continuous monitoring of methane emissions at an oil and gas facility using a multi-open-path laser dispersion spectrometer.\" Sci Rep 14, 623 (2024). (https://doi.org/10.1038/s41598-023-50081-9)</p> </li> <li> <p>Weidmann, D., Hirst, B. et al. \"Locating and Quantifying Methane Emissions by Inverse Analysis of Path-Integrated Concentration Data Using a Markov-Chain Monte Carlo Approach.\" ACS Earth and Space Chemistry 2022 6 (9), 2190-2198  (https://doi.org/10.1021/acsearthspacechem.2c00093)</p> </li> </ul>"},{"location":"#deployment-design","title":"Deployment design","text":"<p>The pyELQ code needs high-quality methane concentration and wind data to be able to provide reliable output on location and quantification of methane emission sources. This requires methane concentration sensors of sufficiently high precision in a layout that allows the detection of relevant methane emission sources, in combination with wind measurements of high enough frequency and accuracy. The optimal sensor layout typically depends on the prevailing meteorological conditions at the site of interest and requires multiple concentration sensors to cover the site under different wind directions.</p>"},{"location":"#pyelq-data-interpretation","title":"pyELQ data interpretation","text":"<p>The results from pyELQ come with uncertainty ranges that are representative of probability density functions sampled by a Markov Chain Monte Carlo method. One should take these uncertainty ranges into account when interpreting the pyELQ output data. Remember that absence of evidence for methane emissions does not always imply evidence for absence of methane emissions; for instance, when meteorological conditions are such that there is no sensor downwind of a methane source during the selected monitoring period, then it will be impossible to detect, localize and quantify this particular source. Also, there are limitations to the forward dispersion model which is used in the analysis. For example, the performance of the Gaussian plume dispersion model will degrade at lower wind speeds. Therefore, careful interpretation of the data is always required.</p>"},{"location":"#installing-pyelq-as-a-package","title":"Installing pyELQ as a package","text":"<p>Suppose you want to use this pyELQ package in a different project. You can install it from PyPi through pip <code>pip install pyelq</code>. Or you could clone the repository and install it from the source code. After activating the environment you want to install pyELQ in, open a terminal, move to the main pyELQ folder where pyproject.toml is located and run <code>pip install .</code>, optionally you can pass the <code>-e</code> flag is for editable mode. All the main options, info and settings for the package are found in the pyproject.toml file which sits in this repo as well.</p>"},{"location":"#examples","title":"Examples","text":"<p>For some examples on how to use this package please check out these Examples</p>"},{"location":"#contribution","title":"Contribution","text":"<p>This project welcomes contributions and suggestions. If you have a suggestion that would make this better you can simply open an issue with a relevant title. Don't forget to give the project a star! Thanks again!</p> <p>For more details on contributing to this repository, see the Contributing guide.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>Distributed under the Apache License Version 2.0. See the license file for more information.</p>"},{"location":"pyelq/coordinate_system/","title":"Coordinate System","text":""},{"location":"pyelq/coordinate_system/#coordinate-system","title":"Coordinate System","text":"<p>Coordinate System.</p> <p>This code provides the definition of, and the functionality for, all the main coordinate systems that are used in pyELQ. Each coordinate system has relevant methods for features that are commonly required. Also provided is a set of conversions between each of the systems, alongside some functionality for interpolation.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate","title":"<code>Coordinate</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for coordinate transformations.</p> <p>Attributes:</p> Name Type Description <code>use_degrees</code> <code>bool</code> <p>Flag if reference uses degrees (True) or radians (False). Defaults to True.</p> <code>ellipsoid</code> <code>Ellipsoid</code> <p>Definition of the Ellipsoid used in the coordinate system, for which the default is WGS84. See: https://en.wikipedia.org/wiki/World_Geodetic_System.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass Coordinate(ABC):\n    \"\"\"Abstract base class for coordinate transformations.\n\n    Attributes:\n        use_degrees (bool): Flag if reference uses degrees (True) or radians (False). Defaults to True.\n        ellipsoid (pm.Ellipsoid): Definition of the Ellipsoid used in the coordinate system, for which the default is\n            WGS84. See: https://en.wikipedia.org/wiki/World_Geodetic_System.\n\n    \"\"\"\n\n    use_degrees: bool = field(init=False)\n    ellipsoid: pm.Ellipsoid = field(init=False)\n\n    def __post_init__(self):\n        self.use_degrees = True\n        self.ellipsoid = pm.Ellipsoid.from_name(\"wgs84\")\n\n    @property\n    @abstractmethod\n    def nof_observations(self) -&gt; int:\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n\n    @abstractmethod\n    def from_array(self, array: np.ndarray) -&gt; None:\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n\n    @abstractmethod\n    def to_array(self, dim: int = 3) -&gt; np.ndarray:\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            np.ndarray: Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n\n    @abstractmethod\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n\n    @abstractmethod\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n\n    @abstractmethod\n    def to_enu(self, ref_latitude: float = None, ref_longitude: float = None, ref_altitude: float = None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n\n    def to_object_type(self, coordinate_object):\n        \"\"\"Converts current object to same class as input coordinate_object.\n\n        Args:\n            coordinate_object (Coordinate): An coordinate object which provides the coordinate system to convert self to\n\n        Returns:\n            (Coordinate): The converted coordinate object\n\n        \"\"\"\n        if type(coordinate_object) is not type(self):\n            if isinstance(coordinate_object, LLA):\n                temp_object = self.to_lla()\n            elif isinstance(coordinate_object, ENU):\n                temp_object = self.to_enu(\n                    ref_latitude=coordinate_object.ref_latitude,\n                    ref_longitude=coordinate_object.ref_longitude,\n                    ref_altitude=coordinate_object.ref_altitude,\n                )\n            elif isinstance(coordinate_object, ECEF):\n                temp_object = self.to_ecef()\n            else:\n                raise TypeError(\"Please provide a valid coordinate type\")\n\n            return temp_object\n\n        return self\n\n    def interpolate(self, values: np.ndarray, locations, dim: int = 3, **kwargs) -&gt; np.ndarray:\n        \"\"\"Interpolate data using coordinate object.\n\n        If locations coordinate system does not match self's coordinate system it will be converted to same type as\n        self. In the ENU case extra checking needs to take place to check reference locations match up.\n\n        If only 1 value is provided which needs to be interpolated to many other locations we just set the value at all\n        these locations to the single input value\n\n        Args:\n            values (np.ndarray): Values to interpolate,  consistent with location in self\n            locations (Coordinate): Coordinate object containing locations to which you want to interpolate\n            dim (int): Number of dimensions to use for interpolation (2 or 3)\n            **kwargs (dict):  Other arguments available in scipy.interpolate.griddata e.g. method, fill_value\n\n        Returns:\n            Result (np.ndarray): Interpolated values at requested locations.\n\n        \"\"\"\n        locations = locations.to_object_type(coordinate_object=self)\n\n        if isinstance(self, ENU):\n            if (\n                self.ref_latitude != locations.ref_latitude\n                or self.ref_longitude != locations.ref_longitude\n                or self.ref_altitude != locations.ref_altitude\n            ):\n                locations = locations.to_lla()\n                locations = locations.to_enu(\n                    ref_latitude=self.ref_latitude, ref_longitude=self.ref_longitude, ref_altitude=self.ref_altitude\n                )\n        result = sti.interpolate(\n            location_in=self.to_array(dim),\n            values_in=values.flatten(),\n            location_out=locations.to_array(dim=dim),\n            **kwargs,\n        )\n\n        return result\n\n    def make_grid(\n        self, bounds: np.ndarray, grid_type: str = \"rectangular\", shape: Union[tuple, np.ndarray] = (5, 5, 1)\n    ) -&gt; np.ndarray:\n        \"\"\"Generates grid of values locations based on specified inputs.\n\n        If the grid type is 'spherical', we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the\n        use in temp_lat_rad and temp_lon_rad.\n\n        Args:\n            bounds (np.ndarray): Limits of the grid on which to generate the grid of size [dim x 2]\n                if dim == 2 we assume the third dimension will be zeros\n            grid_type (str, optional): Type of grid to generate, default 'rectangular':\n                     rectangular == rectangular grid of shape grd_shape,\n                     spherical == grid of shape grid_shape taking into account a spherical spacing\n            shape: (tuple, optional): Number of grid cells to generate in each dimension, total number of\n                grid cells will be the product of the entries of this tuple\n\n        Returns\n            np.ndarray: gridded of locations\n\n        \"\"\"\n        dimension = bounds.shape[0]\n\n        if grid_type == \"rectangular\":\n            dim_0 = np.linspace(bounds[0, 0], bounds[0, 1], num=shape[0])\n            dim_1 = np.linspace(bounds[1, 0], bounds[1, 1], num=shape[1])\n            if dimension == 3:\n                dim_2 = np.linspace(bounds[2, 0], bounds[2, 1], num=shape[2])\n            else:\n                dim_2 = np.array(0)\n\n            dim_0, dim_1, dim_2 = np.meshgrid(dim_0, dim_1, dim_2)\n            array = np.stack([dim_0.flatten(), dim_1.flatten(), dim_2.flatten()], axis=1)\n        elif grid_type == \"spherical\":\n            temp_object = deepcopy(self)\n            temp_object.from_array(array=bounds)\n            temp_object = temp_object.to_lla()\n            temp_object.latitude = (temp_object.latitude - (-90)) / 180\n            temp_object.longitude = (temp_object.longitude - (-180)) / 360\n\n            temp_lat_rad = np.linspace(start=temp_object.latitude[0], stop=temp_object.latitude[1], num=shape[0])\n            temp_lon_rad = np.linspace(start=temp_object.longitude[0], stop=temp_object.longitude[1], num=shape[1])\n\n            longitude = (2 * np.pi * temp_lon_rad - np.pi) * 180 / np.pi\n            latitude = (np.arccos(1 - 2 * temp_lat_rad) - 0.5 * np.pi) * 180 / np.pi\n            if dimension == 3:\n                altitude = np.linspace(start=temp_object.altitude[0], stop=temp_object.altitude[1], num=shape[2])\n                latitude, longitude, altitude = np.meshgrid(latitude, longitude, altitude)\n                array = np.stack(\n                    [latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180, altitude.flatten()], axis=1\n                )\n            else:\n                latitude, longitude = np.meshgrid(latitude, longitude)\n                array = np.stack([latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180], axis=1)\n\n            temp_object.from_array(array=array)\n            temp_object = temp_object.to_object_type(self)\n            array = temp_object.to_array()\n        else:\n            raise NotImplementedError(\"Please provide a valid grid type\")\n\n        return array\n\n    def create_tree(self) -&gt; KDTree:\n        \"\"\"Create KD tree for the purpose of fast distance computation.\n\n        Returns:\n                KDTree: Spatial KD tree\n\n        \"\"\"\n        return KDTree(self.to_array())\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.nof_observations","title":"<code>nof_observations</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.from_array","title":"<code>from_array(array)</code>  <code>abstractmethod</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef from_array(self, array: np.ndarray) -&gt; None:\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_array","title":"<code>to_array(dim=3)</code>  <code>abstractmethod</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_array(self, dim: int = 3) -&gt; np.ndarray:\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        np.ndarray: Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_lla","title":"<code>to_lla()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_ecef","title":"<code>to_ecef()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>  <code>abstractmethod</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_enu(self, ref_latitude: float = None, ref_longitude: float = None, ref_altitude: float = None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_object_type","title":"<code>to_object_type(coordinate_object)</code>","text":"<p>Converts current object to same class as input coordinate_object.</p> <p>Parameters:</p> Name Type Description Default <code>coordinate_object</code> <code>Coordinate</code> <p>An coordinate object which provides the coordinate system to convert self to</p> required <p>Returns:</p> Type Description <code>Coordinate</code> <p>The converted coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_object_type(self, coordinate_object):\n    \"\"\"Converts current object to same class as input coordinate_object.\n\n    Args:\n        coordinate_object (Coordinate): An coordinate object which provides the coordinate system to convert self to\n\n    Returns:\n        (Coordinate): The converted coordinate object\n\n    \"\"\"\n    if type(coordinate_object) is not type(self):\n        if isinstance(coordinate_object, LLA):\n            temp_object = self.to_lla()\n        elif isinstance(coordinate_object, ENU):\n            temp_object = self.to_enu(\n                ref_latitude=coordinate_object.ref_latitude,\n                ref_longitude=coordinate_object.ref_longitude,\n                ref_altitude=coordinate_object.ref_altitude,\n            )\n        elif isinstance(coordinate_object, ECEF):\n            temp_object = self.to_ecef()\n        else:\n            raise TypeError(\"Please provide a valid coordinate type\")\n\n        return temp_object\n\n    return self\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.interpolate","title":"<code>interpolate(values, locations, dim=3, **kwargs)</code>","text":"<p>Interpolate data using coordinate object.</p> <p>If locations coordinate system does not match self's coordinate system it will be converted to same type as self. In the ENU case extra checking needs to take place to check reference locations match up.</p> <p>If only 1 value is provided which needs to be interpolated to many other locations we just set the value at all these locations to the single input value</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>Values to interpolate,  consistent with location in self</p> required <code>locations</code> <code>Coordinate</code> <p>Coordinate object containing locations to which you want to interpolate</p> required <code>dim</code> <code>int</code> <p>Number of dimensions to use for interpolation (2 or 3)</p> <code>3</code> <code>**kwargs</code> <code>dict</code> <p>Other arguments available in scipy.interpolate.griddata e.g. method, fill_value</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>ndarray</code> <p>Interpolated values at requested locations.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def interpolate(self, values: np.ndarray, locations, dim: int = 3, **kwargs) -&gt; np.ndarray:\n    \"\"\"Interpolate data using coordinate object.\n\n    If locations coordinate system does not match self's coordinate system it will be converted to same type as\n    self. In the ENU case extra checking needs to take place to check reference locations match up.\n\n    If only 1 value is provided which needs to be interpolated to many other locations we just set the value at all\n    these locations to the single input value\n\n    Args:\n        values (np.ndarray): Values to interpolate,  consistent with location in self\n        locations (Coordinate): Coordinate object containing locations to which you want to interpolate\n        dim (int): Number of dimensions to use for interpolation (2 or 3)\n        **kwargs (dict):  Other arguments available in scipy.interpolate.griddata e.g. method, fill_value\n\n    Returns:\n        Result (np.ndarray): Interpolated values at requested locations.\n\n    \"\"\"\n    locations = locations.to_object_type(coordinate_object=self)\n\n    if isinstance(self, ENU):\n        if (\n            self.ref_latitude != locations.ref_latitude\n            or self.ref_longitude != locations.ref_longitude\n            or self.ref_altitude != locations.ref_altitude\n        ):\n            locations = locations.to_lla()\n            locations = locations.to_enu(\n                ref_latitude=self.ref_latitude, ref_longitude=self.ref_longitude, ref_altitude=self.ref_altitude\n            )\n    result = sti.interpolate(\n        location_in=self.to_array(dim),\n        values_in=values.flatten(),\n        location_out=locations.to_array(dim=dim),\n        **kwargs,\n    )\n\n    return result\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.make_grid","title":"<code>make_grid(bounds, grid_type='rectangular', shape=(5, 5, 1))</code>","text":"<p>Generates grid of values locations based on specified inputs.</p> <p>If the grid type is 'spherical', we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad and temp_lon_rad.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>ndarray</code> <p>Limits of the grid on which to generate the grid of size [dim x 2] if dim == 2 we assume the third dimension will be zeros</p> required <code>grid_type</code> <code>str</code> <p>Type of grid to generate, default 'rectangular':      rectangular == rectangular grid of shape grd_shape,      spherical == grid of shape grid_shape taking into account a spherical spacing</p> <code>'rectangular'</code> <code>shape</code> <code>Union[tuple, ndarray]</code> <p>(tuple, optional): Number of grid cells to generate in each dimension, total number of grid cells will be the product of the entries of this tuple</p> <code>(5, 5, 1)</code> <p>Returns     np.ndarray: gridded of locations</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def make_grid(\n    self, bounds: np.ndarray, grid_type: str = \"rectangular\", shape: Union[tuple, np.ndarray] = (5, 5, 1)\n) -&gt; np.ndarray:\n    \"\"\"Generates grid of values locations based on specified inputs.\n\n    If the grid type is 'spherical', we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the\n    use in temp_lat_rad and temp_lon_rad.\n\n    Args:\n        bounds (np.ndarray): Limits of the grid on which to generate the grid of size [dim x 2]\n            if dim == 2 we assume the third dimension will be zeros\n        grid_type (str, optional): Type of grid to generate, default 'rectangular':\n                 rectangular == rectangular grid of shape grd_shape,\n                 spherical == grid of shape grid_shape taking into account a spherical spacing\n        shape: (tuple, optional): Number of grid cells to generate in each dimension, total number of\n            grid cells will be the product of the entries of this tuple\n\n    Returns\n        np.ndarray: gridded of locations\n\n    \"\"\"\n    dimension = bounds.shape[0]\n\n    if grid_type == \"rectangular\":\n        dim_0 = np.linspace(bounds[0, 0], bounds[0, 1], num=shape[0])\n        dim_1 = np.linspace(bounds[1, 0], bounds[1, 1], num=shape[1])\n        if dimension == 3:\n            dim_2 = np.linspace(bounds[2, 0], bounds[2, 1], num=shape[2])\n        else:\n            dim_2 = np.array(0)\n\n        dim_0, dim_1, dim_2 = np.meshgrid(dim_0, dim_1, dim_2)\n        array = np.stack([dim_0.flatten(), dim_1.flatten(), dim_2.flatten()], axis=1)\n    elif grid_type == \"spherical\":\n        temp_object = deepcopy(self)\n        temp_object.from_array(array=bounds)\n        temp_object = temp_object.to_lla()\n        temp_object.latitude = (temp_object.latitude - (-90)) / 180\n        temp_object.longitude = (temp_object.longitude - (-180)) / 360\n\n        temp_lat_rad = np.linspace(start=temp_object.latitude[0], stop=temp_object.latitude[1], num=shape[0])\n        temp_lon_rad = np.linspace(start=temp_object.longitude[0], stop=temp_object.longitude[1], num=shape[1])\n\n        longitude = (2 * np.pi * temp_lon_rad - np.pi) * 180 / np.pi\n        latitude = (np.arccos(1 - 2 * temp_lat_rad) - 0.5 * np.pi) * 180 / np.pi\n        if dimension == 3:\n            altitude = np.linspace(start=temp_object.altitude[0], stop=temp_object.altitude[1], num=shape[2])\n            latitude, longitude, altitude = np.meshgrid(latitude, longitude, altitude)\n            array = np.stack(\n                [latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180, altitude.flatten()], axis=1\n            )\n        else:\n            latitude, longitude = np.meshgrid(latitude, longitude)\n            array = np.stack([latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180], axis=1)\n\n        temp_object.from_array(array=array)\n        temp_object = temp_object.to_object_type(self)\n        array = temp_object.to_array()\n    else:\n        raise NotImplementedError(\"Please provide a valid grid type\")\n\n    return array\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.create_tree","title":"<code>create_tree()</code>","text":"<p>Create KD tree for the purpose of fast distance computation.</p> <p>Returns:</p> Name Type Description <code>KDTree</code> <code>KDTree</code> <p>Spatial KD tree</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def create_tree(self) -&gt; KDTree:\n    \"\"\"Create KD tree for the purpose of fast distance computation.\n\n    Returns:\n            KDTree: Spatial KD tree\n\n    \"\"\"\n    return KDTree(self.to_array())\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA","title":"<code>LLA</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Coordinate</code></p> <p>Defines the properties and functionality of the latitude/ longitude/ altitude coordinate system.</p> <p>Attributes:</p> Name Type Description <code>latitude</code> <code>ndarray</code> <p>Latitude values in degrees.</p> <code>longitude</code> <code>ndarray</code> <p>Longitude values in degrees.</p> <code>altitude</code> <code>ndarray</code> <p>Altitude values in meters with respect to a spheroid.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass LLA(Coordinate):\n    \"\"\"Defines the properties and functionality of the latitude/ longitude/ altitude coordinate system.\n\n    Attributes:\n        latitude (np.ndarray): Latitude values in degrees.\n        longitude (np.ndarray): Longitude values in degrees.\n        altitude (np.ndarray): Altitude values in meters with respect to a spheroid.\n\n    \"\"\"\n\n    latitude: np.ndarray = None\n    longitude: np.ndarray = None\n    altitude: np.ndarray = None\n\n    @property\n    def nof_observations(self):\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n        if self.latitude is None:\n            return 0\n        return self.latitude.size\n\n    def from_array(self, array):\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n        dim = array.shape[1]\n        self.latitude = array[:, 0]\n        self.longitude = array[:, 1]\n        self.altitude = np.zeros_like(self.latitude)\n        if dim == 3:\n            self.altitude = array[:, 2]\n\n    def to_array(self, dim=3):\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n        if dim == 2:\n            return np.stack((self.latitude.flatten(), self.longitude.flatten()), axis=1)\n        return np.stack((self.latitude.flatten(), self.longitude.flatten(), self.altitude.flatten()), axis=1)\n\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n        return self\n\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n        if self.altitude is None:\n            self.altitude = np.zeros(self.latitude.shape)\n        ecef_object = ECEF()\n        ecef_object.x, ecef_object.y, ecef_object.z = pm.geodetic2ecef(\n            lat=self.latitude, lon=self.longitude, alt=self.altitude, ell=self.ellipsoid, deg=self.use_degrees\n        )\n\n        return ecef_object\n\n    def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n        if self.altitude is None:\n            self.altitude = np.zeros(self.latitude.shape)\n\n        if ref_altitude is None:\n            ref_altitude = np.amin(self.altitude)\n\n        if ref_latitude is None:\n            ref_latitude = np.amin(self.latitude)\n\n        if ref_longitude is None:\n            ref_longitude = np.amin(self.longitude)\n\n        enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n        enu_object.east, enu_object.north, enu_object.up = pm.geodetic2enu(\n            lat=self.latitude,\n            lon=self.longitude,\n            h=self.altitude,\n            lat0=ref_latitude,\n            lon0=ref_longitude,\n            h0=ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.from_array","title":"<code>from_array(array)</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def from_array(self, array):\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n    dim = array.shape[1]\n    self.latitude = array[:, 0]\n    self.longitude = array[:, 1]\n    self.altitude = np.zeros_like(self.latitude)\n    if dim == 3:\n        self.altitude = array[:, 2]\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_array","title":"<code>to_array(dim=3)</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_array(self, dim=3):\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n    if dim == 2:\n        return np.stack((self.latitude.flatten(), self.longitude.flatten()), axis=1)\n    return np.stack((self.latitude.flatten(), self.longitude.flatten(), self.altitude.flatten()), axis=1)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_lla","title":"<code>to_lla()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n    return self\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_ecef","title":"<code>to_ecef()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n    if self.altitude is None:\n        self.altitude = np.zeros(self.latitude.shape)\n    ecef_object = ECEF()\n    ecef_object.x, ecef_object.y, ecef_object.z = pm.geodetic2ecef(\n        lat=self.latitude, lon=self.longitude, alt=self.altitude, ell=self.ellipsoid, deg=self.use_degrees\n    )\n\n    return ecef_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n    if self.altitude is None:\n        self.altitude = np.zeros(self.latitude.shape)\n\n    if ref_altitude is None:\n        ref_altitude = np.amin(self.altitude)\n\n    if ref_latitude is None:\n        ref_latitude = np.amin(self.latitude)\n\n    if ref_longitude is None:\n        ref_longitude = np.amin(self.longitude)\n\n    enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n    enu_object.east, enu_object.north, enu_object.up = pm.geodetic2enu(\n        lat=self.latitude,\n        lon=self.longitude,\n        h=self.altitude,\n        lat0=ref_latitude,\n        lon0=ref_longitude,\n        h0=ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU","title":"<code>ENU</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Coordinate</code></p> <p>Defines the properties and functionality of a local East-North-Up coordinate system.</p> <p>Positions relative to some reference location in metres.</p> <p>Attributes:</p> Name Type Description <code>ref_latitude</code> <code>float</code> <p>Reference latitude for current ENU system.</p> <code>ref_longitude</code> <code>float</code> <p>Reference longitude for current ENU system.</p> <code>ref_altitude</code> <code>float</code> <p>Reference altitude for current ENU system.</p> <code>east</code> <code>ndarray</code> <p>East values.</p> <code>north</code> <code>ndarray</code> <p>North values.</p> <code>up</code> <code>ndarray</code> <p>(np.ndarray): Up values.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass ENU(Coordinate):\n    \"\"\"Defines the properties and functionality of a local East-North-Up coordinate system.\n\n     Positions relative to some reference location in metres.\n\n    Attributes:\n        ref_latitude (float): Reference latitude for current ENU system.\n        ref_longitude (float): Reference longitude for current ENU system.\n        ref_altitude (float): Reference altitude for current ENU system.\n        east (np.ndarray): East values.\n        north (np.ndarray): North values.\n        up: (np.ndarray): Up values.\n\n    \"\"\"\n\n    ref_latitude: float\n    ref_longitude: float\n    ref_altitude: float\n    east: np.ndarray = None\n    north: np.ndarray = None\n    up: np.ndarray = None\n\n    @property\n    def nof_observations(self):\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n        if self.east is None:\n            return 0\n        return self.east.size\n\n    def from_array(self, array):\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n        dim = array.shape[1]\n        self.east = array[:, 0]\n        self.north = array[:, 1]\n        self.up = np.zeros_like(self.east)\n        if dim == 3:\n            self.up = array[:, 2]\n\n    def to_array(self, dim=3):\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n        if dim == 2:\n            return np.stack((self.east.flatten(), self.north.flatten()), axis=1)\n        return np.stack((self.east.flatten(), self.north.flatten(), self.up.flatten()), axis=1)\n\n    def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n        if ref_latitude is None:\n            ref_latitude = self.ref_latitude\n\n        if ref_longitude is None:\n            ref_longitude = self.ref_longitude\n\n        if ref_altitude is None:\n            ref_altitude = self.ref_altitude\n\n        if (\n            self.ref_latitude == ref_latitude\n            and self.ref_longitude == ref_longitude\n            and self.ref_altitude == ref_altitude\n        ):\n            return self\n\n        ecef_temp = self.to_ecef()\n\n        return ecef_temp.to_enu(ref_longitude=ref_longitude, ref_latitude=ref_latitude, ref_altitude=ref_altitude)\n\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n        lla_object = LLA()\n\n        lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.enu2geodetic(\n            e=self.east,\n            n=self.north,\n            u=self.up,\n            lat0=self.ref_latitude,\n            lon0=self.ref_longitude,\n            h0=self.ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return lla_object\n\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n        ecef_object = ECEF()\n\n        ecef_object.x, ecef_object.y, ecef_object.z = pm.enu2ecef(\n            e1=self.east,\n            n1=self.north,\n            u1=self.up,\n            lat0=self.ref_latitude,\n            lon0=self.ref_longitude,\n            h0=self.ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return ecef_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.from_array","title":"<code>from_array(array)</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def from_array(self, array):\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n    dim = array.shape[1]\n    self.east = array[:, 0]\n    self.north = array[:, 1]\n    self.up = np.zeros_like(self.east)\n    if dim == 3:\n        self.up = array[:, 2]\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_array","title":"<code>to_array(dim=3)</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_array(self, dim=3):\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n    if dim == 2:\n        return np.stack((self.east.flatten(), self.north.flatten()), axis=1)\n    return np.stack((self.east.flatten(), self.north.flatten(), self.up.flatten()), axis=1)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n    if ref_latitude is None:\n        ref_latitude = self.ref_latitude\n\n    if ref_longitude is None:\n        ref_longitude = self.ref_longitude\n\n    if ref_altitude is None:\n        ref_altitude = self.ref_altitude\n\n    if (\n        self.ref_latitude == ref_latitude\n        and self.ref_longitude == ref_longitude\n        and self.ref_altitude == ref_altitude\n    ):\n        return self\n\n    ecef_temp = self.to_ecef()\n\n    return ecef_temp.to_enu(ref_longitude=ref_longitude, ref_latitude=ref_latitude, ref_altitude=ref_altitude)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_lla","title":"<code>to_lla()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n    lla_object = LLA()\n\n    lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.enu2geodetic(\n        e=self.east,\n        n=self.north,\n        u=self.up,\n        lat0=self.ref_latitude,\n        lon0=self.ref_longitude,\n        h0=self.ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return lla_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_ecef","title":"<code>to_ecef()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n    ecef_object = ECEF()\n\n    ecef_object.x, ecef_object.y, ecef_object.z = pm.enu2ecef(\n        e1=self.east,\n        n1=self.north,\n        u1=self.up,\n        lat0=self.ref_latitude,\n        lon0=self.ref_longitude,\n        h0=self.ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return ecef_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF","title":"<code>ECEF</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Coordinate</code></p> <p>Defines the properties and functionality of an Earth-Centered, Earth-Fixed coordinate system.</p> <p>See: https://en.wikipedia.org/wiki/Earth-centered,_Earth-fixed_coordinate_system</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Eastings values [metres]</p> <code>y</code> <code>ndarray</code> <p>Northings values [metres]</p> <code>z</code> <code>ndarray</code> <p>Altitude values [metres]</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass ECEF(Coordinate):\n    \"\"\"Defines the properties and functionality of an Earth-Centered, Earth-Fixed coordinate system.\n\n    See: https://en.wikipedia.org/wiki/Earth-centered,_Earth-fixed_coordinate_system\n\n    Attributes:\n        x (np.ndarray): Eastings values [metres]\n        y (np.ndarray): Northings values [metres]\n        z (np.ndarray): Altitude values [metres]\n\n    \"\"\"\n\n    x: np.ndarray = None\n    y: np.ndarray = None\n    z: np.ndarray = None\n\n    @property\n    def nof_observations(self):\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n        if self.x is None:\n            return 0\n        return self.x.size\n\n    def from_array(self, array):\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n        dim = array.shape[1]\n        self.x = array[:, 0]\n        self.y = array[:, 1]\n        self.z = np.zeros_like(self.x)\n        if dim == 3:\n            self.z = array[:, 2]\n\n    def to_array(self, dim=3):\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n        if dim == 2:\n            return np.stack((self.x.flatten(), self.y.flatten()), axis=1)\n        return np.stack((self.x.flatten(), self.y.flatten(), self.z.flatten()), axis=1)\n\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n        return self\n\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n        lla_object = LLA()\n\n        lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.ecef2geodetic(\n            self.x, self.y, self.z, ell=self.ellipsoid, deg=self.use_degrees\n        )\n\n        return lla_object\n\n    def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n        if ref_latitude is None or ref_longitude is None or ref_altitude is None:\n            lla_object = self.to_lla()\n            return lla_object.to_enu()\n\n        enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n        enu_object.east, enu_object.north, enu_object.up = pm.ecef2enu(\n            x=self.x,\n            y=self.y,\n            z=self.z,\n            lat0=ref_latitude,\n            lon0=ref_longitude,\n            h0=ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.from_array","title":"<code>from_array(array)</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def from_array(self, array):\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n    dim = array.shape[1]\n    self.x = array[:, 0]\n    self.y = array[:, 1]\n    self.z = np.zeros_like(self.x)\n    if dim == 3:\n        self.z = array[:, 2]\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_array","title":"<code>to_array(dim=3)</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_array(self, dim=3):\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n    if dim == 2:\n        return np.stack((self.x.flatten(), self.y.flatten()), axis=1)\n    return np.stack((self.x.flatten(), self.y.flatten(), self.z.flatten()), axis=1)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_ecef","title":"<code>to_ecef()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n    return self\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_lla","title":"<code>to_lla()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n    lla_object = LLA()\n\n    lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.ecef2geodetic(\n        self.x, self.y, self.z, ell=self.ellipsoid, deg=self.use_degrees\n    )\n\n    return lla_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n    if ref_latitude is None or ref_longitude is None or ref_altitude is None:\n        lla_object = self.to_lla()\n        return lla_object.to_enu()\n\n    enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n    enu_object.east, enu_object.north, enu_object.up = pm.ecef2enu(\n        x=self.x,\n        y=self.y,\n        z=self.z,\n        lat0=ref_latitude,\n        lon0=ref_longitude,\n        h0=ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.make_latin_hypercube","title":"<code>make_latin_hypercube(bounds, nof_samples)</code>","text":"<p>Latin Hypercube samples.</p> <p>Draw samples according to a Latin Hypercube design within the specified bounds.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>ndarray</code> <p>Limits of the resulting hypercube of size [dim x 2]</p> required <code>nof_samples</code> <code>int</code> <p>Number of samples to draw</p> required <p>Returns:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>Samples forming the Latin Hypercube</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def make_latin_hypercube(bounds: np.ndarray, nof_samples: int) -&gt; np.ndarray:\n    \"\"\"Latin Hypercube samples.\n\n    Draw samples according to a Latin Hypercube design within the specified bounds.\n\n    Args:\n        bounds (np.ndarray): Limits of the resulting hypercube of size [dim x 2]\n        nof_samples (int): Number of samples to draw\n\n    Returns:\n        array (np.ndarray): Samples forming the Latin Hypercube\n\n    \"\"\"\n    dimension = bounds.shape[0]\n    sampler = qmc.LatinHypercube(d=dimension)\n    sample = sampler.random(n=nof_samples)\n    array = qmc.scale(sample, np.min(bounds, axis=1), np.max(bounds, axis=1))\n    return array\n</code></pre>"},{"location":"pyelq/dlm/","title":"DLM","text":""},{"location":"pyelq/dlm/#dlm","title":"DLM","text":"<p>DLM module.</p> <p>This module provides a class definition for the Dynamic Linear Models following Harrison and West 'Bayesian Forecasting and Dynamic Models' (2nd ed), Springer New York, NY, Chapter 4, https://doi.org/10.1007/b98971</p>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM","title":"<code>DLM</code>  <code>dataclass</code>","text":"<p>Defines the DLM in line with Harrison and West (2nd edition) Chapter 4.</p> <p>Attributes:</p> Name Type Description <code>f_matrix</code> <code>ndarray</code> <p>F matrix linking the state to the observables of size [nof_state_parameters x nof_observables]</p> <code>g_matrix</code> <code>ndarray</code> <p>G matrix characterizing the state evolution of size [nof_state_parameters x nof_state parameters]</p> <code>v_matrix</code> <code>ndarray</code> <p>V matrix being the covariance matrix of the zero mean observation noise of size [nof_state_parameters x nof_observables]</p> <code>w_matrix</code> <code>ndarray</code> <p>W matrix being the covariance matrix of the zero mean system noise of size [nof_state_parameters x nof_state parameters]</p> <code>g_power</code> <code>ndarray</code> <p>Attribute to store G^k, does not get initialized</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>@dataclass\nclass DLM:\n    \"\"\"Defines the DLM in line with Harrison and West (2nd edition) Chapter 4.\n\n    Attributes:\n        f_matrix (np.ndarray, optional): F matrix linking the state to the observables of\n            size [nof_state_parameters x nof_observables]\n        g_matrix (np.ndarray, optional): G matrix characterizing the state evolution of\n            size [nof_state_parameters x nof_state parameters]\n        v_matrix (np.ndarray, optional): V matrix being the covariance matrix of the zero mean observation noise\n            of size [nof_state_parameters x nof_observables]\n        w_matrix (np.ndarray, optional): W matrix being the covariance matrix of the zero mean system noise of\n            size [nof_state_parameters x nof_state parameters]\n        g_power (np.ndarray, optional): Attribute to store G^k, does not get initialized\n\n    \"\"\"\n\n    f_matrix: np.ndarray = None\n    g_matrix: np.ndarray = None\n    v_matrix: np.ndarray = None\n    w_matrix: np.ndarray = None\n    g_power: np.ndarray = field(init=False)\n\n    @property\n    def nof_observables(self) -&gt; int:\n        \"\"\"Int: Number of observables as derived from the associated F matrix.\"\"\"\n        if self.f_matrix is not None and isinstance(self.f_matrix, np.ndarray):\n            return self.f_matrix.shape[1]\n        return 0\n\n    @property\n    def nof_state_parameters(self) -&gt; int:\n        \"\"\"Int: Number of state parameters as derived from the associated G matrix.\"\"\"\n        if self.g_matrix is not None and isinstance(self.g_matrix, np.ndarray):\n            return self.g_matrix.shape[0]\n        return 0\n\n    def calculate_g_power(self, max_power: int) -&gt; None:\n        \"\"\"Calculate the powers of the G matrix.\n\n        Calculate the powers upfront, so we don't have to calculate it at every iteration. Result gets stored in the\n        g_power attribute of the DLM class. We use an iterative way of calculating the power to have the fewest matrix\n        multiplications necessary, i.e. we are not using numpy.linalg.matrix_power as that would leak to k factorial\n        multiplications instead of the k we have now.\n\n        Args:\n            max_power (int): Maximum power to compute\n\n        \"\"\"\n        if self.nof_state_parameters == 1:\n            self.g_power = self.g_matrix ** np.array([[range(max_power + 1)]])\n        else:\n            self.g_power = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_power + 1))\n            self.g_power[:, :, 0] = np.identity(self.nof_state_parameters)\n            for i in range(max_power):\n                self.g_power[:, :, i + 1] = self.g_power[:, :, i] @ self.g_matrix\n\n    def polynomial_f_g(self, nof_observables: int, order: int) -&gt; None:\n        \"\"\"Create F and G matrices associated with a polynomial DLM.\n\n        Following Harrison and West (Chapter 7 on polynomial DLMs) with the exception that we use order==0 for a\n        \"constant\" DLM and order==1 for linear growth DLM, order==2 for quadratic growth etc.\n        Hence, the definition of n-th order polynomial DLM in Harrison &amp; West is implemented here with order=n-1\n        We stack the observables in a block diagonal form. So the first #order of rows belong to the first observable,\n        the second #order rows belong to the second observable etc.\n        Results are being stored in the f_matrix and g_matrix attributes respectively\n\n        Args:\n            nof_observables (int): Dimension of observation\n            order (int): Polynomial order (0=constant, 1=linear, 2=quadratic etc.)\n\n        \"\"\"\n        e_n = np.append(1, np.zeros(order))[:, None]\n        self.f_matrix = np.kron(np.eye(nof_observables), e_n)\n\n        l_n = np.triu(np.ones((order + 1, order + 1)))\n        self.g_matrix = np.kron(np.eye(nof_observables), l_n)\n\n    def simulate_data(self, init_state: np.ndarray, nof_timesteps: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Simulate data from DLM model.\n\n        Function to simulate state evolution and corresponding observations according to model as specified through DLM\n        class attributes (F, G, V and W matrices)\n\n        Args:\n            init_state (np.ndarray): Initial state vector to start simulating from of size [nof_state_parameters x 1]\n            nof_timesteps (int): Number of timesteps to simulate\n\n        Returns:\n            state (np.ndarray): Simulated state vectors of size [nof_state_parameters x nof_timesteps]\n            obs (np.ndarray): Simulated observations of size [nof_observables x nof_timesteps]\n\n        \"\"\"\n        if self.f_matrix is None or self.g_matrix is None or self.v_matrix is None or self.w_matrix is None:\n            raise ValueError(\"Please specify all matrices (F, G, V and W)\")\n\n        obs = np.empty((self.nof_observables, nof_timesteps))\n        state = np.empty((self.nof_state_parameters, nof_timesteps))\n\n        state[:, [0]] = init_state\n        mean_state_noise = np.zeros(self.nof_state_parameters)\n        mean_observation_noise = np.zeros(self.nof_observables)\n\n        for i in range(nof_timesteps):\n            if i == 0:\n                state[:, [i]] = self.g_matrix @ init_state + multivariate_normal.rvs(\n                    mean=mean_state_noise, cov=self.w_matrix, size=1\n                ).reshape((-1, 1))\n            else:\n                state[:, [i]] = self.g_matrix @ state[:, [i - 1]] + multivariate_normal.rvs(\n                    mean=mean_state_noise, cov=self.w_matrix, size=1\n                ).reshape((-1, 1))\n            obs[:, [i]] = self.f_matrix.T @ state[:, [i]] + multivariate_normal.rvs(\n                mean=mean_observation_noise, cov=self.v_matrix, size=1\n            ).reshape((-1, 1))\n\n        return state, obs\n\n    def forecast_mean(\n        self, current_mean_state: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Perform forecasting of the state and observation mean parameters.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), corollary 4.1, assuming F and G are\n        constant over time.\n        Note that in the output the second axis of the output arrays is the forecast dimension consistent with the\n        forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n\n        Args:\n            current_mean_state (np.ndarray): Current mean parameter for the state of size [nof_state_parameters x 1]\n            forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n        Returns:\n            a_t_k (np.array): Forecast values of state mean parameter of the size\n                [nof_observables x size(forecast_steps)]\n            f_t_k (np.array): Forecast values of observation mean parameter of the size\n                [nof_observables x size(forecast_steps)]\n\n        \"\"\"\n        min_forecast = np.amin(forecast_steps)\n\n        if min_forecast &lt; 1:\n            raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n        if isinstance(forecast_steps, int):\n            forecast_steps = [forecast_steps]\n\n        a_t_k = np.hstack([self.g_power[:, :, step] @ current_mean_state for step in forecast_steps])\n        f_t_k = self.f_matrix.T @ a_t_k\n\n        return a_t_k, f_t_k\n\n    def forecast_covariance(\n        self, c_matrix: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Perform forecasting of the state and observation covariance parameters.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n        constant over time.\n        Note that in the output the third axis of the output arrays is the forecast dimension consistent with the\n        forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n        sum_g_w_g is initialized as G^k @ W @ G^k for k==0, hence we initialize as W\n        Because of zero based indexing, in the for loop i==1 means 2-step ahead forecast which requires element\n        (i+1) of the g_power attribute as the third dimension serves as the actual power of the G matrix\n\n        Args:\n            c_matrix (np.ndarray): Current posterior covariance estimate for the state of size\n                [nof_state_parameters x nof_state_parameters]\n            forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n        Returns:\n            r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n                [nof_state_parameters x nof_state_parameters x size(forecast_steps)]\n            q_t_k (np.array): Forecast values of estimated observation covariance of the size\n                [nof_observables x nof_observables x size(forecast_steps)]\n\n        \"\"\"\n        min_forecast = np.amin(forecast_steps)\n        max_forecast = np.amax(forecast_steps)\n\n        if min_forecast &lt; 1:\n            raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n        if isinstance(forecast_steps, int):\n            forecast_steps = [forecast_steps]\n\n        sum_g_w_g = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_forecast))\n        sum_g_w_g[:, :, 0] = self.w_matrix\n        for i in np.arange(1, max_forecast, step=1):\n            sum_g_w_g[:, :, i] = (\n                sum_g_w_g[:, :, i - 1] + self.g_power[:, :, i] @ self.w_matrix @ self.g_power[:, :, i].T\n            )\n\n        r_t_k = np.dstack(\n            [\n                self.g_power[:, :, step] @ c_matrix @ self.g_power[:, :, step].T + sum_g_w_g[:, :, step - 1]\n                for step in forecast_steps\n            ]\n        )\n        q_t_k = np.dstack(\n            [self.f_matrix.T @ r_t_k[:, :, idx] @ self.f_matrix + self.v_matrix for idx in range(r_t_k.shape[2])]\n        )\n\n        return r_t_k, q_t_k\n\n    def update_posterior(\n        self, a_t: np.ndarray, r_matrix_t: np.ndarray, q_matrix_t: np.ndarray, error: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Update of the posterior mean and covariance of the state.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n        constant over time.\n        We are using a solver instead of calculating the inverse of Q directly\n        Setting inf values in Q equal to 0 after the solver function for computational issues, otherwise we would\n        get 0 * inf = nan, where we want the result to be 0.\n\n        Args:\n            a_t (np.ndarray): Current prior mean of the state of size [nof_state_parameters x 1]\n            r_matrix_t (np.ndarray): Current prior covariance of the state of size [nof_state_parameters x nof_state_parameters]\n            q_matrix_t (np.ndarray): Current one step ahead forecast covariance estimate of the observations of size [nof_observables x nof_observables]\n            error (np.ndarray): Error associated with the one step ahead forecast (observation - forecast) of size [nof_observables x 1]\n\n        Returns:\n            m_t (np.array): Posterior mean estimate of the state of size [nof_state_parameters x 1]\n            c_matrix (np.array): Posterior covariance estimate of the state of size [nof_state_parameters x nof_state_parameters]\n\n        \"\"\"\n        if self.nof_state_parameters == 1:\n            a_matrix_t = r_matrix_t @ self.f_matrix.T @ (1 / q_matrix_t)\n        else:\n            a_matrix_t = r_matrix_t @ np.linalg.solve(q_matrix_t.T, self.f_matrix.T).T\n        m_t = a_t + a_matrix_t @ error\n        q_matrix_t[np.isinf(q_matrix_t)] = 0\n        c_matrix = r_matrix_t - a_matrix_t @ q_matrix_t @ a_matrix_t.T\n\n        return m_t, c_matrix\n\n    def dlm_full_update(\n        self,\n        new_observation: np.ndarray,\n        current_mean_state: np.ndarray,\n        current_cov_state: np.ndarray,\n        mode: str = \"learn\",\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform 1 step of the full DLM update.\n\n        Following Harrison and West (2nd ed) we perform all steps to update the entire DLM model and obtain new\n        estimates for all parameters involved, including nan value handling.\n        When mode == 'learn' the parameters are updated, when mode == 'ignore' the current observation is ignored and\n        the posterior is set equal to the prior\n        When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n        such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n        terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n        we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n        forecast gets increased. We set the error equal to zero for computational issues, first but finally set it equal\n        to nan in the end.\n\n        Args:\n            new_observation (np.ndarray): New observations to use in the updating of the estimates of size [nof_observables x 1]\n            current_mean_state (np.ndarray):  Current mean estimate for the state of size [nof_state_parameters x 1]\n            current_cov_state (np.ndarray):  Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n            mode (str, optional): String indicating whether the DLM needs to be updated using the new observation or not. Currently, `learn` and `ignore` are implemented\n\n        Returns:\n            new_mean_state (np.ndarray): New mean estimate for the state of size [nof_state_parameters x 1]\n            new_cov_state (np.ndarray): New covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n            error (np.ndarray): Error between the observation and the forecast (observation - forecast) of size [nof_observables x 1]\n\n        \"\"\"\n        a_t, f_t = self.forecast_mean(current_mean_state, forecast_steps=1)\n        r_matrix_t, q_matrix_t = self.forecast_covariance(current_cov_state, forecast_steps=1)\n        error = new_observation - f_t\n\n        nan_bool = np.isnan(new_observation)\n        nan_idx = np.argwhere(nan_bool.flatten())\n        if np.any(nan_bool):\n            q_matrix_t[nan_idx, :, 0] -= self.v_matrix[nan_idx, :]\n            q_matrix_t[:, nan_idx, 0] -= self.v_matrix[:, nan_idx]\n            q_matrix_t[nan_idx, nan_idx, 0] = np.inf\n            error[nan_idx] = 0\n\n        if mode == \"learn\":\n            new_mean_state, new_cov_state = self.update_posterior(a_t, r_matrix_t[:, :, 0], q_matrix_t[:, :, 0], error)\n        elif mode == \"ignore\":\n            new_mean_state = a_t\n            new_cov_state = r_matrix_t\n        else:\n            raise TypeError(f\"Mode {mode} not implemented\")\n\n        error[nan_idx] = np.nan\n\n        return new_mean_state, new_cov_state, error\n\n    def calculate_mahalanobis_distance(\n        self,\n        new_observations: np.ndarray,\n        current_mean_state: np.ndarray,\n        current_cov_state: np.ndarray,\n        forecast_steps: int = 1,\n        return_statistics=False,\n    ) -&gt; Union[Tuple[float, np.ndarray], Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"Calculate the mahalanobis distance.\n\n        Calculating the Mahalanobis distance which is defined as error.T @ covariance^(-1) @ error\n        The error is flatted in row-major (C-style) This returns the stacked rows, which in our case is the errors per\n        observation parameter stacked and this is exactly what we want: array([[1, 2], [3, 4]]).reshape((-1, 1),\n        order='C') becomes column array([1, 2 3, 4])\n        Using a solve method instead of calculating inverse matrices directly\n        When calculating mhd_per_obs_param we use the partial result and reshape the temporary output such that we can\n        sum the correct elements associated with the same observable together\n        When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n        such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n        terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n        we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n        forecast gets increased. We set the error equal to zero for computational issues, but this does decrease the\n        number of degrees of freedom for that particular Mahalanobis distance calculation, basically decreasing the\n        Mahalanobis distance. We allow the option to output the number of degrees of freedom and chi2 statistic which\n        allows to take this decrease in degrees of freedom into account.\n\n        Args:\n            new_observations (np.ndarray): New observations to use in the calculation of the mahalanobis distance of\n                size [nof_observables x forecast_steps]\n            current_mean_state (np.ndarray): Current mean estimate for the state of size [nof_state_parameters x 1]\n            current_cov_state (np.ndarray): Current covariance estimate for the state of size\n                [nof_state_parameters x nof_state_parameters]\n            forecast_steps (int, optional): Number of steps ahead to forecast and use in the mahalanobis distance\n                calculation\n            return_statistics (bool, optional): Boolean to return used degrees of freedom and chi2 statistic\n        Returns:\n            mhd_overall (float): mahalanobis distance over all observables\n            mhd_per_obs_param (np.ndarray): mahalanobis distance per observation parameter of size [nof_observables, 1]\n\n        \"\"\"\n        if forecast_steps &lt;= 0:\n            raise AttributeError(\"Forecast steps should be a positive integer\")\n\n        if new_observations.size / self.nof_observables != forecast_steps:\n            raise AttributeError(\"Sizes of new observations and forecast steps are not aligning\")\n\n        _, f_t_k = self.forecast_mean(current_mean_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n        if new_observations.shape != f_t_k.shape:\n            raise AttributeError(\"Dimensions of new_observations are not aligning with dimensions of forecast\")\n\n        error = np.subtract(new_observations, f_t_k).reshape((-1, 1), order=\"C\")\n\n        r_t_k, q_t_k = self.forecast_covariance(current_cov_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n        nan_bool = np.isnan(new_observations)\n        if np.any(nan_bool):\n            nan_idx = np.argwhere(nan_bool)\n            for value in nan_idx:\n                q_t_k[value[0], :, value[1]] -= self.v_matrix[value[0], :]\n                q_t_k[:, value[0], value[1]] -= self.v_matrix[:, value[0]]\n\n            q_t_k[nan_idx[:, 0], nan_idx[:, 0], nan_idx[:, 1]] = np.inf\n            error[nan_bool.reshape((-1, 1), order=\"C\")] = 0\n\n        if forecast_steps &gt; 1:\n            full_covariance = self.create_full_covariance(r_t_k=r_t_k, q_t_k=q_t_k, forecast_steps=forecast_steps)\n        else:\n            full_covariance = q_t_k[:, :, 0]\n\n        mhd_overall = mahalanobis_distance(error=error, cov_matrix=full_covariance)\n        mhd_per_obs_param = np.empty((self.nof_observables, 1))\n\n        for i_obs in range(self.nof_observables):\n            ind_hrz = np.array(range(forecast_steps)) + i_obs * forecast_steps\n            mhd_per_obs_param[i_obs] = mahalanobis_distance(\n                error=error[ind_hrz], cov_matrix=full_covariance[np.ix_(ind_hrz, ind_hrz)]\n            )\n\n        if self.nof_observables == 1:\n            mhd_per_obs_param = mhd_per_obs_param.item()\n\n        if return_statistics:\n            dof_per_obs_param = (nan_bool.shape[1] - np.count_nonzero(nan_bool, axis=1)).reshape(\n                self.nof_observables, 1\n            )\n            dof_overall = dof_per_obs_param.sum()\n            chi2_cdf_per_obs_param = chi2.cdf(mhd_per_obs_param.flatten(), dof_per_obs_param.flatten()).reshape(\n                self.nof_observables, 1\n            )\n            chi2_cdf_overall = chi2.cdf(mhd_overall, dof_overall)\n\n            return (\n                mhd_overall,\n                mhd_per_obs_param,\n                dof_overall,\n                dof_per_obs_param,\n                chi2_cdf_overall,\n                chi2_cdf_per_obs_param,\n            )\n\n        return mhd_overall, mhd_per_obs_param\n\n    def create_full_covariance(self, r_t_k: np.ndarray, q_t_k: np.ndarray, forecast_steps: int) -&gt; np.ndarray:\n        \"\"\"Helper function to construct the full covariance matrix.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast distributions) Theorem 4.2 and corollary 4.2\n        we construct the full covariance matrix. This full covariance matrix is the covariance matrix of all forecasted\n        observations with respect to each other. Hence, it's COV[Y_{t+k}, Y_{t+j}] with j and k 1&lt;=j,k&lt;=forecast steps\n        input argument and Y_{t+k} the k step ahead forecast of the observation at time t\n\n        The matrix is build up using the different blocks for different covariances between observations i and j.\n        The diagonals of each block are calculated first as q_t_k[i, j, :].\n        Next the i, j-th (lower triangular) entry of the m, n-th block is calculated as\n        (F.T @ G^(i-j) r_t_k[:, :, j] @ F)[i, j]\n        Next each upper triangular part of each lower diagonal block is calculated and next the entire upper triangular\n        part of the full matrix is calculated\n\n        Args:\n            r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n                [nof_state_parameters x nof_state_parameters x forecast_steps]\n            q_t_k (np.array): Forecast values of estimated observation covariance of the size\n                [nof_observables x nof_observables x forecast_steps]\n            forecast_steps (int): Maximum number of steps ahead to forecast and use all of those in the mahalanobis\n                distance calculation\n\n        Returns:\n            full_covariance (np.array): Full covariance matrix of all forecasted observations with respect to each other\n            having size [(nof_observables * forecast_steps) X (nof_observables * forecast_steps)]\n\n        \"\"\"\n        full_covariance = np.zeros((forecast_steps * self.nof_observables, forecast_steps * self.nof_observables))\n        base_idx = np.array(range(forecast_steps))\n        for block_i in range(self.nof_observables):\n            for block_j in range(block_i + 1):\n                block_rows = base_idx + block_i * forecast_steps\n                block_cols = base_idx + block_j * forecast_steps\n                full_covariance[block_rows, block_cols] = q_t_k[block_i, block_j, :]\n\n        temp_idx = np.array(range(self.nof_observables))\n        for sub_i in np.arange(start=1, stop=forecast_steps, step=1):\n            sub_row = temp_idx * forecast_steps + sub_i\n            for sub_j in range(sub_i):\n                sub_col = temp_idx * forecast_steps + sub_j\n                sub_idx = np.ix_(sub_row, sub_col)\n                full_covariance[sub_idx] = (\n                    self.f_matrix.T @ self.g_power[:, :, sub_i - sub_j] @ r_t_k[:, :, sub_j] @ self.f_matrix\n                )\n\n        for block_i in range(self.nof_observables):\n            for block_j in range(block_i):\n                block_rows = base_idx + block_i * forecast_steps\n                block_cols = base_idx + block_j * forecast_steps\n                block_idx = np.ix_(block_rows, block_cols)\n                full_covariance[block_idx] = full_covariance[block_idx] + np.tril(full_covariance[block_idx], k=-1).T\n\n        full_covariance = np.tril(full_covariance) + np.tril(full_covariance, k=-1).T\n\n        return full_covariance\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.nof_observables","title":"<code>nof_observables</code>  <code>property</code>","text":""},{"location":"pyelq/dlm/#pyelq.dlm.DLM.nof_state_parameters","title":"<code>nof_state_parameters</code>  <code>property</code>","text":""},{"location":"pyelq/dlm/#pyelq.dlm.DLM.calculate_g_power","title":"<code>calculate_g_power(max_power)</code>","text":"<p>Calculate the powers of the G matrix.</p> <p>Calculate the powers upfront, so we don't have to calculate it at every iteration. Result gets stored in the g_power attribute of the DLM class. We use an iterative way of calculating the power to have the fewest matrix multiplications necessary, i.e. we are not using numpy.linalg.matrix_power as that would leak to k factorial multiplications instead of the k we have now.</p> <p>Parameters:</p> Name Type Description Default <code>max_power</code> <code>int</code> <p>Maximum power to compute</p> required Source code in <code>src/pyelq/dlm.py</code> <pre><code>def calculate_g_power(self, max_power: int) -&gt; None:\n    \"\"\"Calculate the powers of the G matrix.\n\n    Calculate the powers upfront, so we don't have to calculate it at every iteration. Result gets stored in the\n    g_power attribute of the DLM class. We use an iterative way of calculating the power to have the fewest matrix\n    multiplications necessary, i.e. we are not using numpy.linalg.matrix_power as that would leak to k factorial\n    multiplications instead of the k we have now.\n\n    Args:\n        max_power (int): Maximum power to compute\n\n    \"\"\"\n    if self.nof_state_parameters == 1:\n        self.g_power = self.g_matrix ** np.array([[range(max_power + 1)]])\n    else:\n        self.g_power = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_power + 1))\n        self.g_power[:, :, 0] = np.identity(self.nof_state_parameters)\n        for i in range(max_power):\n            self.g_power[:, :, i + 1] = self.g_power[:, :, i] @ self.g_matrix\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.polynomial_f_g","title":"<code>polynomial_f_g(nof_observables, order)</code>","text":"<p>Create F and G matrices associated with a polynomial DLM.</p> <p>Following Harrison and West (Chapter 7 on polynomial DLMs) with the exception that we use order==0 for a \"constant\" DLM and order==1 for linear growth DLM, order==2 for quadratic growth etc. Hence, the definition of n-th order polynomial DLM in Harrison &amp; West is implemented here with order=n-1 We stack the observables in a block diagonal form. So the first #order of rows belong to the first observable, the second #order rows belong to the second observable etc. Results are being stored in the f_matrix and g_matrix attributes respectively</p> <p>Parameters:</p> Name Type Description Default <code>nof_observables</code> <code>int</code> <p>Dimension of observation</p> required <code>order</code> <code>int</code> <p>Polynomial order (0=constant, 1=linear, 2=quadratic etc.)</p> required Source code in <code>src/pyelq/dlm.py</code> <pre><code>def polynomial_f_g(self, nof_observables: int, order: int) -&gt; None:\n    \"\"\"Create F and G matrices associated with a polynomial DLM.\n\n    Following Harrison and West (Chapter 7 on polynomial DLMs) with the exception that we use order==0 for a\n    \"constant\" DLM and order==1 for linear growth DLM, order==2 for quadratic growth etc.\n    Hence, the definition of n-th order polynomial DLM in Harrison &amp; West is implemented here with order=n-1\n    We stack the observables in a block diagonal form. So the first #order of rows belong to the first observable,\n    the second #order rows belong to the second observable etc.\n    Results are being stored in the f_matrix and g_matrix attributes respectively\n\n    Args:\n        nof_observables (int): Dimension of observation\n        order (int): Polynomial order (0=constant, 1=linear, 2=quadratic etc.)\n\n    \"\"\"\n    e_n = np.append(1, np.zeros(order))[:, None]\n    self.f_matrix = np.kron(np.eye(nof_observables), e_n)\n\n    l_n = np.triu(np.ones((order + 1, order + 1)))\n    self.g_matrix = np.kron(np.eye(nof_observables), l_n)\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.simulate_data","title":"<code>simulate_data(init_state, nof_timesteps)</code>","text":"<p>Simulate data from DLM model.</p> <p>Function to simulate state evolution and corresponding observations according to model as specified through DLM class attributes (F, G, V and W matrices)</p> <p>Parameters:</p> Name Type Description Default <code>init_state</code> <code>ndarray</code> <p>Initial state vector to start simulating from of size [nof_state_parameters x 1]</p> required <code>nof_timesteps</code> <code>int</code> <p>Number of timesteps to simulate</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>ndarray</code> <p>Simulated state vectors of size [nof_state_parameters x nof_timesteps]</p> <code>obs</code> <code>ndarray</code> <p>Simulated observations of size [nof_observables x nof_timesteps]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def simulate_data(self, init_state: np.ndarray, nof_timesteps: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Simulate data from DLM model.\n\n    Function to simulate state evolution and corresponding observations according to model as specified through DLM\n    class attributes (F, G, V and W matrices)\n\n    Args:\n        init_state (np.ndarray): Initial state vector to start simulating from of size [nof_state_parameters x 1]\n        nof_timesteps (int): Number of timesteps to simulate\n\n    Returns:\n        state (np.ndarray): Simulated state vectors of size [nof_state_parameters x nof_timesteps]\n        obs (np.ndarray): Simulated observations of size [nof_observables x nof_timesteps]\n\n    \"\"\"\n    if self.f_matrix is None or self.g_matrix is None or self.v_matrix is None or self.w_matrix is None:\n        raise ValueError(\"Please specify all matrices (F, G, V and W)\")\n\n    obs = np.empty((self.nof_observables, nof_timesteps))\n    state = np.empty((self.nof_state_parameters, nof_timesteps))\n\n    state[:, [0]] = init_state\n    mean_state_noise = np.zeros(self.nof_state_parameters)\n    mean_observation_noise = np.zeros(self.nof_observables)\n\n    for i in range(nof_timesteps):\n        if i == 0:\n            state[:, [i]] = self.g_matrix @ init_state + multivariate_normal.rvs(\n                mean=mean_state_noise, cov=self.w_matrix, size=1\n            ).reshape((-1, 1))\n        else:\n            state[:, [i]] = self.g_matrix @ state[:, [i - 1]] + multivariate_normal.rvs(\n                mean=mean_state_noise, cov=self.w_matrix, size=1\n            ).reshape((-1, 1))\n        obs[:, [i]] = self.f_matrix.T @ state[:, [i]] + multivariate_normal.rvs(\n            mean=mean_observation_noise, cov=self.v_matrix, size=1\n        ).reshape((-1, 1))\n\n    return state, obs\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.forecast_mean","title":"<code>forecast_mean(current_mean_state, forecast_steps=1)</code>","text":"<p>Perform forecasting of the state and observation mean parameters.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), corollary 4.1, assuming F and G are constant over time. Note that in the output the second axis of the output arrays is the forecast dimension consistent with the forecast steps input, all forecast steps contained in the forecast steps argument are returned.</p> <p>Parameters:</p> Name Type Description Default <code>current_mean_state</code> <code>ndarray</code> <p>Current mean parameter for the state of size [nof_state_parameters x 1]</p> required <code>forecast_steps</code> <code>Union[int, list, ndarray]</code> <p>Steps ahead to forecast</p> <code>1</code> <p>Returns:</p> Name Type Description <code>a_t_k</code> <code>array</code> <p>Forecast values of state mean parameter of the size [nof_observables x size(forecast_steps)]</p> <code>f_t_k</code> <code>array</code> <p>Forecast values of observation mean parameter of the size [nof_observables x size(forecast_steps)]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def forecast_mean(\n    self, current_mean_state: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform forecasting of the state and observation mean parameters.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), corollary 4.1, assuming F and G are\n    constant over time.\n    Note that in the output the second axis of the output arrays is the forecast dimension consistent with the\n    forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n\n    Args:\n        current_mean_state (np.ndarray): Current mean parameter for the state of size [nof_state_parameters x 1]\n        forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n    Returns:\n        a_t_k (np.array): Forecast values of state mean parameter of the size\n            [nof_observables x size(forecast_steps)]\n        f_t_k (np.array): Forecast values of observation mean parameter of the size\n            [nof_observables x size(forecast_steps)]\n\n    \"\"\"\n    min_forecast = np.amin(forecast_steps)\n\n    if min_forecast &lt; 1:\n        raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n    if isinstance(forecast_steps, int):\n        forecast_steps = [forecast_steps]\n\n    a_t_k = np.hstack([self.g_power[:, :, step] @ current_mean_state for step in forecast_steps])\n    f_t_k = self.f_matrix.T @ a_t_k\n\n    return a_t_k, f_t_k\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.forecast_covariance","title":"<code>forecast_covariance(c_matrix, forecast_steps=1)</code>","text":"<p>Perform forecasting of the state and observation covariance parameters.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are constant over time. Note that in the output the third axis of the output arrays is the forecast dimension consistent with the forecast steps input, all forecast steps contained in the forecast steps argument are returned. sum_g_w_g is initialized as G^k @ W @ G^k for k==0, hence we initialize as W Because of zero based indexing, in the for loop i==1 means 2-step ahead forecast which requires element (i+1) of the g_power attribute as the third dimension serves as the actual power of the G matrix</p> <p>Parameters:</p> Name Type Description Default <code>c_matrix</code> <code>ndarray</code> <p>Current posterior covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>forecast_steps</code> <code>Union[int, list, ndarray]</code> <p>Steps ahead to forecast</p> <code>1</code> <p>Returns:</p> Name Type Description <code>r_t_k</code> <code>array</code> <p>Forecast values of estimated prior state covariance of the size [nof_state_parameters x nof_state_parameters x size(forecast_steps)]</p> <code>q_t_k</code> <code>array</code> <p>Forecast values of estimated observation covariance of the size [nof_observables x nof_observables x size(forecast_steps)]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def forecast_covariance(\n    self, c_matrix: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform forecasting of the state and observation covariance parameters.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n    constant over time.\n    Note that in the output the third axis of the output arrays is the forecast dimension consistent with the\n    forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n    sum_g_w_g is initialized as G^k @ W @ G^k for k==0, hence we initialize as W\n    Because of zero based indexing, in the for loop i==1 means 2-step ahead forecast which requires element\n    (i+1) of the g_power attribute as the third dimension serves as the actual power of the G matrix\n\n    Args:\n        c_matrix (np.ndarray): Current posterior covariance estimate for the state of size\n            [nof_state_parameters x nof_state_parameters]\n        forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n    Returns:\n        r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n            [nof_state_parameters x nof_state_parameters x size(forecast_steps)]\n        q_t_k (np.array): Forecast values of estimated observation covariance of the size\n            [nof_observables x nof_observables x size(forecast_steps)]\n\n    \"\"\"\n    min_forecast = np.amin(forecast_steps)\n    max_forecast = np.amax(forecast_steps)\n\n    if min_forecast &lt; 1:\n        raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n    if isinstance(forecast_steps, int):\n        forecast_steps = [forecast_steps]\n\n    sum_g_w_g = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_forecast))\n    sum_g_w_g[:, :, 0] = self.w_matrix\n    for i in np.arange(1, max_forecast, step=1):\n        sum_g_w_g[:, :, i] = (\n            sum_g_w_g[:, :, i - 1] + self.g_power[:, :, i] @ self.w_matrix @ self.g_power[:, :, i].T\n        )\n\n    r_t_k = np.dstack(\n        [\n            self.g_power[:, :, step] @ c_matrix @ self.g_power[:, :, step].T + sum_g_w_g[:, :, step - 1]\n            for step in forecast_steps\n        ]\n    )\n    q_t_k = np.dstack(\n        [self.f_matrix.T @ r_t_k[:, :, idx] @ self.f_matrix + self.v_matrix for idx in range(r_t_k.shape[2])]\n    )\n\n    return r_t_k, q_t_k\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.update_posterior","title":"<code>update_posterior(a_t, r_matrix_t, q_matrix_t, error)</code>","text":"<p>Update of the posterior mean and covariance of the state.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are constant over time. We are using a solver instead of calculating the inverse of Q directly Setting inf values in Q equal to 0 after the solver function for computational issues, otherwise we would get 0 * inf = nan, where we want the result to be 0.</p> <p>Parameters:</p> Name Type Description Default <code>a_t</code> <code>ndarray</code> <p>Current prior mean of the state of size [nof_state_parameters x 1]</p> required <code>r_matrix_t</code> <code>ndarray</code> <p>Current prior covariance of the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>q_matrix_t</code> <code>ndarray</code> <p>Current one step ahead forecast covariance estimate of the observations of size [nof_observables x nof_observables]</p> required <code>error</code> <code>ndarray</code> <p>Error associated with the one step ahead forecast (observation - forecast) of size [nof_observables x 1]</p> required <p>Returns:</p> Name Type Description <code>m_t</code> <code>array</code> <p>Posterior mean estimate of the state of size [nof_state_parameters x 1]</p> <code>c_matrix</code> <code>array</code> <p>Posterior covariance estimate of the state of size [nof_state_parameters x nof_state_parameters]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def update_posterior(\n    self, a_t: np.ndarray, r_matrix_t: np.ndarray, q_matrix_t: np.ndarray, error: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Update of the posterior mean and covariance of the state.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n    constant over time.\n    We are using a solver instead of calculating the inverse of Q directly\n    Setting inf values in Q equal to 0 after the solver function for computational issues, otherwise we would\n    get 0 * inf = nan, where we want the result to be 0.\n\n    Args:\n        a_t (np.ndarray): Current prior mean of the state of size [nof_state_parameters x 1]\n        r_matrix_t (np.ndarray): Current prior covariance of the state of size [nof_state_parameters x nof_state_parameters]\n        q_matrix_t (np.ndarray): Current one step ahead forecast covariance estimate of the observations of size [nof_observables x nof_observables]\n        error (np.ndarray): Error associated with the one step ahead forecast (observation - forecast) of size [nof_observables x 1]\n\n    Returns:\n        m_t (np.array): Posterior mean estimate of the state of size [nof_state_parameters x 1]\n        c_matrix (np.array): Posterior covariance estimate of the state of size [nof_state_parameters x nof_state_parameters]\n\n    \"\"\"\n    if self.nof_state_parameters == 1:\n        a_matrix_t = r_matrix_t @ self.f_matrix.T @ (1 / q_matrix_t)\n    else:\n        a_matrix_t = r_matrix_t @ np.linalg.solve(q_matrix_t.T, self.f_matrix.T).T\n    m_t = a_t + a_matrix_t @ error\n    q_matrix_t[np.isinf(q_matrix_t)] = 0\n    c_matrix = r_matrix_t - a_matrix_t @ q_matrix_t @ a_matrix_t.T\n\n    return m_t, c_matrix\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.dlm_full_update","title":"<code>dlm_full_update(new_observation, current_mean_state, current_cov_state, mode='learn')</code>","text":"<p>Perform 1 step of the full DLM update.</p> <p>Following Harrison and West (2nd ed) we perform all steps to update the entire DLM model and obtain new estimates for all parameters involved, including nan value handling. When mode == 'learn' the parameters are updated, when mode == 'ignore' the current observation is ignored and the posterior is set equal to the prior When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance) terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively, we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new forecast gets increased. We set the error equal to zero for computational issues, first but finally set it equal to nan in the end.</p> <p>Parameters:</p> Name Type Description Default <code>new_observation</code> <code>ndarray</code> <p>New observations to use in the updating of the estimates of size [nof_observables x 1]</p> required <code>current_mean_state</code> <code>ndarray</code> <p>Current mean estimate for the state of size [nof_state_parameters x 1]</p> required <code>current_cov_state</code> <code>ndarray</code> <p>Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>mode</code> <code>str</code> <p>String indicating whether the DLM needs to be updated using the new observation or not. Currently, <code>learn</code> and <code>ignore</code> are implemented</p> <code>'learn'</code> <p>Returns:</p> Name Type Description <code>new_mean_state</code> <code>ndarray</code> <p>New mean estimate for the state of size [nof_state_parameters x 1]</p> <code>new_cov_state</code> <code>ndarray</code> <p>New covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> <code>error</code> <code>ndarray</code> <p>Error between the observation and the forecast (observation - forecast) of size [nof_observables x 1]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def dlm_full_update(\n    self,\n    new_observation: np.ndarray,\n    current_mean_state: np.ndarray,\n    current_cov_state: np.ndarray,\n    mode: str = \"learn\",\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform 1 step of the full DLM update.\n\n    Following Harrison and West (2nd ed) we perform all steps to update the entire DLM model and obtain new\n    estimates for all parameters involved, including nan value handling.\n    When mode == 'learn' the parameters are updated, when mode == 'ignore' the current observation is ignored and\n    the posterior is set equal to the prior\n    When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n    such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n    terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n    we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n    forecast gets increased. We set the error equal to zero for computational issues, first but finally set it equal\n    to nan in the end.\n\n    Args:\n        new_observation (np.ndarray): New observations to use in the updating of the estimates of size [nof_observables x 1]\n        current_mean_state (np.ndarray):  Current mean estimate for the state of size [nof_state_parameters x 1]\n        current_cov_state (np.ndarray):  Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n        mode (str, optional): String indicating whether the DLM needs to be updated using the new observation or not. Currently, `learn` and `ignore` are implemented\n\n    Returns:\n        new_mean_state (np.ndarray): New mean estimate for the state of size [nof_state_parameters x 1]\n        new_cov_state (np.ndarray): New covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n        error (np.ndarray): Error between the observation and the forecast (observation - forecast) of size [nof_observables x 1]\n\n    \"\"\"\n    a_t, f_t = self.forecast_mean(current_mean_state, forecast_steps=1)\n    r_matrix_t, q_matrix_t = self.forecast_covariance(current_cov_state, forecast_steps=1)\n    error = new_observation - f_t\n\n    nan_bool = np.isnan(new_observation)\n    nan_idx = np.argwhere(nan_bool.flatten())\n    if np.any(nan_bool):\n        q_matrix_t[nan_idx, :, 0] -= self.v_matrix[nan_idx, :]\n        q_matrix_t[:, nan_idx, 0] -= self.v_matrix[:, nan_idx]\n        q_matrix_t[nan_idx, nan_idx, 0] = np.inf\n        error[nan_idx] = 0\n\n    if mode == \"learn\":\n        new_mean_state, new_cov_state = self.update_posterior(a_t, r_matrix_t[:, :, 0], q_matrix_t[:, :, 0], error)\n    elif mode == \"ignore\":\n        new_mean_state = a_t\n        new_cov_state = r_matrix_t\n    else:\n        raise TypeError(f\"Mode {mode} not implemented\")\n\n    error[nan_idx] = np.nan\n\n    return new_mean_state, new_cov_state, error\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.calculate_mahalanobis_distance","title":"<code>calculate_mahalanobis_distance(new_observations, current_mean_state, current_cov_state, forecast_steps=1, return_statistics=False)</code>","text":"<p>Calculate the mahalanobis distance.</p> <p>Calculating the Mahalanobis distance which is defined as error.T @ covariance^(-1) @ error The error is flatted in row-major (C-style) This returns the stacked rows, which in our case is the errors per observation parameter stacked and this is exactly what we want: array([[1, 2], [3, 4]]).reshape((-1, 1), order='C') becomes column array([1, 2 3, 4]) Using a solve method instead of calculating inverse matrices directly When calculating mhd_per_obs_param we use the partial result and reshape the temporary output such that we can sum the correct elements associated with the same observable together When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance) terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively, we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new forecast gets increased. We set the error equal to zero for computational issues, but this does decrease the number of degrees of freedom for that particular Mahalanobis distance calculation, basically decreasing the Mahalanobis distance. We allow the option to output the number of degrees of freedom and chi2 statistic which allows to take this decrease in degrees of freedom into account.</p> <p>Parameters:</p> Name Type Description Default <code>new_observations</code> <code>ndarray</code> <p>New observations to use in the calculation of the mahalanobis distance of size [nof_observables x forecast_steps]</p> required <code>current_mean_state</code> <code>ndarray</code> <p>Current mean estimate for the state of size [nof_state_parameters x 1]</p> required <code>current_cov_state</code> <code>ndarray</code> <p>Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>forecast_steps</code> <code>int</code> <p>Number of steps ahead to forecast and use in the mahalanobis distance calculation</p> <code>1</code> <code>return_statistics</code> <code>bool</code> <p>Boolean to return used degrees of freedom and chi2 statistic</p> <code>False</code> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def calculate_mahalanobis_distance(\n    self,\n    new_observations: np.ndarray,\n    current_mean_state: np.ndarray,\n    current_cov_state: np.ndarray,\n    forecast_steps: int = 1,\n    return_statistics=False,\n) -&gt; Union[Tuple[float, np.ndarray], Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Calculate the mahalanobis distance.\n\n    Calculating the Mahalanobis distance which is defined as error.T @ covariance^(-1) @ error\n    The error is flatted in row-major (C-style) This returns the stacked rows, which in our case is the errors per\n    observation parameter stacked and this is exactly what we want: array([[1, 2], [3, 4]]).reshape((-1, 1),\n    order='C') becomes column array([1, 2 3, 4])\n    Using a solve method instead of calculating inverse matrices directly\n    When calculating mhd_per_obs_param we use the partial result and reshape the temporary output such that we can\n    sum the correct elements associated with the same observable together\n    When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n    such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n    terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n    we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n    forecast gets increased. We set the error equal to zero for computational issues, but this does decrease the\n    number of degrees of freedom for that particular Mahalanobis distance calculation, basically decreasing the\n    Mahalanobis distance. We allow the option to output the number of degrees of freedom and chi2 statistic which\n    allows to take this decrease in degrees of freedom into account.\n\n    Args:\n        new_observations (np.ndarray): New observations to use in the calculation of the mahalanobis distance of\n            size [nof_observables x forecast_steps]\n        current_mean_state (np.ndarray): Current mean estimate for the state of size [nof_state_parameters x 1]\n        current_cov_state (np.ndarray): Current covariance estimate for the state of size\n            [nof_state_parameters x nof_state_parameters]\n        forecast_steps (int, optional): Number of steps ahead to forecast and use in the mahalanobis distance\n            calculation\n        return_statistics (bool, optional): Boolean to return used degrees of freedom and chi2 statistic\n    Returns:\n        mhd_overall (float): mahalanobis distance over all observables\n        mhd_per_obs_param (np.ndarray): mahalanobis distance per observation parameter of size [nof_observables, 1]\n\n    \"\"\"\n    if forecast_steps &lt;= 0:\n        raise AttributeError(\"Forecast steps should be a positive integer\")\n\n    if new_observations.size / self.nof_observables != forecast_steps:\n        raise AttributeError(\"Sizes of new observations and forecast steps are not aligning\")\n\n    _, f_t_k = self.forecast_mean(current_mean_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n    if new_observations.shape != f_t_k.shape:\n        raise AttributeError(\"Dimensions of new_observations are not aligning with dimensions of forecast\")\n\n    error = np.subtract(new_observations, f_t_k).reshape((-1, 1), order=\"C\")\n\n    r_t_k, q_t_k = self.forecast_covariance(current_cov_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n    nan_bool = np.isnan(new_observations)\n    if np.any(nan_bool):\n        nan_idx = np.argwhere(nan_bool)\n        for value in nan_idx:\n            q_t_k[value[0], :, value[1]] -= self.v_matrix[value[0], :]\n            q_t_k[:, value[0], value[1]] -= self.v_matrix[:, value[0]]\n\n        q_t_k[nan_idx[:, 0], nan_idx[:, 0], nan_idx[:, 1]] = np.inf\n        error[nan_bool.reshape((-1, 1), order=\"C\")] = 0\n\n    if forecast_steps &gt; 1:\n        full_covariance = self.create_full_covariance(r_t_k=r_t_k, q_t_k=q_t_k, forecast_steps=forecast_steps)\n    else:\n        full_covariance = q_t_k[:, :, 0]\n\n    mhd_overall = mahalanobis_distance(error=error, cov_matrix=full_covariance)\n    mhd_per_obs_param = np.empty((self.nof_observables, 1))\n\n    for i_obs in range(self.nof_observables):\n        ind_hrz = np.array(range(forecast_steps)) + i_obs * forecast_steps\n        mhd_per_obs_param[i_obs] = mahalanobis_distance(\n            error=error[ind_hrz], cov_matrix=full_covariance[np.ix_(ind_hrz, ind_hrz)]\n        )\n\n    if self.nof_observables == 1:\n        mhd_per_obs_param = mhd_per_obs_param.item()\n\n    if return_statistics:\n        dof_per_obs_param = (nan_bool.shape[1] - np.count_nonzero(nan_bool, axis=1)).reshape(\n            self.nof_observables, 1\n        )\n        dof_overall = dof_per_obs_param.sum()\n        chi2_cdf_per_obs_param = chi2.cdf(mhd_per_obs_param.flatten(), dof_per_obs_param.flatten()).reshape(\n            self.nof_observables, 1\n        )\n        chi2_cdf_overall = chi2.cdf(mhd_overall, dof_overall)\n\n        return (\n            mhd_overall,\n            mhd_per_obs_param,\n            dof_overall,\n            dof_per_obs_param,\n            chi2_cdf_overall,\n            chi2_cdf_per_obs_param,\n        )\n\n    return mhd_overall, mhd_per_obs_param\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.create_full_covariance","title":"<code>create_full_covariance(r_t_k, q_t_k, forecast_steps)</code>","text":"<p>Helper function to construct the full covariance matrix.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast distributions) Theorem 4.2 and corollary 4.2 we construct the full covariance matrix. This full covariance matrix is the covariance matrix of all forecasted observations with respect to each other. Hence, it's COV[Y_{t+k}, Y_{t+j}] with j and k 1&lt;=j,k&lt;=forecast steps input argument and Y_{t+k} the k step ahead forecast of the observation at time t</p> <p>The matrix is build up using the different blocks for different covariances between observations i and j. The diagonals of each block are calculated first as q_t_k[i, j, :]. Next the i, j-th (lower triangular) entry of the m, n-th block is calculated as (F.T @ G^(i-j) r_t_k[:, :, j] @ F)[i, j] Next each upper triangular part of each lower diagonal block is calculated and next the entire upper triangular part of the full matrix is calculated</p> <p>Parameters:</p> Name Type Description Default <code>r_t_k</code> <code>array</code> <p>Forecast values of estimated prior state covariance of the size [nof_state_parameters x nof_state_parameters x forecast_steps]</p> required <code>q_t_k</code> <code>array</code> <p>Forecast values of estimated observation covariance of the size [nof_observables x nof_observables x forecast_steps]</p> required <code>forecast_steps</code> <code>int</code> <p>Maximum number of steps ahead to forecast and use all of those in the mahalanobis distance calculation</p> required <p>Returns:</p> Name Type Description <code>full_covariance</code> <code>array</code> <p>Full covariance matrix of all forecasted observations with respect to each other</p> <code>ndarray</code> <p>having size [(nof_observables * forecast_steps) X (nof_observables * forecast_steps)]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def create_full_covariance(self, r_t_k: np.ndarray, q_t_k: np.ndarray, forecast_steps: int) -&gt; np.ndarray:\n    \"\"\"Helper function to construct the full covariance matrix.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast distributions) Theorem 4.2 and corollary 4.2\n    we construct the full covariance matrix. This full covariance matrix is the covariance matrix of all forecasted\n    observations with respect to each other. Hence, it's COV[Y_{t+k}, Y_{t+j}] with j and k 1&lt;=j,k&lt;=forecast steps\n    input argument and Y_{t+k} the k step ahead forecast of the observation at time t\n\n    The matrix is build up using the different blocks for different covariances between observations i and j.\n    The diagonals of each block are calculated first as q_t_k[i, j, :].\n    Next the i, j-th (lower triangular) entry of the m, n-th block is calculated as\n    (F.T @ G^(i-j) r_t_k[:, :, j] @ F)[i, j]\n    Next each upper triangular part of each lower diagonal block is calculated and next the entire upper triangular\n    part of the full matrix is calculated\n\n    Args:\n        r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n            [nof_state_parameters x nof_state_parameters x forecast_steps]\n        q_t_k (np.array): Forecast values of estimated observation covariance of the size\n            [nof_observables x nof_observables x forecast_steps]\n        forecast_steps (int): Maximum number of steps ahead to forecast and use all of those in the mahalanobis\n            distance calculation\n\n    Returns:\n        full_covariance (np.array): Full covariance matrix of all forecasted observations with respect to each other\n        having size [(nof_observables * forecast_steps) X (nof_observables * forecast_steps)]\n\n    \"\"\"\n    full_covariance = np.zeros((forecast_steps * self.nof_observables, forecast_steps * self.nof_observables))\n    base_idx = np.array(range(forecast_steps))\n    for block_i in range(self.nof_observables):\n        for block_j in range(block_i + 1):\n            block_rows = base_idx + block_i * forecast_steps\n            block_cols = base_idx + block_j * forecast_steps\n            full_covariance[block_rows, block_cols] = q_t_k[block_i, block_j, :]\n\n    temp_idx = np.array(range(self.nof_observables))\n    for sub_i in np.arange(start=1, stop=forecast_steps, step=1):\n        sub_row = temp_idx * forecast_steps + sub_i\n        for sub_j in range(sub_i):\n            sub_col = temp_idx * forecast_steps + sub_j\n            sub_idx = np.ix_(sub_row, sub_col)\n            full_covariance[sub_idx] = (\n                self.f_matrix.T @ self.g_power[:, :, sub_i - sub_j] @ r_t_k[:, :, sub_j] @ self.f_matrix\n            )\n\n    for block_i in range(self.nof_observables):\n        for block_j in range(block_i):\n            block_rows = base_idx + block_i * forecast_steps\n            block_cols = base_idx + block_j * forecast_steps\n            block_idx = np.ix_(block_rows, block_cols)\n            full_covariance[block_idx] = full_covariance[block_idx] + np.tril(full_covariance[block_idx], k=-1).T\n\n    full_covariance = np.tril(full_covariance) + np.tril(full_covariance, k=-1).T\n\n    return full_covariance\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.mahalanobis_distance","title":"<code>mahalanobis_distance(error, cov_matrix)</code>","text":"<p>Calculate Mahalanobis distance for multivariate observations.</p> <p>m = e.T @ inv(cov) @ e Sometimes the solution does not exist when np.inf value is present in cov_matrix (computational limitations?) Hence, we set it to a large value instead</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>ndarray</code> <p>n x p   observation error</p> required <code>cov_matrix</code> <code>ndarray</code> <p>p x p covariance matrix</p> required <p>Returns:</p> Type Description <code>float</code> <p>np.ndarray: n x 1  mahalanobis distance score for each observation</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def mahalanobis_distance(error: np.ndarray, cov_matrix: np.ndarray) -&gt; float:\n    \"\"\"Calculate Mahalanobis distance for multivariate observations.\n\n    m = e.T @ inv(cov) @ e\n    Sometimes the solution does not exist when np.inf value is present in cov_matrix (computational limitations?)\n    Hence, we set it to a large value instead\n\n    Args:\n        error (np.ndarray):  n x p   observation error\n        cov_matrix (np.ndarray): p x p covariance matrix\n\n    Returns:\n        np.ndarray: n x 1  mahalanobis distance score for each observation\n\n    \"\"\"\n    if cov_matrix.size == 1:\n        return error.item() ** 2 / cov_matrix.item()\n\n    partial_solution = np.linalg.solve(cov_matrix, error)\n    if np.any(np.isnan(partial_solution)):\n        cov_matrix[np.isinf(cov_matrix)] = 1e100\n        partial_solution = np.linalg.solve(cov_matrix, error)\n\n    return np.sum(error * partial_solution, axis=0).item()\n</code></pre>"},{"location":"pyelq/gas_species/","title":"Gas Species","text":""},{"location":"pyelq/gas_species/#gas-species","title":"Gas Species","text":"<p>Gas Species module.</p> <p>The superclass for the Gas species classes. It contains a few gas species with its properties and functionality to calculate the density of the gas and do emission rate conversions from m^3/s to kg/hr and back</p>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies","title":"<code>GasSpecies</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the properties of a particular gas species.</p> <p>Attributes:</p> Name Type Description <code>global_background</code> <code>float</code> <p>Global background concentration [ppm]</p> <code>half_life</code> <code>float</code> <p>Half life of gas [hr]</p> <code>__molar_gas_constant</code> <code>float</code> <p>R, molar gas constant [JK^-1mol^-1]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass GasSpecies(ABC):\n    \"\"\"Defines the properties of a particular gas species.\n\n    Attributes:\n        global_background (float, optional): Global background concentration [ppm]\n        half_life (float, optional): Half life of gas [hr]\n        __molar_gas_constant (float): R, molar gas constant [JK^-1mol^-1]\n\n    \"\"\"\n\n    global_background: float = field(init=False)\n    half_life: float = field(init=False)\n    __molar_gas_constant: float = 8.31446261815324\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Str: Name of gas.\"\"\"\n\n    @property\n    @abstractmethod\n    def molar_mass(self) -&gt; float:\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n\n    @property\n    @abstractmethod\n    def formula(self) -&gt; str:\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n\n    def gas_density(\n        self, temperature: Union[np.ndarray, float] = 273.15, pressure: Union[np.ndarray, float] = 101.325\n    ) -&gt; np.ndarray:\n        \"\"\"Calculating the density of the gas.\n\n        Calculating the density of the gas given temperature and pressure if temperature and pressure are not provided\n        we use Standard Temperature and Pressure (STP).\n\n        https://en.wikipedia.org/wiki/Ideal_gas_law\n\n        Args:\n            temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n                defaults to 273.15 [K]\n            pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n                defaults to 101.325 [kPa]\n\n        Returns:\n             density (np.ndarray): Array of gas density values [kg/m^3]\n\n        \"\"\"\n        specific_gas_constant = self.__molar_gas_constant / self.molar_mass\n        density = np.divide(pressure, (temperature * specific_gas_constant))\n        return density\n\n    def convert_emission_m3s_to_kghr(\n        self,\n        emission_m3s: Union[np.ndarray, float],\n        temperature: Union[np.ndarray, float] = 273.15,\n        pressure: Union[np.ndarray, float] = 101.325,\n    ) -&gt; np.ndarray:\n        \"\"\"Converting emission rates from m^3/s to kg/hr given temperature and pressure.\n\n         If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n        Args:\n            emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n            temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n                defaults to 273.15 [K]\n            pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n                defaults to 101.325 [kPa]\n\n        Returns:\n             emission_kghr (np.ndarray): [p x 1] array of emission rates in  [kg/hr]\n\n        \"\"\"\n        density = self.gas_density(temperature=temperature, pressure=pressure)\n        emission_kghr = np.multiply(emission_m3s, density) * 3600\n        return emission_kghr\n\n    def convert_emission_kghr_to_m3s(\n        self,\n        emission_kghr: Union[np.ndarray, float],\n        temperature: Union[np.ndarray, float] = 273.15,\n        pressure: Union[np.ndarray, float] = 101.325,\n    ) -&gt; np.ndarray:\n        \"\"\"Converting emission rates from  kg/hr to m^3/s given temperature and pressure.\n\n        If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n        Args:\n            emission_kghr (np.ndarray): Array of emission rates in  [kg/hr]\n            temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n                defaults to 273.15 [K]\n            pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n                defaults to 101.325 [kPa]\n\n        Returns:\n             emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n\n        \"\"\"\n        density = self.gas_density(temperature=temperature, pressure=pressure)\n        emission_m3s = np.divide(emission_kghr, density) / 3600\n        return emission_m3s\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.molar_mass","title":"<code>molar_mass</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.formula","title":"<code>formula</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.gas_density","title":"<code>gas_density(temperature=273.15, pressure=101.325)</code>","text":"<p>Calculating the density of the gas.</p> <p>Calculating the density of the gas given temperature and pressure if temperature and pressure are not provided we use Standard Temperature and Pressure (STP).</p> <p>https://en.wikipedia.org/wiki/Ideal_gas_law</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>Union[ndarray, float]</code> <p>Array of temperatures [Kelvin], defaults to 273.15 [K]</p> <code>273.15</code> <code>pressure</code> <code>Union[ndarray, float]</code> <p>Array of pressures [kPa], defaults to 101.325 [kPa]</p> <code>101.325</code> <p>Returns:</p> Name Type Description <code>density</code> <code>ndarray</code> <p>Array of gas density values [kg/m^3]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>def gas_density(\n    self, temperature: Union[np.ndarray, float] = 273.15, pressure: Union[np.ndarray, float] = 101.325\n) -&gt; np.ndarray:\n    \"\"\"Calculating the density of the gas.\n\n    Calculating the density of the gas given temperature and pressure if temperature and pressure are not provided\n    we use Standard Temperature and Pressure (STP).\n\n    https://en.wikipedia.org/wiki/Ideal_gas_law\n\n    Args:\n        temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n            defaults to 273.15 [K]\n        pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n            defaults to 101.325 [kPa]\n\n    Returns:\n         density (np.ndarray): Array of gas density values [kg/m^3]\n\n    \"\"\"\n    specific_gas_constant = self.__molar_gas_constant / self.molar_mass\n    density = np.divide(pressure, (temperature * specific_gas_constant))\n    return density\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.convert_emission_m3s_to_kghr","title":"<code>convert_emission_m3s_to_kghr(emission_m3s, temperature=273.15, pressure=101.325)</code>","text":"<p>Converting emission rates from m^3/s to kg/hr given temperature and pressure.</p> <p>If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).</p> <p>Parameters:</p> Name Type Description Default <code>emission_m3s</code> <code>Union[ndarray, float]</code> <p>Array of emission rates [m^3/s]</p> required <code>temperature</code> <code>Union[ndarray, float]</code> <p>Array of temperatures [Kelvin], defaults to 273.15 [K]</p> <code>273.15</code> <code>pressure</code> <code>Union[ndarray, float]</code> <p>Array of pressures [kPa], defaults to 101.325 [kPa]</p> <code>101.325</code> <p>Returns:</p> Name Type Description <code>emission_kghr</code> <code>ndarray</code> <p>[p x 1] array of emission rates in  [kg/hr]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>def convert_emission_m3s_to_kghr(\n    self,\n    emission_m3s: Union[np.ndarray, float],\n    temperature: Union[np.ndarray, float] = 273.15,\n    pressure: Union[np.ndarray, float] = 101.325,\n) -&gt; np.ndarray:\n    \"\"\"Converting emission rates from m^3/s to kg/hr given temperature and pressure.\n\n     If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n    Args:\n        emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n        temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n            defaults to 273.15 [K]\n        pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n            defaults to 101.325 [kPa]\n\n    Returns:\n         emission_kghr (np.ndarray): [p x 1] array of emission rates in  [kg/hr]\n\n    \"\"\"\n    density = self.gas_density(temperature=temperature, pressure=pressure)\n    emission_kghr = np.multiply(emission_m3s, density) * 3600\n    return emission_kghr\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.convert_emission_kghr_to_m3s","title":"<code>convert_emission_kghr_to_m3s(emission_kghr, temperature=273.15, pressure=101.325)</code>","text":"<p>Converting emission rates from  kg/hr to m^3/s given temperature and pressure.</p> <p>If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).</p> <p>Parameters:</p> Name Type Description Default <code>emission_kghr</code> <code>ndarray</code> <p>Array of emission rates in  [kg/hr]</p> required <code>temperature</code> <code>Union[ndarray, float]</code> <p>Array of temperatures [Kelvin], defaults to 273.15 [K]</p> <code>273.15</code> <code>pressure</code> <code>Union[ndarray, float]</code> <p>Array of pressures [kPa], defaults to 101.325 [kPa]</p> <code>101.325</code> <p>Returns:</p> Name Type Description <code>emission_m3s</code> <code>Union[ndarray, float]</code> <p>Array of emission rates [m^3/s]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>def convert_emission_kghr_to_m3s(\n    self,\n    emission_kghr: Union[np.ndarray, float],\n    temperature: Union[np.ndarray, float] = 273.15,\n    pressure: Union[np.ndarray, float] = 101.325,\n) -&gt; np.ndarray:\n    \"\"\"Converting emission rates from  kg/hr to m^3/s given temperature and pressure.\n\n    If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n    Args:\n        emission_kghr (np.ndarray): Array of emission rates in  [kg/hr]\n        temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n            defaults to 273.15 [K]\n        pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n            defaults to 101.325 [kPa]\n\n    Returns:\n         emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n\n    \"\"\"\n    density = self.gas_density(temperature=temperature, pressure=pressure)\n    emission_m3s = np.divide(emission_kghr, density) / 3600\n    return emission_m3s\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4","title":"<code>CH4</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of CH4.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass CH4(GasSpecies):\n    \"\"\"Defines the properties of CH4.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Methane\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 16.04246\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"CH4\"\n\n    global_background = 1.85\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6","title":"<code>C2H6</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of C2H6.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass C2H6(GasSpecies):\n    \"\"\"Defines the properties of C2H6.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Ethane\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 30.06904\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"C2H6\"\n\n    global_background = 5e-4\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8","title":"<code>C3H8</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of C3H8.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass C3H8(GasSpecies):\n    \"\"\"Defines the properties of C3H8.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Propane\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 46.0055\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"C3H8\"\n\n    global_background = 5e-4\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2","title":"<code>CO2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of CO2.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass CO2(GasSpecies):\n    \"\"\"Defines the properties of CO2.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Carbon Dioxide\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 44.0095\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"CO2\"\n\n    global_background = 400\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2","title":"<code>NO2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of NO2.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass NO2(GasSpecies):\n    \"\"\"Defines the properties of NO2.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Nitrogen Dioxide\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 46.0055\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"NO2\"\n\n    global_background = 0\n    half_life = 12\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.H2","title":"<code>H2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of H2.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass H2(GasSpecies):\n    \"\"\"Defines the properties of H2.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Hydrogen\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 2.01568\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"H2\"\n\n    global_background = 0.5\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.H2.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.H2.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.H2.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/model/","title":"Model","text":""},{"location":"pyelq/model/#model","title":"Model","text":"<p>ELQModel module.</p> <p>This module provides a class definition for the main functionalities of the codebase, providing the interface with the openMCMC repo and defining some plotting wrappers.</p>"},{"location":"pyelq/model/#pyelq.model.ELQModel","title":"<code>ELQModel</code>  <code>dataclass</code>","text":"<p>Class for setting up, running, and post-processing the full ELQModel analysis.</p> <p>Attributes:</p> Name Type Description <code>form</code> <code>dict</code> <p>dictionary detailing the form of the predictor for the concentration data. For details of the required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.</p> <code>transform</code> <code>dict</code> <p>dictionary detailing transformations applied to the model components. For details of the required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.</p> <code>model</code> <code>Model</code> <p>full model specification for the analysis, constructed in self.to_mcmc().</p> <code>mcmc</code> <code>MCMC</code> <p>MCMC object containing model and sampler specification for the problem. Constructed from the other components in self.to_mcmc().</p> <code>n_iter</code> <code>int</code> <p>number of MCMC iterations to be run.</p> <code>n_thin</code> <code>int</code> <p>number of iterations to thin by.</p> <code>fitted_values</code> <code>ndarray</code> <p>samples of fitted values (i.e. model predictions for the data) generated during the MCMC sampler. Attached in self.from_mcmc().</p> Source code in <code>src/pyelq/model.py</code> <pre><code>@dataclass\nclass ELQModel:\n    \"\"\"Class for setting up, running, and post-processing the full ELQModel analysis.\n\n    Attributes:\n        form (dict): dictionary detailing the form of the predictor for the concentration data. For details of the\n            required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.\n        transform (dict): dictionary detailing transformations applied to the model components. For details of the\n            required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.\n        model (Model): full model specification for the analysis, constructed in self.to_mcmc().\n        mcmc (MCMC): MCMC object containing model and sampler specification for the problem. Constructed from the\n            other components in self.to_mcmc().\n        n_iter (int): number of MCMC iterations to be run.\n        n_thin (int): number of iterations to thin by.\n        fitted_values (np.ndarray): samples of fitted values (i.e. model predictions for the data) generated during the\n            MCMC sampler. Attached in self.from_mcmc().\n\n    \"\"\"\n\n    form: dict = field(init=False)\n    transform: dict = field(init=False)\n    model: Model = field(init=False)\n    mcmc: MCMC = field(init=False)\n    n_iter: int = 1000\n    n_thin: int = 1\n    fitted_values: np.ndarray = field(init=False)\n\n    def __init__(\n        self,\n        sensor_object: SensorGroup,\n        meteorology: Union[Meteorology, MeteorologyGroup],\n        gas_species: GasSpecies,\n        background: Background = SpatioTemporalBackground(),\n        source_model: Union[list, SourceModel] = Normal(),\n        error_model: ErrorModel = BySensor(),\n        offset_model: PerSensor = None,\n    ):\n        \"\"\"Initialise the ELQModel model.\n\n        Model form is as follows:\n        y = A*s + b + d + e\n        where:\n        - y is the vector of observed concentration data (extracted from the sensor object).\n        - A*s is the source contribution (from the source model and dispersion model).\n        - b is from the background model.\n        - d is from the offset model.\n        - e is residual error term and var(e) comes from the error precision model.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (Union[Meteorology, MeteorologyGroup]): meteorology data.\n            gas_species (GasSpecies): gas species object.\n            background (Background): background model specification. Defaults to SpatioTemporalBackground().\n            source_model (Union[list, SourceModel]): source model specification. This can be a list of multiple\n            SourceModels or a single SourceModel. Defaults to Normal(). If a single SourceModel is used, it will\n            be converted to a list.\n            error_model (Precision): measurement precision model specification. Defaults to BySensor().\n            offset_model (PerSensor): offset model specification. Defaults to None.\n\n        \"\"\"\n        self.sensor_object = sensor_object\n        self.meteorology = meteorology\n        self.gas_species = gas_species\n        self.components = {\n            \"background\": background,\n            \"error_model\": error_model,\n            \"offset\": offset_model,\n        }\n\n        if source_model is not None:\n            if not isinstance(source_model, list):\n                source_model = [source_model]\n            for source in source_model:\n                if source.label_string is None:\n                    self.components[\"source\"] = source\n                else:\n                    self.components[\"source_\" + source.label_string] = source\n\n        if error_model is None:\n            self.components[\"error_model\"] = BySensor()\n            warnings.warn(\"None is not an allowed type for error_model: resetting to default BySensor model.\")\n        for key in list(self.components.keys()):\n            if self.components[key] is None:\n                self.components.pop(key)\n\n    def initialise(self):\n        \"\"\"Take data inputs and extract relevant properties.\"\"\"\n        self.form = {}\n        self.transform = {}\n        for key, component in self.components.items():\n\n            if \"background\" in key:\n                self.form[\"bg\"] = \"B_bg\"\n                self.transform[\"bg\"] = False\n            if re.match(\"source\", key):\n                source_component_map = component.map\n                self.transform[source_component_map[\"source\"]] = False\n                self.form[source_component_map[\"source\"]] = source_component_map[\"coupling_matrix\"]\n            if \"offset\" in key:\n                self.form[\"d\"] = \"B_d\"\n                self.transform[\"d\"] = False\n\n            self.components[key].initialise(self.sensor_object, self.meteorology, self.gas_species)\n\n    def to_mcmc(self):\n        \"\"\"Convert the ELQModel specification into an MCMC solver object that can be run.\n\n        Executing the following steps:\n            - Initialise the model object with the data likelihood (response distribution for y), and add all the\n                associated prior distributions, as specified by the model components.\n            - Initialise the state dictionary with the observed sensor data, and add parameters associated with all\n                the associated prior distributions, as specified by the model components.\n            - Initialise the MCMC sampler objects associated with each of the model components.\n            - Create the MCMC solver object, using all of the above information.\n\n        \"\"\"\n        response_precision = self.components[\"error_model\"].precision_parameter\n        model = [\n            location_scale.Normal(\n                \"y\",\n                mean=parameter.LinearCombinationWithTransform(self.form, self.transform),\n                precision=response_precision,\n            )\n        ]\n\n        initial_state = {\"y\": self.sensor_object.concentration}\n\n        for component in self.components.values():\n            model = component.make_model(model)\n            initial_state = component.make_state(initial_state)\n\n        self.model = Model(model, response={\"y\": \"mean\"})\n\n        sampler_list = []\n        for component in self.components.values():\n            sampler_list = component.make_sampler(self.model, sampler_list)\n\n        self.mcmc = MCMC(initial_state, sampler_list, self.model, n_burn=0, n_iter=self.n_iter, n_thin=self.n_thin)\n\n    def run_mcmc(self):\n        \"\"\"Run the mcmc function.\"\"\"\n        self.mcmc.run_mcmc()\n\n    def from_mcmc(self):\n        \"\"\"Extract information from MCMC solver class once its has run.\n\n        Performs two operations:\n            - For each of the components of the model: extracts the related sampled parameter values and attaches these\n                to the component class.\n            - For all keys in the mcmc.store dictionary: extracts the sampled parameter values from self.mcmc.store and\n                puts them into the equivalent fields in the state\n\n        \"\"\"\n        state = self.mcmc.state\n        for component in self.components.values():\n            component.from_mcmc(self.mcmc.store)\n        for key in self.mcmc.store:\n            state[key] = self.mcmc.store[key]\n\n        self.make_combined_source_model()\n\n    def make_combined_source_model(self):\n        \"\"\"Aggregate multiple individual source models into a single combined source model.\n\n        This function iterates through the existing source models stored in `self.components` and consolidates them\n        into a unified source model named `\"sources_combined\"`. This is particularly useful when multiple source\n        models are involved in an analysis, and a merged representation is required for visualization.\n\n        The combined source model is created as an instance of the `Normal` model, with the label string\n        \"sources_combined\" with the following attributes:\n        - emission_rate: concatenated across all source models.\n        - all_source_locations: concatenated across all source models.\n        - number_on_sources: derived by summing the individual source counts across all source models\n        - label_string: concatenated across all source models.\n        - individual_source_labels: concatenated across all source models.\n\n        Once combined, the `\"sources_combined\"` model is stored in the `self.components` dictionary for later use.\n\n        Raises:\n            ValueError: If the reference locations of the individual source models are inconsistent.\n            This is checked by comparing the reference latitude, longitude, and altitude of each source model.\n\n        \"\"\"\n        combined_model = Normal(label_string=\"sources_combined\")\n        emission_rate = np.empty((0, self.mcmc.n_iter))\n        all_source_locations_east = np.empty((0, self.mcmc.n_iter))\n        all_source_locations_north = np.empty((0, self.mcmc.n_iter))\n        all_source_locations_up = np.empty((0, self.mcmc.n_iter))\n        number_on_sources = np.empty((0, self.mcmc.n_iter))\n        label_string = []\n        individual_source_labels = []\n\n        ref_latitude = None\n        ref_longitude = None\n        ref_altitude = None\n        for key, component in self.components.items():\n            if re.match(\"source\", key):\n                comp_ref_latitude = component.all_source_locations.ref_latitude\n                comp_ref_longitude = component.all_source_locations.ref_longitude\n                comp_ref_altitude = component.all_source_locations.ref_altitude\n                if ref_latitude is None and ref_longitude is None and ref_altitude is None:\n                    ref_latitude = comp_ref_latitude\n                    ref_longitude = comp_ref_longitude\n                    ref_altitude = comp_ref_altitude\n                else:\n                    if (\n                        not np.isclose(ref_latitude, comp_ref_latitude)\n                        or not np.isclose(ref_longitude, comp_ref_longitude)\n                        or not np.isclose(ref_altitude, comp_ref_altitude)\n                    ):\n                        raise ValueError(\n                            f\"Inconsistent reference locations in component '{key}'. \"\n                            \"All source models must share the same reference location.\"\n                        )\n                emission_rate = np.concatenate((emission_rate, component.emission_rate))\n                number_on_sources = np.concatenate(\n                    (\n                        number_on_sources.reshape((-1, self.mcmc.n_iter)),\n                        component.number_on_sources.reshape(-1, self.mcmc.n_iter),\n                    ),\n                    axis=0,\n                )\n                label_string.append(component.label_string)\n                individual_source_labels.append(component.individual_source_labels)\n\n                all_source_locations_east = np.concatenate(\n                    (\n                        all_source_locations_east,\n                        component.all_source_locations.east.reshape((-1, self.mcmc.n_iter)),\n                    ),\n                    axis=0,\n                )\n                all_source_locations_north = np.concatenate(\n                    (\n                        all_source_locations_north,\n                        component.all_source_locations.north.reshape((-1, self.mcmc.n_iter)),\n                    ),\n                    axis=0,\n                )\n                all_source_locations_up = np.concatenate(\n                    (\n                        all_source_locations_up,\n                        component.all_source_locations.up.reshape((-1, self.mcmc.n_iter)),\n                    ),\n                    axis=0,\n                )\n\n        combined_model.all_source_locations = ENU(\n            ref_altitude=ref_altitude,\n            ref_latitude=ref_latitude,\n            ref_longitude=ref_longitude,\n            east=all_source_locations_east,\n            north=all_source_locations_north,\n            up=all_source_locations_up,\n        )\n\n        combined_model.emission_rate = emission_rate\n        combined_model.label_string = label_string\n        combined_model.number_on_sources = np.sum(number_on_sources, axis=0)\n        combined_model.individual_source_labels = [item for sublist in individual_source_labels for item in sublist]\n        self.components[\"sources_combined\"] = combined_model\n\n    def plot_log_posterior(self, burn_in_value: int, plot: Plot = Plot()) -&gt; Plot():\n        \"\"\"Plots the trace of the log posterior over the iterations of the MCMC.\n\n        Args:\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'log_posterior_plot'\n\n        \"\"\"\n        plot.plot_single_trace(object_to_plot=self.mcmc, burn_in=burn_in_value)\n        return plot\n\n    def plot_fitted_values(self, plot: Plot = Plot()) -&gt; Plot:\n        \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when possible.\n\n        Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n        measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n        Args:\n            plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with key 'fitted_values'\n\n        \"\"\"\n        plot.plot_fitted_values_per_sensor(\n            mcmc_object=self.mcmc, sensor_object=self.sensor_object, background_model=self.components[\"background\"]\n        )\n        return plot\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.__init__","title":"<code>__init__(sensor_object, meteorology, gas_species, background=SpatioTemporalBackground(), source_model=Normal(), error_model=BySensor(), offset_model=None)</code>","text":"<p>Initialise the ELQModel model.</p> <p>Model form is as follows: y = As + b + d + e where: - y is the vector of observed concentration data (extracted from the sensor object). - As is the source contribution (from the source model and dispersion model). - b is from the background model. - d is from the offset model. - e is residual error term and var(e) comes from the error precision model.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>Union[Meteorology, MeteorologyGroup]</code> <p>meteorology data.</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species object.</p> required <code>background</code> <code>Background</code> <p>background model specification. Defaults to SpatioTemporalBackground().</p> <code>SpatioTemporalBackground()</code> <code>source_model</code> <code>Union[list, SourceModel]</code> <p>source model specification. This can be a list of multiple</p> <code>Normal()</code> <code>error_model</code> <code>Precision</code> <p>measurement precision model specification. Defaults to BySensor().</p> <code>BySensor()</code> <code>offset_model</code> <code>PerSensor</code> <p>offset model specification. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/model.py</code> <pre><code>def __init__(\n    self,\n    sensor_object: SensorGroup,\n    meteorology: Union[Meteorology, MeteorologyGroup],\n    gas_species: GasSpecies,\n    background: Background = SpatioTemporalBackground(),\n    source_model: Union[list, SourceModel] = Normal(),\n    error_model: ErrorModel = BySensor(),\n    offset_model: PerSensor = None,\n):\n    \"\"\"Initialise the ELQModel model.\n\n    Model form is as follows:\n    y = A*s + b + d + e\n    where:\n    - y is the vector of observed concentration data (extracted from the sensor object).\n    - A*s is the source contribution (from the source model and dispersion model).\n    - b is from the background model.\n    - d is from the offset model.\n    - e is residual error term and var(e) comes from the error precision model.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (Union[Meteorology, MeteorologyGroup]): meteorology data.\n        gas_species (GasSpecies): gas species object.\n        background (Background): background model specification. Defaults to SpatioTemporalBackground().\n        source_model (Union[list, SourceModel]): source model specification. This can be a list of multiple\n        SourceModels or a single SourceModel. Defaults to Normal(). If a single SourceModel is used, it will\n        be converted to a list.\n        error_model (Precision): measurement precision model specification. Defaults to BySensor().\n        offset_model (PerSensor): offset model specification. Defaults to None.\n\n    \"\"\"\n    self.sensor_object = sensor_object\n    self.meteorology = meteorology\n    self.gas_species = gas_species\n    self.components = {\n        \"background\": background,\n        \"error_model\": error_model,\n        \"offset\": offset_model,\n    }\n\n    if source_model is not None:\n        if not isinstance(source_model, list):\n            source_model = [source_model]\n        for source in source_model:\n            if source.label_string is None:\n                self.components[\"source\"] = source\n            else:\n                self.components[\"source_\" + source.label_string] = source\n\n    if error_model is None:\n        self.components[\"error_model\"] = BySensor()\n        warnings.warn(\"None is not an allowed type for error_model: resetting to default BySensor model.\")\n    for key in list(self.components.keys()):\n        if self.components[key] is None:\n            self.components.pop(key)\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.initialise","title":"<code>initialise()</code>","text":"<p>Take data inputs and extract relevant properties.</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def initialise(self):\n    \"\"\"Take data inputs and extract relevant properties.\"\"\"\n    self.form = {}\n    self.transform = {}\n    for key, component in self.components.items():\n\n        if \"background\" in key:\n            self.form[\"bg\"] = \"B_bg\"\n            self.transform[\"bg\"] = False\n        if re.match(\"source\", key):\n            source_component_map = component.map\n            self.transform[source_component_map[\"source\"]] = False\n            self.form[source_component_map[\"source\"]] = source_component_map[\"coupling_matrix\"]\n        if \"offset\" in key:\n            self.form[\"d\"] = \"B_d\"\n            self.transform[\"d\"] = False\n\n        self.components[key].initialise(self.sensor_object, self.meteorology, self.gas_species)\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.to_mcmc","title":"<code>to_mcmc()</code>","text":"<p>Convert the ELQModel specification into an MCMC solver object that can be run.</p> Executing the following steps <ul> <li>Initialise the model object with the data likelihood (response distribution for y), and add all the     associated prior distributions, as specified by the model components.</li> <li>Initialise the state dictionary with the observed sensor data, and add parameters associated with all     the associated prior distributions, as specified by the model components.</li> <li>Initialise the MCMC sampler objects associated with each of the model components.</li> <li>Create the MCMC solver object, using all of the above information.</li> </ul> Source code in <code>src/pyelq/model.py</code> <pre><code>def to_mcmc(self):\n    \"\"\"Convert the ELQModel specification into an MCMC solver object that can be run.\n\n    Executing the following steps:\n        - Initialise the model object with the data likelihood (response distribution for y), and add all the\n            associated prior distributions, as specified by the model components.\n        - Initialise the state dictionary with the observed sensor data, and add parameters associated with all\n            the associated prior distributions, as specified by the model components.\n        - Initialise the MCMC sampler objects associated with each of the model components.\n        - Create the MCMC solver object, using all of the above information.\n\n    \"\"\"\n    response_precision = self.components[\"error_model\"].precision_parameter\n    model = [\n        location_scale.Normal(\n            \"y\",\n            mean=parameter.LinearCombinationWithTransform(self.form, self.transform),\n            precision=response_precision,\n        )\n    ]\n\n    initial_state = {\"y\": self.sensor_object.concentration}\n\n    for component in self.components.values():\n        model = component.make_model(model)\n        initial_state = component.make_state(initial_state)\n\n    self.model = Model(model, response={\"y\": \"mean\"})\n\n    sampler_list = []\n    for component in self.components.values():\n        sampler_list = component.make_sampler(self.model, sampler_list)\n\n    self.mcmc = MCMC(initial_state, sampler_list, self.model, n_burn=0, n_iter=self.n_iter, n_thin=self.n_thin)\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.run_mcmc","title":"<code>run_mcmc()</code>","text":"<p>Run the mcmc function.</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def run_mcmc(self):\n    \"\"\"Run the mcmc function.\"\"\"\n    self.mcmc.run_mcmc()\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.from_mcmc","title":"<code>from_mcmc()</code>","text":"<p>Extract information from MCMC solver class once its has run.</p> Performs two operations <ul> <li>For each of the components of the model: extracts the related sampled parameter values and attaches these     to the component class.</li> <li>For all keys in the mcmc.store dictionary: extracts the sampled parameter values from self.mcmc.store and     puts them into the equivalent fields in the state</li> </ul> Source code in <code>src/pyelq/model.py</code> <pre><code>def from_mcmc(self):\n    \"\"\"Extract information from MCMC solver class once its has run.\n\n    Performs two operations:\n        - For each of the components of the model: extracts the related sampled parameter values and attaches these\n            to the component class.\n        - For all keys in the mcmc.store dictionary: extracts the sampled parameter values from self.mcmc.store and\n            puts them into the equivalent fields in the state\n\n    \"\"\"\n    state = self.mcmc.state\n    for component in self.components.values():\n        component.from_mcmc(self.mcmc.store)\n    for key in self.mcmc.store:\n        state[key] = self.mcmc.store[key]\n\n    self.make_combined_source_model()\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.make_combined_source_model","title":"<code>make_combined_source_model()</code>","text":"<p>Aggregate multiple individual source models into a single combined source model.</p> <p>This function iterates through the existing source models stored in <code>self.components</code> and consolidates them into a unified source model named <code>\"sources_combined\"</code>. This is particularly useful when multiple source models are involved in an analysis, and a merged representation is required for visualization.</p> <p>The combined source model is created as an instance of the <code>Normal</code> model, with the label string \"sources_combined\" with the following attributes: - emission_rate: concatenated across all source models. - all_source_locations: concatenated across all source models. - number_on_sources: derived by summing the individual source counts across all source models - label_string: concatenated across all source models. - individual_source_labels: concatenated across all source models.</p> <p>Once combined, the <code>\"sources_combined\"</code> model is stored in the <code>self.components</code> dictionary for later use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reference locations of the individual source models are inconsistent.</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def make_combined_source_model(self):\n    \"\"\"Aggregate multiple individual source models into a single combined source model.\n\n    This function iterates through the existing source models stored in `self.components` and consolidates them\n    into a unified source model named `\"sources_combined\"`. This is particularly useful when multiple source\n    models are involved in an analysis, and a merged representation is required for visualization.\n\n    The combined source model is created as an instance of the `Normal` model, with the label string\n    \"sources_combined\" with the following attributes:\n    - emission_rate: concatenated across all source models.\n    - all_source_locations: concatenated across all source models.\n    - number_on_sources: derived by summing the individual source counts across all source models\n    - label_string: concatenated across all source models.\n    - individual_source_labels: concatenated across all source models.\n\n    Once combined, the `\"sources_combined\"` model is stored in the `self.components` dictionary for later use.\n\n    Raises:\n        ValueError: If the reference locations of the individual source models are inconsistent.\n        This is checked by comparing the reference latitude, longitude, and altitude of each source model.\n\n    \"\"\"\n    combined_model = Normal(label_string=\"sources_combined\")\n    emission_rate = np.empty((0, self.mcmc.n_iter))\n    all_source_locations_east = np.empty((0, self.mcmc.n_iter))\n    all_source_locations_north = np.empty((0, self.mcmc.n_iter))\n    all_source_locations_up = np.empty((0, self.mcmc.n_iter))\n    number_on_sources = np.empty((0, self.mcmc.n_iter))\n    label_string = []\n    individual_source_labels = []\n\n    ref_latitude = None\n    ref_longitude = None\n    ref_altitude = None\n    for key, component in self.components.items():\n        if re.match(\"source\", key):\n            comp_ref_latitude = component.all_source_locations.ref_latitude\n            comp_ref_longitude = component.all_source_locations.ref_longitude\n            comp_ref_altitude = component.all_source_locations.ref_altitude\n            if ref_latitude is None and ref_longitude is None and ref_altitude is None:\n                ref_latitude = comp_ref_latitude\n                ref_longitude = comp_ref_longitude\n                ref_altitude = comp_ref_altitude\n            else:\n                if (\n                    not np.isclose(ref_latitude, comp_ref_latitude)\n                    or not np.isclose(ref_longitude, comp_ref_longitude)\n                    or not np.isclose(ref_altitude, comp_ref_altitude)\n                ):\n                    raise ValueError(\n                        f\"Inconsistent reference locations in component '{key}'. \"\n                        \"All source models must share the same reference location.\"\n                    )\n            emission_rate = np.concatenate((emission_rate, component.emission_rate))\n            number_on_sources = np.concatenate(\n                (\n                    number_on_sources.reshape((-1, self.mcmc.n_iter)),\n                    component.number_on_sources.reshape(-1, self.mcmc.n_iter),\n                ),\n                axis=0,\n            )\n            label_string.append(component.label_string)\n            individual_source_labels.append(component.individual_source_labels)\n\n            all_source_locations_east = np.concatenate(\n                (\n                    all_source_locations_east,\n                    component.all_source_locations.east.reshape((-1, self.mcmc.n_iter)),\n                ),\n                axis=0,\n            )\n            all_source_locations_north = np.concatenate(\n                (\n                    all_source_locations_north,\n                    component.all_source_locations.north.reshape((-1, self.mcmc.n_iter)),\n                ),\n                axis=0,\n            )\n            all_source_locations_up = np.concatenate(\n                (\n                    all_source_locations_up,\n                    component.all_source_locations.up.reshape((-1, self.mcmc.n_iter)),\n                ),\n                axis=0,\n            )\n\n    combined_model.all_source_locations = ENU(\n        ref_altitude=ref_altitude,\n        ref_latitude=ref_latitude,\n        ref_longitude=ref_longitude,\n        east=all_source_locations_east,\n        north=all_source_locations_north,\n        up=all_source_locations_up,\n    )\n\n    combined_model.emission_rate = emission_rate\n    combined_model.label_string = label_string\n    combined_model.number_on_sources = np.sum(number_on_sources, axis=0)\n    combined_model.individual_source_labels = [item for sublist in individual_source_labels for item in sublist]\n    self.components[\"sources_combined\"] = combined_model\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.plot_log_posterior","title":"<code>plot_log_posterior(burn_in_value, plot=Plot())</code>","text":"<p>Plots the trace of the log posterior over the iterations of the MCMC.</p> <p>Parameters:</p> Name Type Description Default <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> <code>Plot()</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'log_posterior_plot'</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def plot_log_posterior(self, burn_in_value: int, plot: Plot = Plot()) -&gt; Plot():\n    \"\"\"Plots the trace of the log posterior over the iterations of the MCMC.\n\n    Args:\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'log_posterior_plot'\n\n    \"\"\"\n    plot.plot_single_trace(object_to_plot=self.mcmc, burn_in=burn_in_value)\n    return plot\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.plot_fitted_values","title":"<code>plot_fitted_values(plot=Plot())</code>","text":"<p>Plot the fitted values from the mcmc object against time, also shows the estimated background when possible.</p> <p>Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> <code>Plot()</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'fitted_values'</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def plot_fitted_values(self, plot: Plot = Plot()) -&gt; Plot:\n    \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when possible.\n\n    Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n    measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n    Args:\n        plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with key 'fitted_values'\n\n    \"\"\"\n    plot.plot_fitted_values_per_sensor(\n        mcmc_object=self.mcmc, sensor_object=self.sensor_object, background_model=self.components[\"background\"]\n    )\n    return plot\n</code></pre>"},{"location":"pyelq/preprocessing/","title":"Pre-Processing","text":""},{"location":"pyelq/preprocessing/#pre-processing","title":"Pre-processing","text":"<p>Class for performing preprocessing on the loaded data.</p>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor","title":"<code>Preprocessor</code>  <code>dataclass</code>","text":"<p>Class which implements generic functionality for pre-processing of sensor and meteorology information.</p> <p>Attributes:</p> Name Type Description <code>time_bin_edges</code> <code>Union[DatetimeArray, None]</code> <p>edges of the time bins to be used for</p> <code>sensor_object</code> <code>SensorGroup</code> <p>sensor group object containing raw data.</p> <code>met_object</code> <code>Meteorology</code> <p>met object containing raw data.</p> <code>aggregate_function</code> <code>str</code> <p>function to be used for aggregation of data. Defaults to mean.</p> <code>sensor_fields</code> <code>list</code> <p>standard list of sensor attributes that we wish to regularize and/or filter.</p> <code>met_fields</code> <code>list</code> <p>standard list of meteorology attributes that we wish to regularize/filter.</p> <code>is_regularized</code> <code>bool</code> <p>flag indicating whether the met and sensor data has been regularized.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>@dataclass\nclass Preprocessor:\n    \"\"\"Class which implements generic functionality for pre-processing of sensor and meteorology information.\n\n    Attributes:\n        time_bin_edges (Union[pd.arrays.DatetimeArray, None]): edges of the time bins to be used for\n        smoothing/interpolation. If None, no smoothing/interpolation is performed.\n        sensor_object (SensorGroup): sensor group object containing raw data.\n        met_object (Meteorology): met object containing raw data.\n        aggregate_function (str): function to be used for aggregation of data. Defaults to mean.\n        sensor_fields (list): standard list of sensor attributes that we wish to regularize and/or filter.\n        met_fields (list): standard list of meteorology attributes that we wish to regularize/filter.\n        is_regularized (bool): flag indicating whether the met and sensor data has been regularized.\n    \"\"\"\n\n    time_bin_edges: Union[pd.arrays.DatetimeArray, None]\n    sensor_object: SensorGroup\n    met_object: Union[Meteorology, MeteorologyGroup]\n    aggregate_function: str = \"mean\"\n    sensor_fields = [\"time\", \"concentration\", \"source_on\"]\n    met_fields = [\n        \"time\",\n        \"wind_direction\",\n        \"wind_speed\",\n        \"pressure\",\n        \"temperature\",\n        \"u_component\",\n        \"v_component\",\n        \"w_component\",\n        \"wind_turbulence_horizontal\",\n        \"wind_turbulence_vertical\",\n    ]\n    is_regularized: bool = field(init=False, default=False)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialise the class.\n\n        Attaching the sensor and meteorology objects as attributes, and running initial regularization and NaN filtering\n        steps. If time_bin_edges is provided, the data will be smoothed/interpolated onto the specified time grid and\n        NaN values will be filtered out. If time_bin_edges is None, the data will be filtered for NaN values only.\n\n        Before running the regularization (for the case where time_bin_edges is provided) &amp; NaN filtering, the function\n        ensures that u_component and v_component are present as fields on met_object. The post-smoothing wind speed and\n        direction are then calculated from the smoothed u and v components, to eliminate the need to take means of\n        directions when binning.\n\n        If time_bin_edges is provided, the sensor and meteorology group objects attached to the class will have\n        identical numbers of data points per device, identical time stamps, and be free of NaNs. If time_bin_edges is\n        None, the sensor and meteorology group objects will not have identical time stamps, but will be free of NaNs.\n        The time stamps for the sensor objects and meteorology objects will be the original time stamps.\n\n        \"\"\"\n        self.met_object.calculate_uv_from_wind_speed_direction()\n        if self.time_bin_edges is not None:\n            self.regularize_data()\n\n        self.met_object.calculate_wind_direction_from_uv()\n        self.met_object.calculate_wind_speed_from_uv()\n        if self.is_regularized:\n            self.filter_nans()\n        else:\n            self.filter_nans_no_regularize()\n\n    def regularize_data(self) -&gt; None:\n        \"\"\"Smoothing or interpolation of data onto a common set of time points.\n\n        Function which takes in sensor and meteorology objects containing raw data (on original time points), and\n        smooths or interpolates these onto a common set of time points.\n\n        When a SensorGroup object is supplied, the function will return a SensorGroup object with the same number of\n        sensors. When a MeteorologyGroup object is supplied, the function will return a MeteorologyGroup object with the\n        same number of objects. When a Meteorology object is supplied, the function will return a MeteorologyGroup\n        object with the same number of objects as there is sensors in the SensorGroup object. The individual Meteorology\n        objects will be identical.\n\n        Assumes that sensor_object and met_object attributes contain the RAW data, on the original time stamps, as\n        loaded from file/API using the relevant data access class.\n\n        After the function has been run, the sensor and meteorology group objects attached to the class as attributes\n        will have identical time stamps, but may still contain NaNs.\n\n        \"\"\"\n        self.is_regularized = True\n        sensor_out = deepcopy(self.sensor_object)\n        for sns_new, sns_old in zip(sensor_out.values(), self.sensor_object.values()):\n            for field in self.sensor_fields:\n                if (field != \"time\") and (getattr(sns_old, field) is not None):\n                    time_out, resampled_values = temporal_resampling(\n                        sns_old.time, getattr(sns_old, field), self.time_bin_edges, self.aggregate_function\n                    )\n                    setattr(sns_new, field, resampled_values)\n            sns_new.time = time_out\n\n        met_out = MeteorologyGroup()\n        if isinstance(self.met_object, Meteorology):\n            single_met_object = self.interpolate_single_met_object(met_in_object=self.met_object)\n            for key in sensor_out.keys():\n                met_out[key] = single_met_object\n        else:\n            for key, temp_met_object in self.met_object.items():\n                met_out[key] = self.interpolate_single_met_object(met_in_object=temp_met_object)\n\n        self.sensor_object = sensor_out\n        self.met_object = met_out\n\n    def filter_nans(self) -&gt; None:\n        \"\"\"Filter out data points where any of the specified sensor or meteorology fields has a NaN value.\n\n        Assumes that sensor_object and met_object attributes have first been passed through the regularize_data\n        function, and thus have fields on aligned time grids.\n\n        Function first works through all sensor and meteorology fields and finds indices of all times where there is a\n        NaN value in any field. Then, it uses the resulting index to filter all fields.\n\n        The result of this function is that the sensor_object and met_object attributes of the class are updated, any\n        NaN values having been removed.\n\n        \"\"\"\n        for sns_key, met_key in zip(self.sensor_object, self.met_object):\n            sns_in = self.sensor_object[sns_key]\n            met_in = self.met_object[met_key]\n\n            filter_index_sensor = self.get_nan_filter_index(sns_in, self.sensor_fields)\n            filter_index_met = self.get_nan_filter_index(met_in, self.met_fields)\n            filter_index = np.logical_and(filter_index_sensor, filter_index_met)\n\n            self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, filter_index)\n            self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, filter_index)\n\n    def filter_nans_no_regularize(self) -&gt; None:\n        \"\"\"Filter out data points where any of the specified sensor or meteorology fields has a NaN value.\n\n        Function first works through all sensor and meteorology fields and finds indices of all times where there is a\n        NaN value in any field. Then, it uses the resulting index to filter all fields.\n        The sensor_object and met_object are not assumed to have the same time stamps and they are treated separately.\n\n        The result of this function is that the sensor_object and met_object attributes of the class are updated, any\n        NaN values having been removed.\n\n        \"\"\"\n        for sns_key in self.sensor_object:\n            sns_in = self.sensor_object[sns_key]\n            filter_index = self.get_nan_filter_index(sns_in, self.sensor_fields)\n            self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, filter_index)\n\n        if isinstance(self.met_object, Meteorology):\n            filter_index = self.get_nan_filter_index(self.met_object, self.met_fields)\n            self.met_object = self.filter_object_fields(self.met_object, self.met_fields, filter_index)\n        else:\n            raise TypeError(\"MeteorologyGroup not required in case with no regularization.\")\n\n    @staticmethod\n    def get_nan_filter_index(obj: Union[Sensor, Meteorology], field_list: list) -&gt; np.ndarray:\n        \"\"\"Get a index for a given object to be able to filter out on NaN values in listed fields.\n\n        Args:\n            obj: Sensor or Meteorology object.\n            field_list (list): list of field names to be checked for NaN values.\n\n        Returns:\n            filter_index (np.ndarray): boolean array indicating which indices do not have NaN values in\n                any of the specified fields.\n\n        \"\"\"\n        filter_index = np.ones(obj.nof_observations, dtype=bool)\n\n        for field_name in field_list:\n            if (field_name != \"time\") and (getattr(obj, field_name) is not None):\n                filter_index = np.logical_and(filter_index, np.logical_not(np.isnan(getattr(obj, field_name))))\n\n        return filter_index\n\n    def filter_on_met(self, filter_variable: list, lower_limit: list = None, upper_limit: list = None) -&gt; None:\n        \"\"\"Filter the supplied data on given properties of the meteorological data.\n\n        If self.is_regularized, the filtering is done on both sensor_object and met_object and assumes that the\n        SensorGroup and MeteorologyGroup objects attached as attributes have corresponding values (one per sensor\n        device), and have attributes that have been pre-smoothed/interpolated onto a common time grid per device.\n\n        If the data is not regularized, the filtering is done on the met_object only. In this case a MeteorologyGroup\n        is not allowed.\n\n        The result of this function is that the sensor_object and met_object attributes are updated with the filtered\n        versions.\n\n        Args:\n            filter_variable (list of str): list of meteorology variables that we wish to use for filtering.\n            lower_limit (list of float): list of lower limits associated with the variables in filter_variables.\n                Defaults to None.\n            upper_limit (list of float): list of upper limits associated with the variables in filter_variables.\n                Defaults to None.\n\n        \"\"\"\n        if lower_limit is None:\n            lower_limit = [-np.inf] * len(filter_variable)\n        if upper_limit is None:\n            upper_limit = [np.inf] * len(filter_variable)\n\n        if self.is_regularized:\n            for vrb, low, high in zip(filter_variable, lower_limit, upper_limit):\n                for sns_key, met_key in zip(self.sensor_object, self.met_object):\n                    sns_in = self.sensor_object[sns_key]\n                    met_in = self.met_object[met_key]\n                    index_keep = np.logical_and(getattr(met_in, vrb) &gt;= low, getattr(met_in, vrb) &lt;= high)\n                    self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, index_keep)\n                    self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, index_keep)\n        else:\n            if isinstance(self.met_object, MeteorologyGroup):\n                raise TypeError(\"MeteorologyGroup not required in case with no regularization.\")\n            for vrb, low, high in zip(filter_variable, lower_limit, upper_limit):\n                index_keep = np.logical_and(getattr(self.met_object, vrb) &gt;= low, getattr(self.met_object, vrb) &lt;= high)\n                self.met_object = self.filter_object_fields(self.met_object, self.met_fields, index_keep)\n\n    def block_data(\n        self, time_edges: pd.arrays.DatetimeArray, data_object: Union[SensorGroup, MeteorologyGroup]\n    ) -&gt; list:\n        \"\"\"Break the supplied data group objects into time-blocked chunks.\n\n        Returning a list of sensor and meteorology group objects per time chunk.\n\n        If there is no data for a given device in a particular period, then that device is simply dropped from the group\n        object in that block.\n\n        Either a SensorGroup or a MeteorologyGroup object can be supplied, and the list of blocked objects returned will\n        be of the same type.\n\n        Args:\n            time_edges (pd.Arrays.DatetimeArray): [(n_period + 1) x 1] array of edges of the time bins to be used for\n                dividing the data into blocks.\n            data_object (SensorGroup or MeteorologyGroup): data object containing either or meteorological data, to be\n                divided into blocks.\n\n        Returns:\n            data_list (list): list of [n_period x 1] data objects, each list element being either a SensorGroup or\n                MeteorologyGroup object (depending on the input) containing the data for the corresponding period.\n\n        \"\"\"\n        data_list = []\n        nof_periods = len(time_edges) - 1\n        if isinstance(data_object, SensorGroup):\n            field_list = self.sensor_fields\n        elif isinstance(data_object, MeteorologyGroup):\n            field_list = self.met_fields\n        else:\n            raise TypeError(\"Data input must be either a SensorGroup or MeteorologyGroup.\")\n\n        for k in range(nof_periods):\n            data_list.append(type(data_object)())\n            for key, dat in data_object.items():\n                idx_time = (dat.time &gt;= time_edges[k]) &amp; (dat.time &lt;= time_edges[k + 1])\n                if np.any(idx_time):\n                    data_list[-1][key] = deepcopy(dat)\n                    data_list[-1][key] = self.filter_object_fields(data_list[-1][key], field_list, idx_time)\n        return data_list\n\n    @staticmethod\n    def filter_object_fields(\n        data_object: Union[Sensor, Meteorology], fields: list, index: np.ndarray\n    ) -&gt; Union[Sensor, Meteorology]:\n        \"\"\"Apply a filter index to all the fields in a given data object.\n\n        Can be used for either a Sensor or Meteorology object.\n\n        Args:\n            data_object (Union[Sensor, Meteorology]): sensor or meteorology object (corresponding to a single device)\n                for which fields are to be filtered.\n            fields (list): list of field names to be filtered.\n            index (np.ndarray): filter index.\n\n        Returns:\n            Union[Sensor, Meteorology]: filtered data object.\n\n        \"\"\"\n        return_object = deepcopy(data_object)\n        for field_name in fields:\n            if getattr(return_object, field_name) is not None:\n                setattr(return_object, field_name, getattr(return_object, field_name)[index])\n        return return_object\n\n    def interpolate_single_met_object(self, met_in_object: Meteorology) -&gt; Meteorology:\n        \"\"\"Interpolate a single Meteorology object onto the time grid of the class.\n\n        Args:\n            met_in_object (Meteorology): Meteorology object to be interpolated onto the time grid of the class.\n\n        Returns:\n            met_out_object (Meteorology): interpolated Meteorology object.\n\n        \"\"\"\n        met_out_object = Meteorology()\n        time_out = None\n        for field_name in self.met_fields:\n            if (field_name != \"time\") and (getattr(met_in_object, field_name) is not None):\n                time_out, resampled_values = temporal_resampling(\n                    met_in_object.time,\n                    getattr(met_in_object, field_name),\n                    self.time_bin_edges,\n                    self.aggregate_function,\n                )\n                setattr(met_out_object, field_name, resampled_values)\n\n        if time_out is not None:\n            met_out_object.time = time_out\n\n        return met_out_object\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialise the class.</p> <p>Attaching the sensor and meteorology objects as attributes, and running initial regularization and NaN filtering steps. If time_bin_edges is provided, the data will be smoothed/interpolated onto the specified time grid and NaN values will be filtered out. If time_bin_edges is None, the data will be filtered for NaN values only.</p> <p>Before running the regularization (for the case where time_bin_edges is provided) &amp; NaN filtering, the function ensures that u_component and v_component are present as fields on met_object. The post-smoothing wind speed and direction are then calculated from the smoothed u and v components, to eliminate the need to take means of directions when binning.</p> <p>If time_bin_edges is provided, the sensor and meteorology group objects attached to the class will have identical numbers of data points per device, identical time stamps, and be free of NaNs. If time_bin_edges is None, the sensor and meteorology group objects will not have identical time stamps, but will be free of NaNs. The time stamps for the sensor objects and meteorology objects will be the original time stamps.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialise the class.\n\n    Attaching the sensor and meteorology objects as attributes, and running initial regularization and NaN filtering\n    steps. If time_bin_edges is provided, the data will be smoothed/interpolated onto the specified time grid and\n    NaN values will be filtered out. If time_bin_edges is None, the data will be filtered for NaN values only.\n\n    Before running the regularization (for the case where time_bin_edges is provided) &amp; NaN filtering, the function\n    ensures that u_component and v_component are present as fields on met_object. The post-smoothing wind speed and\n    direction are then calculated from the smoothed u and v components, to eliminate the need to take means of\n    directions when binning.\n\n    If time_bin_edges is provided, the sensor and meteorology group objects attached to the class will have\n    identical numbers of data points per device, identical time stamps, and be free of NaNs. If time_bin_edges is\n    None, the sensor and meteorology group objects will not have identical time stamps, but will be free of NaNs.\n    The time stamps for the sensor objects and meteorology objects will be the original time stamps.\n\n    \"\"\"\n    self.met_object.calculate_uv_from_wind_speed_direction()\n    if self.time_bin_edges is not None:\n        self.regularize_data()\n\n    self.met_object.calculate_wind_direction_from_uv()\n    self.met_object.calculate_wind_speed_from_uv()\n    if self.is_regularized:\n        self.filter_nans()\n    else:\n        self.filter_nans_no_regularize()\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.regularize_data","title":"<code>regularize_data()</code>","text":"<p>Smoothing or interpolation of data onto a common set of time points.</p> <p>Function which takes in sensor and meteorology objects containing raw data (on original time points), and smooths or interpolates these onto a common set of time points.</p> <p>When a SensorGroup object is supplied, the function will return a SensorGroup object with the same number of sensors. When a MeteorologyGroup object is supplied, the function will return a MeteorologyGroup object with the same number of objects. When a Meteorology object is supplied, the function will return a MeteorologyGroup object with the same number of objects as there is sensors in the SensorGroup object. The individual Meteorology objects will be identical.</p> <p>Assumes that sensor_object and met_object attributes contain the RAW data, on the original time stamps, as loaded from file/API using the relevant data access class.</p> <p>After the function has been run, the sensor and meteorology group objects attached to the class as attributes will have identical time stamps, but may still contain NaNs.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def regularize_data(self) -&gt; None:\n    \"\"\"Smoothing or interpolation of data onto a common set of time points.\n\n    Function which takes in sensor and meteorology objects containing raw data (on original time points), and\n    smooths or interpolates these onto a common set of time points.\n\n    When a SensorGroup object is supplied, the function will return a SensorGroup object with the same number of\n    sensors. When a MeteorologyGroup object is supplied, the function will return a MeteorologyGroup object with the\n    same number of objects. When a Meteorology object is supplied, the function will return a MeteorologyGroup\n    object with the same number of objects as there is sensors in the SensorGroup object. The individual Meteorology\n    objects will be identical.\n\n    Assumes that sensor_object and met_object attributes contain the RAW data, on the original time stamps, as\n    loaded from file/API using the relevant data access class.\n\n    After the function has been run, the sensor and meteorology group objects attached to the class as attributes\n    will have identical time stamps, but may still contain NaNs.\n\n    \"\"\"\n    self.is_regularized = True\n    sensor_out = deepcopy(self.sensor_object)\n    for sns_new, sns_old in zip(sensor_out.values(), self.sensor_object.values()):\n        for field in self.sensor_fields:\n            if (field != \"time\") and (getattr(sns_old, field) is not None):\n                time_out, resampled_values = temporal_resampling(\n                    sns_old.time, getattr(sns_old, field), self.time_bin_edges, self.aggregate_function\n                )\n                setattr(sns_new, field, resampled_values)\n        sns_new.time = time_out\n\n    met_out = MeteorologyGroup()\n    if isinstance(self.met_object, Meteorology):\n        single_met_object = self.interpolate_single_met_object(met_in_object=self.met_object)\n        for key in sensor_out.keys():\n            met_out[key] = single_met_object\n    else:\n        for key, temp_met_object in self.met_object.items():\n            met_out[key] = self.interpolate_single_met_object(met_in_object=temp_met_object)\n\n    self.sensor_object = sensor_out\n    self.met_object = met_out\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_nans","title":"<code>filter_nans()</code>","text":"<p>Filter out data points where any of the specified sensor or meteorology fields has a NaN value.</p> <p>Assumes that sensor_object and met_object attributes have first been passed through the regularize_data function, and thus have fields on aligned time grids.</p> <p>Function first works through all sensor and meteorology fields and finds indices of all times where there is a NaN value in any field. Then, it uses the resulting index to filter all fields.</p> <p>The result of this function is that the sensor_object and met_object attributes of the class are updated, any NaN values having been removed.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def filter_nans(self) -&gt; None:\n    \"\"\"Filter out data points where any of the specified sensor or meteorology fields has a NaN value.\n\n    Assumes that sensor_object and met_object attributes have first been passed through the regularize_data\n    function, and thus have fields on aligned time grids.\n\n    Function first works through all sensor and meteorology fields and finds indices of all times where there is a\n    NaN value in any field. Then, it uses the resulting index to filter all fields.\n\n    The result of this function is that the sensor_object and met_object attributes of the class are updated, any\n    NaN values having been removed.\n\n    \"\"\"\n    for sns_key, met_key in zip(self.sensor_object, self.met_object):\n        sns_in = self.sensor_object[sns_key]\n        met_in = self.met_object[met_key]\n\n        filter_index_sensor = self.get_nan_filter_index(sns_in, self.sensor_fields)\n        filter_index_met = self.get_nan_filter_index(met_in, self.met_fields)\n        filter_index = np.logical_and(filter_index_sensor, filter_index_met)\n\n        self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, filter_index)\n        self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, filter_index)\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_nans_no_regularize","title":"<code>filter_nans_no_regularize()</code>","text":"<p>Filter out data points where any of the specified sensor or meteorology fields has a NaN value.</p> <p>Function first works through all sensor and meteorology fields and finds indices of all times where there is a NaN value in any field. Then, it uses the resulting index to filter all fields. The sensor_object and met_object are not assumed to have the same time stamps and they are treated separately.</p> <p>The result of this function is that the sensor_object and met_object attributes of the class are updated, any NaN values having been removed.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def filter_nans_no_regularize(self) -&gt; None:\n    \"\"\"Filter out data points where any of the specified sensor or meteorology fields has a NaN value.\n\n    Function first works through all sensor and meteorology fields and finds indices of all times where there is a\n    NaN value in any field. Then, it uses the resulting index to filter all fields.\n    The sensor_object and met_object are not assumed to have the same time stamps and they are treated separately.\n\n    The result of this function is that the sensor_object and met_object attributes of the class are updated, any\n    NaN values having been removed.\n\n    \"\"\"\n    for sns_key in self.sensor_object:\n        sns_in = self.sensor_object[sns_key]\n        filter_index = self.get_nan_filter_index(sns_in, self.sensor_fields)\n        self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, filter_index)\n\n    if isinstance(self.met_object, Meteorology):\n        filter_index = self.get_nan_filter_index(self.met_object, self.met_fields)\n        self.met_object = self.filter_object_fields(self.met_object, self.met_fields, filter_index)\n    else:\n        raise TypeError(\"MeteorologyGroup not required in case with no regularization.\")\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.get_nan_filter_index","title":"<code>get_nan_filter_index(obj, field_list)</code>  <code>staticmethod</code>","text":"<p>Get a index for a given object to be able to filter out on NaN values in listed fields.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Sensor, Meteorology]</code> <p>Sensor or Meteorology object.</p> required <code>field_list</code> <code>list</code> <p>list of field names to be checked for NaN values.</p> required <p>Returns:</p> Name Type Description <code>filter_index</code> <code>ndarray</code> <p>boolean array indicating which indices do not have NaN values in any of the specified fields.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>@staticmethod\ndef get_nan_filter_index(obj: Union[Sensor, Meteorology], field_list: list) -&gt; np.ndarray:\n    \"\"\"Get a index for a given object to be able to filter out on NaN values in listed fields.\n\n    Args:\n        obj: Sensor or Meteorology object.\n        field_list (list): list of field names to be checked for NaN values.\n\n    Returns:\n        filter_index (np.ndarray): boolean array indicating which indices do not have NaN values in\n            any of the specified fields.\n\n    \"\"\"\n    filter_index = np.ones(obj.nof_observations, dtype=bool)\n\n    for field_name in field_list:\n        if (field_name != \"time\") and (getattr(obj, field_name) is not None):\n            filter_index = np.logical_and(filter_index, np.logical_not(np.isnan(getattr(obj, field_name))))\n\n    return filter_index\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_on_met","title":"<code>filter_on_met(filter_variable, lower_limit=None, upper_limit=None)</code>","text":"<p>Filter the supplied data on given properties of the meteorological data.</p> <p>If self.is_regularized, the filtering is done on both sensor_object and met_object and assumes that the SensorGroup and MeteorologyGroup objects attached as attributes have corresponding values (one per sensor device), and have attributes that have been pre-smoothed/interpolated onto a common time grid per device.</p> <p>If the data is not regularized, the filtering is done on the met_object only. In this case a MeteorologyGroup is not allowed.</p> <p>The result of this function is that the sensor_object and met_object attributes are updated with the filtered versions.</p> <p>Parameters:</p> Name Type Description Default <code>filter_variable</code> <code>list of str</code> <p>list of meteorology variables that we wish to use for filtering.</p> required <code>lower_limit</code> <code>list of float</code> <p>list of lower limits associated with the variables in filter_variables. Defaults to None.</p> <code>None</code> <code>upper_limit</code> <code>list of float</code> <p>list of upper limits associated with the variables in filter_variables. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def filter_on_met(self, filter_variable: list, lower_limit: list = None, upper_limit: list = None) -&gt; None:\n    \"\"\"Filter the supplied data on given properties of the meteorological data.\n\n    If self.is_regularized, the filtering is done on both sensor_object and met_object and assumes that the\n    SensorGroup and MeteorologyGroup objects attached as attributes have corresponding values (one per sensor\n    device), and have attributes that have been pre-smoothed/interpolated onto a common time grid per device.\n\n    If the data is not regularized, the filtering is done on the met_object only. In this case a MeteorologyGroup\n    is not allowed.\n\n    The result of this function is that the sensor_object and met_object attributes are updated with the filtered\n    versions.\n\n    Args:\n        filter_variable (list of str): list of meteorology variables that we wish to use for filtering.\n        lower_limit (list of float): list of lower limits associated with the variables in filter_variables.\n            Defaults to None.\n        upper_limit (list of float): list of upper limits associated with the variables in filter_variables.\n            Defaults to None.\n\n    \"\"\"\n    if lower_limit is None:\n        lower_limit = [-np.inf] * len(filter_variable)\n    if upper_limit is None:\n        upper_limit = [np.inf] * len(filter_variable)\n\n    if self.is_regularized:\n        for vrb, low, high in zip(filter_variable, lower_limit, upper_limit):\n            for sns_key, met_key in zip(self.sensor_object, self.met_object):\n                sns_in = self.sensor_object[sns_key]\n                met_in = self.met_object[met_key]\n                index_keep = np.logical_and(getattr(met_in, vrb) &gt;= low, getattr(met_in, vrb) &lt;= high)\n                self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, index_keep)\n                self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, index_keep)\n    else:\n        if isinstance(self.met_object, MeteorologyGroup):\n            raise TypeError(\"MeteorologyGroup not required in case with no regularization.\")\n        for vrb, low, high in zip(filter_variable, lower_limit, upper_limit):\n            index_keep = np.logical_and(getattr(self.met_object, vrb) &gt;= low, getattr(self.met_object, vrb) &lt;= high)\n            self.met_object = self.filter_object_fields(self.met_object, self.met_fields, index_keep)\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.block_data","title":"<code>block_data(time_edges, data_object)</code>","text":"<p>Break the supplied data group objects into time-blocked chunks.</p> <p>Returning a list of sensor and meteorology group objects per time chunk.</p> <p>If there is no data for a given device in a particular period, then that device is simply dropped from the group object in that block.</p> <p>Either a SensorGroup or a MeteorologyGroup object can be supplied, and the list of blocked objects returned will be of the same type.</p> <p>Parameters:</p> Name Type Description Default <code>time_edges</code> <code>DatetimeArray</code> <p>[(n_period + 1) x 1] array of edges of the time bins to be used for dividing the data into blocks.</p> required <code>data_object</code> <code>SensorGroup or MeteorologyGroup</code> <p>data object containing either or meteorological data, to be divided into blocks.</p> required <p>Returns:</p> Name Type Description <code>data_list</code> <code>list</code> <p>list of [n_period x 1] data objects, each list element being either a SensorGroup or MeteorologyGroup object (depending on the input) containing the data for the corresponding period.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def block_data(\n    self, time_edges: pd.arrays.DatetimeArray, data_object: Union[SensorGroup, MeteorologyGroup]\n) -&gt; list:\n    \"\"\"Break the supplied data group objects into time-blocked chunks.\n\n    Returning a list of sensor and meteorology group objects per time chunk.\n\n    If there is no data for a given device in a particular period, then that device is simply dropped from the group\n    object in that block.\n\n    Either a SensorGroup or a MeteorologyGroup object can be supplied, and the list of blocked objects returned will\n    be of the same type.\n\n    Args:\n        time_edges (pd.Arrays.DatetimeArray): [(n_period + 1) x 1] array of edges of the time bins to be used for\n            dividing the data into blocks.\n        data_object (SensorGroup or MeteorologyGroup): data object containing either or meteorological data, to be\n            divided into blocks.\n\n    Returns:\n        data_list (list): list of [n_period x 1] data objects, each list element being either a SensorGroup or\n            MeteorologyGroup object (depending on the input) containing the data for the corresponding period.\n\n    \"\"\"\n    data_list = []\n    nof_periods = len(time_edges) - 1\n    if isinstance(data_object, SensorGroup):\n        field_list = self.sensor_fields\n    elif isinstance(data_object, MeteorologyGroup):\n        field_list = self.met_fields\n    else:\n        raise TypeError(\"Data input must be either a SensorGroup or MeteorologyGroup.\")\n\n    for k in range(nof_periods):\n        data_list.append(type(data_object)())\n        for key, dat in data_object.items():\n            idx_time = (dat.time &gt;= time_edges[k]) &amp; (dat.time &lt;= time_edges[k + 1])\n            if np.any(idx_time):\n                data_list[-1][key] = deepcopy(dat)\n                data_list[-1][key] = self.filter_object_fields(data_list[-1][key], field_list, idx_time)\n    return data_list\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_object_fields","title":"<code>filter_object_fields(data_object, fields, index)</code>  <code>staticmethod</code>","text":"<p>Apply a filter index to all the fields in a given data object.</p> <p>Can be used for either a Sensor or Meteorology object.</p> <p>Parameters:</p> Name Type Description Default <code>data_object</code> <code>Union[Sensor, Meteorology]</code> <p>sensor or meteorology object (corresponding to a single device) for which fields are to be filtered.</p> required <code>fields</code> <code>list</code> <p>list of field names to be filtered.</p> required <code>index</code> <code>ndarray</code> <p>filter index.</p> required <p>Returns:</p> Type Description <code>Union[Sensor, Meteorology]</code> <p>Union[Sensor, Meteorology]: filtered data object.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>@staticmethod\ndef filter_object_fields(\n    data_object: Union[Sensor, Meteorology], fields: list, index: np.ndarray\n) -&gt; Union[Sensor, Meteorology]:\n    \"\"\"Apply a filter index to all the fields in a given data object.\n\n    Can be used for either a Sensor or Meteorology object.\n\n    Args:\n        data_object (Union[Sensor, Meteorology]): sensor or meteorology object (corresponding to a single device)\n            for which fields are to be filtered.\n        fields (list): list of field names to be filtered.\n        index (np.ndarray): filter index.\n\n    Returns:\n        Union[Sensor, Meteorology]: filtered data object.\n\n    \"\"\"\n    return_object = deepcopy(data_object)\n    for field_name in fields:\n        if getattr(return_object, field_name) is not None:\n            setattr(return_object, field_name, getattr(return_object, field_name)[index])\n    return return_object\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.interpolate_single_met_object","title":"<code>interpolate_single_met_object(met_in_object)</code>","text":"<p>Interpolate a single Meteorology object onto the time grid of the class.</p> <p>Parameters:</p> Name Type Description Default <code>met_in_object</code> <code>Meteorology</code> <p>Meteorology object to be interpolated onto the time grid of the class.</p> required <p>Returns:</p> Name Type Description <code>met_out_object</code> <code>Meteorology</code> <p>interpolated Meteorology object.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def interpolate_single_met_object(self, met_in_object: Meteorology) -&gt; Meteorology:\n    \"\"\"Interpolate a single Meteorology object onto the time grid of the class.\n\n    Args:\n        met_in_object (Meteorology): Meteorology object to be interpolated onto the time grid of the class.\n\n    Returns:\n        met_out_object (Meteorology): interpolated Meteorology object.\n\n    \"\"\"\n    met_out_object = Meteorology()\n    time_out = None\n    for field_name in self.met_fields:\n        if (field_name != \"time\") and (getattr(met_in_object, field_name) is not None):\n            time_out, resampled_values = temporal_resampling(\n                met_in_object.time,\n                getattr(met_in_object, field_name),\n                self.time_bin_edges,\n                self.aggregate_function,\n            )\n            setattr(met_out_object, field_name, resampled_values)\n\n    if time_out is not None:\n        met_out_object.time = time_out\n\n    return met_out_object\n</code></pre>"},{"location":"pyelq/source_map/","title":"Source Map","text":""},{"location":"pyelq/source_map/#source-map","title":"Source Map","text":"<p>SourceMap module.</p> <p>The class for the source maps used in pyELQ</p>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap","title":"<code>SourceMap</code>  <code>dataclass</code>","text":"<p>Defines SourceMap class.</p> <p>Attributes:</p> Name Type Description <code>location</code> <code>Coordinate</code> <p>Coordinate object specifying the potential source locations</p> <code>prior_value</code> <code>ndarray</code> <p>Array with prior values for each source</p> <code>inclusion_idx</code> <code>ndarray</code> <p>Array of lists containing indices of the observations of a corresponding sensor_object which are within the inclusion_radius of that particular source</p> <code>inclusion_n_obs</code> <code>list</code> <p>Array containing number of observations of a sensor_object within radius for each source</p> Source code in <code>src/pyelq/source_map.py</code> <pre><code>@dataclass\nclass SourceMap:\n    \"\"\"Defines SourceMap class.\n\n    Attributes:\n        location (Coordinate, optional): Coordinate object specifying the potential source locations\n        prior_value (np.ndarray, optional): Array with prior values for each source\n        inclusion_idx (np.ndarray, optional): Array of lists containing indices of the observations of a\n            corresponding sensor_object which are within the inclusion_radius of that particular source\n        inclusion_n_obs (list, optional): Array containing number of observations of a sensor_object within\n            radius for each source\n\n    \"\"\"\n\n    location: Coordinate = field(init=False, default=None)\n    prior_value: np.ndarray = None\n    inclusion_idx: np.ndarray = field(init=False, default=None)\n    inclusion_n_obs: np.ndarray = field(init=False, default=None)\n\n    @property\n    def nof_sources(self) -&gt; int:\n        \"\"\"Number of sources.\"\"\"\n        if self.location is None:\n            return 0\n        return self.location.nof_observations\n\n    def calculate_inclusion_idx(self, sensor_object: Sensor, inclusion_radius: Union[int, np.ndarray]) -&gt; None:\n        \"\"\"Find observation indices which are within specified radius of each source location.\n\n        This method takes the sensor object and for each source in the source_map object it calculates which\n        observations are within the specified radius.\n        When sensor_object location and sourcemap_object location are not of the same type, simply convert both to ECEF\n        and calculate inclusion indices accordingly.\n        The result is an array of lists which are the indices of the observations in sensor_object which are within the\n        specified radius. Result is stored in the corresponding attribute.\n        Also calculating number of observations in radius per source and storing result as a list in inclusion_n_obs\n        attribute\n        When a location attribute is in LLA we convert to ECEF for the inclusion radius to make sense\n\n        Args:\n            sensor_object (Sensor): Sensor object containing location information on the observations under\n                consideration\n            inclusion_radius (Union[float, np.ndarray], optional): Inclusion radius in [m] radius from source\n                for which we take observations into account\n\n        \"\"\"\n        sensor_kd_tree = sensor_object.location.to_ecef().create_tree()\n        source_points = self.location.to_ecef().to_array()\n\n        inclusion_idx = sensor_kd_tree.query_ball_point(source_points, inclusion_radius)\n        idx_array = np.array(inclusion_idx, dtype=object)\n        self.inclusion_idx = idx_array\n        self.inclusion_n_obs = np.array([len(value) for value in self.inclusion_idx])\n\n    def generate_sources(\n        self,\n        coordinate_object: Coordinate,\n        sourcemap_limits: np.ndarray,\n        sourcemap_type: str = \"central\",\n        nof_sources: int = 5,\n        grid_shape: Union[tuple, np.ndarray] = (5, 5, 1),\n    ) -&gt; None:\n        \"\"\"Generates source locations based on specified inputs.\n\n        The result gets stored in the location attribute\n\n        In grid_sphere we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad\n        and temp_lon_rad\n\n        Args:\n            coordinate_object (Coordinate): Empty coordinate object which specifies the coordinate class to populate\n                location with\n            sourcemap_limits (np.ndarray): Limits of the sourcemap on which to generate the sources of size [dim x 2]\n                if dim == 2 we assume the third dimension will be zeros. Assuming the units of the limits are defined in\n                the desired coordinate system\n            sourcemap_type (str, optional): Type of sourcemap to generate: central == 1 central source,\n                hypercube == nof_sources through a Latin Hypercube design, grid == grid of shape grid_shape\n                filled with sources, grid_sphere == grid of shape grid_shape taking into account a spherical spacing\n            nof_sources (int, optional): Number of sources to generate (used in 'hypercube' case)\n            grid_shape: (tuple, optional): Number of sources to generate in each dimension, total number of\n                sources will be the product of the entries of this tuple (used in 'grid' and 'grid_sphere' case)\n\n        \"\"\"\n        sourcemap_dimension = sourcemap_limits.shape[0]\n        if sourcemap_type == \"central\":\n            array = sourcemap_limits.mean(axis=1).reshape(1, sourcemap_dimension)\n        elif sourcemap_type == \"hypercube\":\n            array = make_latin_hypercube(bounds=sourcemap_limits, nof_samples=nof_sources)\n        elif sourcemap_type == \"grid\":\n            array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"rectangular\", shape=grid_shape)\n        elif sourcemap_type == \"grid_sphere\":\n            array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"spherical\", shape=grid_shape)\n        else:\n            raise NotImplementedError(\"Please provide a valid sourcemap type\")\n        coordinate_object.from_array(array=array)\n        self.location = coordinate_object\n</code></pre>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap.nof_sources","title":"<code>nof_sources</code>  <code>property</code>","text":"<p>Number of sources.</p>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap.calculate_inclusion_idx","title":"<code>calculate_inclusion_idx(sensor_object, inclusion_radius)</code>","text":"<p>Find observation indices which are within specified radius of each source location.</p> <p>This method takes the sensor object and for each source in the source_map object it calculates which observations are within the specified radius. When sensor_object location and sourcemap_object location are not of the same type, simply convert both to ECEF and calculate inclusion indices accordingly. The result is an array of lists which are the indices of the observations in sensor_object which are within the specified radius. Result is stored in the corresponding attribute. Also calculating number of observations in radius per source and storing result as a list in inclusion_n_obs attribute When a location attribute is in LLA we convert to ECEF for the inclusion radius to make sense</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Sensor object containing location information on the observations under consideration</p> required <code>inclusion_radius</code> <code>Union[float, ndarray]</code> <p>Inclusion radius in [m] radius from source for which we take observations into account</p> required Source code in <code>src/pyelq/source_map.py</code> <pre><code>def calculate_inclusion_idx(self, sensor_object: Sensor, inclusion_radius: Union[int, np.ndarray]) -&gt; None:\n    \"\"\"Find observation indices which are within specified radius of each source location.\n\n    This method takes the sensor object and for each source in the source_map object it calculates which\n    observations are within the specified radius.\n    When sensor_object location and sourcemap_object location are not of the same type, simply convert both to ECEF\n    and calculate inclusion indices accordingly.\n    The result is an array of lists which are the indices of the observations in sensor_object which are within the\n    specified radius. Result is stored in the corresponding attribute.\n    Also calculating number of observations in radius per source and storing result as a list in inclusion_n_obs\n    attribute\n    When a location attribute is in LLA we convert to ECEF for the inclusion radius to make sense\n\n    Args:\n        sensor_object (Sensor): Sensor object containing location information on the observations under\n            consideration\n        inclusion_radius (Union[float, np.ndarray], optional): Inclusion radius in [m] radius from source\n            for which we take observations into account\n\n    \"\"\"\n    sensor_kd_tree = sensor_object.location.to_ecef().create_tree()\n    source_points = self.location.to_ecef().to_array()\n\n    inclusion_idx = sensor_kd_tree.query_ball_point(source_points, inclusion_radius)\n    idx_array = np.array(inclusion_idx, dtype=object)\n    self.inclusion_idx = idx_array\n    self.inclusion_n_obs = np.array([len(value) for value in self.inclusion_idx])\n</code></pre>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap.generate_sources","title":"<code>generate_sources(coordinate_object, sourcemap_limits, sourcemap_type='central', nof_sources=5, grid_shape=(5, 5, 1))</code>","text":"<p>Generates source locations based on specified inputs.</p> <p>The result gets stored in the location attribute</p> <p>In grid_sphere we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad and temp_lon_rad</p> <p>Parameters:</p> Name Type Description Default <code>coordinate_object</code> <code>Coordinate</code> <p>Empty coordinate object which specifies the coordinate class to populate location with</p> required <code>sourcemap_limits</code> <code>ndarray</code> <p>Limits of the sourcemap on which to generate the sources of size [dim x 2] if dim == 2 we assume the third dimension will be zeros. Assuming the units of the limits are defined in the desired coordinate system</p> required <code>sourcemap_type</code> <code>str</code> <p>Type of sourcemap to generate: central == 1 central source, hypercube == nof_sources through a Latin Hypercube design, grid == grid of shape grid_shape filled with sources, grid_sphere == grid of shape grid_shape taking into account a spherical spacing</p> <code>'central'</code> <code>nof_sources</code> <code>int</code> <p>Number of sources to generate (used in 'hypercube' case)</p> <code>5</code> <code>grid_shape</code> <code>Union[tuple, ndarray]</code> <p>(tuple, optional): Number of sources to generate in each dimension, total number of sources will be the product of the entries of this tuple (used in 'grid' and 'grid_sphere' case)</p> <code>(5, 5, 1)</code> Source code in <code>src/pyelq/source_map.py</code> <pre><code>def generate_sources(\n    self,\n    coordinate_object: Coordinate,\n    sourcemap_limits: np.ndarray,\n    sourcemap_type: str = \"central\",\n    nof_sources: int = 5,\n    grid_shape: Union[tuple, np.ndarray] = (5, 5, 1),\n) -&gt; None:\n    \"\"\"Generates source locations based on specified inputs.\n\n    The result gets stored in the location attribute\n\n    In grid_sphere we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad\n    and temp_lon_rad\n\n    Args:\n        coordinate_object (Coordinate): Empty coordinate object which specifies the coordinate class to populate\n            location with\n        sourcemap_limits (np.ndarray): Limits of the sourcemap on which to generate the sources of size [dim x 2]\n            if dim == 2 we assume the third dimension will be zeros. Assuming the units of the limits are defined in\n            the desired coordinate system\n        sourcemap_type (str, optional): Type of sourcemap to generate: central == 1 central source,\n            hypercube == nof_sources through a Latin Hypercube design, grid == grid of shape grid_shape\n            filled with sources, grid_sphere == grid of shape grid_shape taking into account a spherical spacing\n        nof_sources (int, optional): Number of sources to generate (used in 'hypercube' case)\n        grid_shape: (tuple, optional): Number of sources to generate in each dimension, total number of\n            sources will be the product of the entries of this tuple (used in 'grid' and 'grid_sphere' case)\n\n    \"\"\"\n    sourcemap_dimension = sourcemap_limits.shape[0]\n    if sourcemap_type == \"central\":\n        array = sourcemap_limits.mean(axis=1).reshape(1, sourcemap_dimension)\n    elif sourcemap_type == \"hypercube\":\n        array = make_latin_hypercube(bounds=sourcemap_limits, nof_samples=nof_sources)\n    elif sourcemap_type == \"grid\":\n        array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"rectangular\", shape=grid_shape)\n    elif sourcemap_type == \"grid_sphere\":\n        array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"spherical\", shape=grid_shape)\n    else:\n        raise NotImplementedError(\"Please provide a valid sourcemap type\")\n    coordinate_object.from_array(array=array)\n    self.location = coordinate_object\n</code></pre>"},{"location":"pyelq/component/background/","title":"Background","text":""},{"location":"pyelq/component/background/#background","title":"Background","text":"<p>Model components for background modelling.</p>"},{"location":"pyelq/component/background/#pyelq.component.background.Background","title":"<code>Background</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code></p> <p>Superclass for background models.</p> <p>Attributes:</p> Name Type Description <code>n_obs</code> <code>int</code> <p>total number of observations in the background model (across all sensors).</p> <code>n_parameter</code> <code>int</code> <p>number of parameters in the background model</p> <code>bg</code> <code>ndarray</code> <p>array of sampled background values, populated in self.from_mcmc() after the MCMC run is completed.</p> <code>precision_scalar</code> <code>ndarray</code> <p>array of sampled background precision values, populated in self.from_mcmc() after the MCMC run is completed. Only populated if update_precision is True.</p> <code>precision_matrix</code> <code>Union[ndarray, csr_array]</code> <p>un-scaled precision matrix for the background parameter vector.</p> <code>mean_bg</code> <code>float</code> <p>global mean background value. Should be populated from the value specified in the GasSpecies object.</p> <code>update_precision</code> <code>bool</code> <p>logical determining whether the background (scalar) precision parameter should be updated as part of the MCMC. Defaults to False.</p> <code>prior_precision_shape</code> <code>float</code> <p>shape parameter for the prior gamma distribution for the scalar precision parameter(s).</p> <code>prior_precision_rate</code> <code>float</code> <p>rate parameter for the prior gamma distribution for the scalar precision parameter(s).</p> <code>initial_precision</code> <code>float</code> <p>initial value for the scalar precision parameter.</p> <code>basis_matrix</code> <code>csr_array</code> <p>[n_obs x n_time] matrix mapping the background model parameters on to the observations.</p> <code>precision_time_0</code> <code>float</code> <p>precision relating to the first time stamp in the model. Defaults to 0.01.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>@dataclass\nclass Background(Component):\n    \"\"\"Superclass for background models.\n\n    Attributes:\n        n_obs (int): total number of observations in the background model (across all sensors).\n        n_parameter (int): number of parameters in the background model\n        bg (np.ndarray): array of sampled background values, populated in self.from_mcmc() after the MCMC run is\n            completed.\n        precision_scalar (np.ndarray): array of sampled background precision values, populated in self.from_mcmc() after\n            the MCMC run is completed. Only populated if update_precision is True.\n        precision_matrix (Union[np.ndarray, sparse.csr_array]): un-scaled precision matrix for the background parameter\n            vector.\n        mean_bg (float): global mean background value. Should be populated from the value specified in the GasSpecies\n            object.\n        update_precision (bool): logical determining whether the background (scalar) precision parameter should be\n            updated as part of the MCMC. Defaults to False.\n        prior_precision_shape (float): shape parameter for the prior gamma distribution for the scalar precision\n            parameter(s).\n        prior_precision_rate (float): rate parameter for the prior gamma distribution for the scalar precision\n            parameter(s).\n        initial_precision (float): initial value for the scalar precision parameter.\n        basis_matrix (sparse.csr_array): [n_obs x n_time] matrix mapping the background model parameters on to the\n            observations.\n        precision_time_0 (float): precision relating to the first time stamp in the model. Defaults to 0.01.\n\n    \"\"\"\n\n    n_obs: int = field(init=False)\n    n_parameter: int = field(init=False)\n    bg: np.ndarray = field(init=False)\n    precision_scalar: np.ndarray = field(init=False)\n    precision_matrix: Union[np.ndarray, sparse.csc_matrix] = field(init=False)\n    mean_bg: Union[float, None] = None\n    update_precision: bool = False\n    prior_precision_shape: float = 1e-3\n    prior_precision_rate: float = 1e-3\n    initial_precision: float = 1.0\n    basis_matrix: sparse.csr_array = field(init=False)\n    precision_time_0: float = field(init=False, default=0.01)\n\n    @abstractmethod\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n\n    def make_model(self, model: list = None) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to None.\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n        bg_precision_predictor = parameter.ScaledMatrix(matrix=\"P_bg\", scalar=\"lambda_bg\")\n        model.append(Normal(\"bg\", mean=\"mu_bg\", precision=bg_precision_predictor))\n        if self.update_precision:\n            model.append(Gamma(\"lambda_bg\", shape=\"a_lam_bg\", rate=\"b_lam_bg\"))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalNormal(\"bg\", model))\n        if self.update_precision:\n            sampler_list.append(NormalGamma(\"lambda_bg\", model))\n        return sampler_list\n\n    def make_state(self, state: dict = None) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to None.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n        state[\"mu_bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n        state[\"B_bg\"] = self.basis_matrix\n        state[\"bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n        state[\"P_bg\"] = self.precision_matrix\n        state[\"lambda_bg\"] = self.initial_precision\n        if self.update_precision:\n            state[\"a_lam_bg\"] = self.prior_precision_shape\n            state[\"b_lam_bg\"] = self.prior_precision_rate\n        return state\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.bg = store[\"bg\"]\n        if self.update_precision:\n            self.precision_scalar = store[\"lambda_bg\"]\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>  <code>abstractmethod</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>@abstractmethod\ndef initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.make_model","title":"<code>make_model(model=None)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_model(self, model: list = None) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to None.\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n    bg_precision_predictor = parameter.ScaledMatrix(matrix=\"P_bg\", scalar=\"lambda_bg\")\n    model.append(Normal(\"bg\", mean=\"mu_bg\", precision=bg_precision_predictor))\n    if self.update_precision:\n        model.append(Gamma(\"lambda_bg\", shape=\"a_lam_bg\", rate=\"b_lam_bg\"))\n    return model\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.make_sampler","title":"<code>make_sampler(model, sampler_list=None)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalNormal(\"bg\", model))\n    if self.update_precision:\n        sampler_list.append(NormalGamma(\"lambda_bg\", model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.make_state","title":"<code>make_state(state=None)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_state(self, state: dict = None) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to None.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n    state[\"mu_bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n    state[\"B_bg\"] = self.basis_matrix\n    state[\"bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n    state[\"P_bg\"] = self.precision_matrix\n    state[\"lambda_bg\"] = self.initial_precision\n    if self.update_precision:\n        state[\"a_lam_bg\"] = self.prior_precision_shape\n        state[\"b_lam_bg\"] = self.prior_precision_rate\n    return state\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.bg = store[\"bg\"]\n    if self.update_precision:\n        self.precision_scalar = store[\"lambda_bg\"]\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.TemporalBackground","title":"<code>TemporalBackground</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Background</code></p> <p>Model which imposes only temporal correlation on the background parameters.</p> <p>Assumes that the prior mean concentration of the background at every location/time point is the global average background concentration as defined in the input GasSpecies object.</p> <p>Generates the (un-scaled) prior background precision matrix using the function gmrf.precision_temporal: this precision matrix imposes first-oder Markov structure for the temporal dependence.</p> <p>By default, the times used for the model definition are the set of unique times in the observation set.</p> <p>This background model only requires the initialise function, and does not require the implementation of any further methods.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>Union[ndarray, DatetimeArray]</code> <p>vector of times used in defining the model.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>@dataclass\nclass TemporalBackground(Background):\n    \"\"\"Model which imposes only temporal correlation on the background parameters.\n\n    Assumes that the prior mean concentration of the background at every location/time point is the global average\n    background concentration as defined in the input GasSpecies object.\n\n    Generates the (un-scaled) prior background precision matrix using the function gmrf.precision_temporal: this\n    precision matrix imposes first-oder Markov structure for the temporal dependence.\n\n    By default, the times used for the model definition are the set of unique times in the observation set.\n\n    This background model only requires the initialise function, and does not require the implementation of any further\n    methods.\n\n    Attributes:\n        time (Union[np.ndarray, pd.arrays.DatetimeArray]): vector of times used in defining the model.\n\n    \"\"\"\n\n    time: Union[np.ndarray, pd.arrays.DatetimeArray] = field(init=False)\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Create temporal background model from sensor, meteorology and gas species inputs.\n\n        The precision matrix is made to be full rank by adjusting the precision at the first time point using the\n        precision_time_0 attribute.\n\n        Args:\n            sensor_object (SensorGroup): sensor data object.\n            meteorology (MeteorologyGroup): meteorology data object.\n            gas_species (GasSpecies): gas species data object.\n\n        \"\"\"\n        self.n_obs = sensor_object.nof_observations\n        self.time, unique_inverse = np.unique(sensor_object.time, return_inverse=True)\n        self.time = pd.array(self.time, dtype=\"datetime64[ns]\")\n        self.n_parameter = len(self.time)\n        self.basis_matrix = sparse.csr_array((np.ones(self.n_obs), (np.array(range(self.n_obs)), unique_inverse)))\n        self.precision_matrix = gmrf.precision_temporal(time=self.time)\n        lam = self.precision_matrix[0, 0]\n        self.precision_matrix[0, 0] = lam * (2.0 - lam / (self.precision_time_0 + lam))\n        if self.mean_bg is None:\n            self.mean_bg = gas_species.global_background\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.TemporalBackground.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Create temporal background model from sensor, meteorology and gas species inputs.</p> <p>The precision matrix is made to be full rank by adjusting the precision at the first time point using the precision_time_0 attribute.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data object.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data object.</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species data object.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Create temporal background model from sensor, meteorology and gas species inputs.\n\n    The precision matrix is made to be full rank by adjusting the precision at the first time point using the\n    precision_time_0 attribute.\n\n    Args:\n        sensor_object (SensorGroup): sensor data object.\n        meteorology (MeteorologyGroup): meteorology data object.\n        gas_species (GasSpecies): gas species data object.\n\n    \"\"\"\n    self.n_obs = sensor_object.nof_observations\n    self.time, unique_inverse = np.unique(sensor_object.time, return_inverse=True)\n    self.time = pd.array(self.time, dtype=\"datetime64[ns]\")\n    self.n_parameter = len(self.time)\n    self.basis_matrix = sparse.csr_array((np.ones(self.n_obs), (np.array(range(self.n_obs)), unique_inverse)))\n    self.precision_matrix = gmrf.precision_temporal(time=self.time)\n    lam = self.precision_matrix[0, 0]\n    self.precision_matrix[0, 0] = lam * (2.0 - lam / (self.precision_time_0 + lam))\n    if self.mean_bg is None:\n        self.mean_bg = gas_species.global_background\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground","title":"<code>SpatioTemporalBackground</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Background</code></p> <p>Model which imposes both spatial and temporal correlation on the background parameters.</p> <p>Defines a grid in time, and assumes a correlated time-series per sensor using the defined time grid.</p> <p>The background parameter is an [n_location * n_time x 1] (if self.spatial_dependence is True) or an [n_time x 1] vector (if self.spatial_dependence is False). In the spatio-temporal case, the background vector is assumed to unwrap over space and time as follows: bg = [b_1(t_1), b_2(t_1),..., b_n_lct(t_1),...,b_1(t_k),..., b_n_lct(t_k),...].T where n_lct is the number of sensor locations. This unwrapping mechanism is chosen as it greatly speeds up the sparse matrix operations in the solver (vs. the alternative).</p> <p>self.basis_matrix is set up to map the elements of the full background vector onto the observations, on the basis of spatial location and nearest time knot.</p> <p>The temporal background correlation is computed using gmrf.precision_temporal, and the spatial correlation is computed using a squared exponential correlation function, parametrized by self.spatial_correlation_param (spatial correlation, measured in metres). The full precision matrix is simply a Kronecker product between the two component precision matrices.</p> <p>Attributes:</p> Name Type Description <code>n_time</code> <code>int</code> <p>number of time knots for which the model is defined. Note that this does not need to be the same as the number of concentration observations in the analysis.</p> <code>n_location</code> <code>int</code> <p>number of spatial knots in the model.</p> <code>time</code> <code>DatetimeArray</code> <p>vector of times used in defining the model.</p> <code>spatial_dependence</code> <code>bool</code> <p>flag indicating whether the background parameters should be spatially correlated. If True, the model assumes a separate background time-series per sensor location, and assumes these time-series to be spatially correlated. If False (default), the background parameters are assumed to be common between sensors (only temporally correlated).</p> <code>spatial_correlation_param</code> <code>float</code> <p>correlation length parameter, determining the degree of spatial correlation imposed on the background time-series. Units are metres. Assumes equal correlation in all spatial directions. Defaults to 1.0.</p> <code>location</code> <code>ndarray</code> <p>[n_location x 3] array of sensor locations, used for calculating the spatial correlation between the sensor background values. If self.spatial_dependence is False, this attribute is simply set to be the location of the first sensor in the sensor object.</p> <code>temporal_precision_matrix</code> <code>Union[ndarray, csc_matrix]</code> <p>temporal component of the precision matrix. The full model precision matrix is the Kronecker product of this matrix with self.spatial_precision_matrix.</p> <code>spatial_precision_matrix</code> <code>ndarray</code> <p>spatial component of the precision matrix. The full model precision matrix is the Kronecker product of this matrix with the self.temporal_precision_matrix. Simply set to 1 if self.spatial_dependence is False.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>@dataclass\nclass SpatioTemporalBackground(Background):\n    \"\"\"Model which imposes both spatial and temporal correlation on the background parameters.\n\n    Defines a grid in time, and assumes a correlated time-series per sensor using the defined time grid.\n\n    The background parameter is an [n_location * n_time x 1] (if self.spatial_dependence is True) or an [n_time x 1]\n    vector (if self.spatial_dependence is False). In the spatio-temporal case, the background vector is assumed to\n    unwrap over space and time as follows:\n    bg = [b_1(t_1), b_2(t_1),..., b_n_lct(t_1),...,b_1(t_k),..., b_n_lct(t_k),...].T\n    where n_lct is the number of sensor locations.\n    This unwrapping mechanism is chosen as it greatly speeds up the sparse matrix operations in the solver (vs. the\n    alternative).\n\n    self.basis_matrix is set up to map the elements of the full background vector onto the observations, on the basis\n    of spatial location and nearest time knot.\n\n    The temporal background correlation is computed using gmrf.precision_temporal, and the spatial correlation is\n    computed using a squared exponential correlation function, parametrized by self.spatial_correlation_param (spatial\n    correlation, measured in metres). The full precision matrix is simply a Kronecker product between the two\n    component precision matrices.\n\n    Attributes:\n        n_time (int): number of time knots for which the model is defined. Note that this does not need to be the same\n            as the number of concentration observations in the analysis.\n        n_location (int): number of spatial knots in the model.\n        time (pd.arrays.DatetimeArray): vector of times used in defining the model.\n        spatial_dependence (bool): flag indicating whether the background parameters should be spatially correlated. If\n            True, the model assumes a separate background time-series per sensor location, and assumes these\n            time-series to be spatially correlated. If False (default), the background parameters are assumed to be\n            common between sensors (only temporally correlated).\n        spatial_correlation_param (float): correlation length parameter, determining the degree of spatial correlation\n            imposed on the background time-series. Units are metres. Assumes equal correlation in all spatial\n            directions. Defaults to 1.0.\n        location (np.ndarray): [n_location x 3] array of sensor locations, used for calculating the spatial correlation\n            between the sensor background values. If self.spatial_dependence is False, this attribute is simply set to\n            be the location of the first sensor in the sensor object.\n        temporal_precision_matrix (Union[np.ndarray, sparse.csc_matrix]): temporal component of the precision matrix.\n            The full model precision matrix is the Kronecker product of this matrix with self.spatial_precision_matrix.\n        spatial_precision_matrix (np.ndarray): spatial component of the precision matrix. The full model precision\n            matrix is the Kronecker product of this matrix with the self.temporal_precision_matrix. Simply set to 1 if\n            self.spatial_dependence is False.\n\n    \"\"\"\n\n    n_time: Union[int, None] = None\n    n_location: int = field(init=False)\n    time: pd.arrays.DatetimeArray = field(init=False)\n    spatial_dependence: bool = False\n    spatial_correlation_param: float = field(init=False, default=1.0)\n    location: Coordinate = field(init=False)\n    temporal_precision_matrix: Union[np.ndarray, sparse.csc_matrix] = field(init=False)\n    spatial_precision_matrix: np.ndarray = field(init=False)\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data wind data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n        self.make_temporal_knots(sensor_object)\n        self.make_spatial_knots(sensor_object)\n        self.n_parameter = self.n_time * self.n_location\n        self.n_obs = sensor_object.nof_observations\n\n        self.make_precision_matrix()\n        self.make_parameter_mapping(sensor_object)\n\n        if self.mean_bg is None:\n            self.mean_bg = gas_species.global_background\n\n    def make_parameter_mapping(self, sensor_object: SensorGroup):\n        \"\"\"Create the mapping of parameters onto observations, through creation of the associated basis matrix.\n\n        The background vector unwraps first over the spatial (sensor) location dimension, then over the temporal\n        dimension. For more detail, see the main class docstring.\n\n        The data vector in the solver state is assumed to consist of the individual sensor data vectors stacked\n        consecutively.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        nn_object = NearestNeighbors(n_neighbors=1, algorithm=\"kd_tree\").fit(self.time.to_numpy().reshape(-1, 1))\n        for k, sensor in enumerate(sensor_object.values()):\n            _, time_index = nn_object.kneighbors(sensor.time.to_numpy().reshape(-1, 1))\n            basis_matrix = sparse.csr_array(\n                (np.ones(sensor.nof_observations), (np.array(range(sensor.nof_observations)), time_index.flatten())),\n                shape=(sensor.nof_observations, self.n_time),\n            )\n            if self.spatial_dependence:\n                basis_matrix = sparse.kron(basis_matrix, np.eye(N=self.n_location, M=1, k=-k).T)\n\n            if k == 0:\n                self.basis_matrix = basis_matrix\n            else:\n                self.basis_matrix = sparse.vstack([self.basis_matrix, basis_matrix])\n\n    def make_temporal_knots(self, sensor_object: SensorGroup):\n        \"\"\"Create the temporal grid for the model.\n\n        If self.n_time is not specified, then the model will use the unique set of times from the sensor data.\n\n        If self.n_time is specified, then the model will define a time grid with self.n_time elements.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        if self.n_time is None:\n            self.time = pd.array(np.unique(sensor_object.time), dtype=\"datetime64[ns]\")\n            self.n_time = len(self.time)\n        else:\n            self.time = pd.date_range(\n                start=np.min(sensor_object.time), end=np.max(sensor_object.time), periods=self.n_time\n            ).array\n\n    def make_spatial_knots(self, sensor_object: SensorGroup):\n        \"\"\"Create the spatial grid for the model.\n\n        If self.spatial_dependence is False, the code assumes that only a single (arbitrary) location is used, thereby\n        eliminating any spatial dependence.\n\n        If self.spatial_dependence is True, a separate but correlated time-series of background parameters is assumed\n        for each sensor location.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        if self.spatial_dependence:\n            self.n_location = sensor_object.nof_sensors\n            self.get_locations_from_sensors(sensor_object)\n        else:\n            self.n_location = 1\n            self.location = sensor_object[list(sensor_object.keys())[0]].location\n\n    def make_precision_matrix(self):\n        \"\"\"Create the full precision matrix for the background parameters.\n\n        Defined as the Kronecker product of the temporal precision matrix and the spatial precision matrix.\n\n        The precision matrix is made to be full rank by adjusting the precision at the first time point using the\n        precision_time_0 attribute.\n\n        \"\"\"\n        self.temporal_precision_matrix = gmrf.precision_temporal(time=self.time)\n        lam = self.temporal_precision_matrix[0, 0]\n        self.temporal_precision_matrix[0, 0] = lam * (2.0 - lam / (self.precision_time_0 + lam))\n\n        if self.spatial_dependence:\n            self.make_spatial_precision_matrix()\n            self.precision_matrix = sparse.kron(self.temporal_precision_matrix, self.spatial_precision_matrix)\n        else:\n            self.precision_matrix = self.temporal_precision_matrix\n        if (self.n_parameter == 1) and sparse.issparse(self.precision_matrix):\n            self.precision_matrix = self.precision_matrix.toarray()\n\n    def make_spatial_precision_matrix(self):\n        \"\"\"Create the spatial precision matrix for the model.\n\n        The spatial precision matrix is simply calculated as the inverse of a squared exponential covariance matrix\n        calculated using the sensor locations.\n\n        \"\"\"\n        location_array = self.location.to_array()\n        spatial_covariance_matrix = np.exp(\n            -(1 / (2 * np.power(self.spatial_correlation_param, 2)))\n            * (\n                np.power(location_array[:, [0]] - location_array[:, [0]].T, 2)\n                + np.power(location_array[:, [1]] - location_array[:, [1]].T, 2)\n                + np.power(location_array[:, [2]] - location_array[:, [2]].T, 2)\n            )\n        )\n        self.spatial_precision_matrix = np.linalg.inv(\n            spatial_covariance_matrix + (1e-6) * np.eye(spatial_covariance_matrix.shape[0])\n        )\n\n    def get_locations_from_sensors(self, sensor_object: SensorGroup):\n        \"\"\"Extract the location information from the sensor object.\n\n        Attaches a Coordinate.ENU object as the self.location attribute, with all the sensor locations stored on the\n        same object.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        self.location = deepcopy(sensor_object[list(sensor_object.keys())[0]].location.to_enu())\n        self.location.east = np.full(shape=(self.n_location,), fill_value=np.nan)\n        self.location.north = np.full(shape=(self.n_location,), fill_value=np.nan)\n        self.location.up = np.full(shape=(self.n_location,), fill_value=np.nan)\n        for k, sensor in enumerate(sensor_object.values()):\n            if isinstance(sensor, Beam):\n                self.location.east[k] = np.mean(sensor.location.to_enu().east, axis=0)\n                self.location.north[k] = np.mean(sensor.location.to_enu().north, axis=0)\n                self.location.up[k] = np.mean(sensor.location.to_enu().up, axis=0)\n            else:\n                self.location.east[k] = sensor.location.to_enu().east.item()\n                self.location.north[k] = sensor.location.to_enu().north.item()\n                self.location.up[k] = sensor.location.to_enu().up.item()\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data wind data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data wind data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n    self.make_temporal_knots(sensor_object)\n    self.make_spatial_knots(sensor_object)\n    self.n_parameter = self.n_time * self.n_location\n    self.n_obs = sensor_object.nof_observations\n\n    self.make_precision_matrix()\n    self.make_parameter_mapping(sensor_object)\n\n    if self.mean_bg is None:\n        self.mean_bg = gas_species.global_background\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_parameter_mapping","title":"<code>make_parameter_mapping(sensor_object)</code>","text":"<p>Create the mapping of parameters onto observations, through creation of the associated basis matrix.</p> <p>The background vector unwraps first over the spatial (sensor) location dimension, then over the temporal dimension. For more detail, see the main class docstring.</p> <p>The data vector in the solver state is assumed to consist of the individual sensor data vectors stacked consecutively.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_parameter_mapping(self, sensor_object: SensorGroup):\n    \"\"\"Create the mapping of parameters onto observations, through creation of the associated basis matrix.\n\n    The background vector unwraps first over the spatial (sensor) location dimension, then over the temporal\n    dimension. For more detail, see the main class docstring.\n\n    The data vector in the solver state is assumed to consist of the individual sensor data vectors stacked\n    consecutively.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    nn_object = NearestNeighbors(n_neighbors=1, algorithm=\"kd_tree\").fit(self.time.to_numpy().reshape(-1, 1))\n    for k, sensor in enumerate(sensor_object.values()):\n        _, time_index = nn_object.kneighbors(sensor.time.to_numpy().reshape(-1, 1))\n        basis_matrix = sparse.csr_array(\n            (np.ones(sensor.nof_observations), (np.array(range(sensor.nof_observations)), time_index.flatten())),\n            shape=(sensor.nof_observations, self.n_time),\n        )\n        if self.spatial_dependence:\n            basis_matrix = sparse.kron(basis_matrix, np.eye(N=self.n_location, M=1, k=-k).T)\n\n        if k == 0:\n            self.basis_matrix = basis_matrix\n        else:\n            self.basis_matrix = sparse.vstack([self.basis_matrix, basis_matrix])\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_temporal_knots","title":"<code>make_temporal_knots(sensor_object)</code>","text":"<p>Create the temporal grid for the model.</p> <p>If self.n_time is not specified, then the model will use the unique set of times from the sensor data.</p> <p>If self.n_time is specified, then the model will define a time grid with self.n_time elements.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_temporal_knots(self, sensor_object: SensorGroup):\n    \"\"\"Create the temporal grid for the model.\n\n    If self.n_time is not specified, then the model will use the unique set of times from the sensor data.\n\n    If self.n_time is specified, then the model will define a time grid with self.n_time elements.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    if self.n_time is None:\n        self.time = pd.array(np.unique(sensor_object.time), dtype=\"datetime64[ns]\")\n        self.n_time = len(self.time)\n    else:\n        self.time = pd.date_range(\n            start=np.min(sensor_object.time), end=np.max(sensor_object.time), periods=self.n_time\n        ).array\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_spatial_knots","title":"<code>make_spatial_knots(sensor_object)</code>","text":"<p>Create the spatial grid for the model.</p> <p>If self.spatial_dependence is False, the code assumes that only a single (arbitrary) location is used, thereby eliminating any spatial dependence.</p> <p>If self.spatial_dependence is True, a separate but correlated time-series of background parameters is assumed for each sensor location.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_spatial_knots(self, sensor_object: SensorGroup):\n    \"\"\"Create the spatial grid for the model.\n\n    If self.spatial_dependence is False, the code assumes that only a single (arbitrary) location is used, thereby\n    eliminating any spatial dependence.\n\n    If self.spatial_dependence is True, a separate but correlated time-series of background parameters is assumed\n    for each sensor location.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    if self.spatial_dependence:\n        self.n_location = sensor_object.nof_sensors\n        self.get_locations_from_sensors(sensor_object)\n    else:\n        self.n_location = 1\n        self.location = sensor_object[list(sensor_object.keys())[0]].location\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_precision_matrix","title":"<code>make_precision_matrix()</code>","text":"<p>Create the full precision matrix for the background parameters.</p> <p>Defined as the Kronecker product of the temporal precision matrix and the spatial precision matrix.</p> <p>The precision matrix is made to be full rank by adjusting the precision at the first time point using the precision_time_0 attribute.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_precision_matrix(self):\n    \"\"\"Create the full precision matrix for the background parameters.\n\n    Defined as the Kronecker product of the temporal precision matrix and the spatial precision matrix.\n\n    The precision matrix is made to be full rank by adjusting the precision at the first time point using the\n    precision_time_0 attribute.\n\n    \"\"\"\n    self.temporal_precision_matrix = gmrf.precision_temporal(time=self.time)\n    lam = self.temporal_precision_matrix[0, 0]\n    self.temporal_precision_matrix[0, 0] = lam * (2.0 - lam / (self.precision_time_0 + lam))\n\n    if self.spatial_dependence:\n        self.make_spatial_precision_matrix()\n        self.precision_matrix = sparse.kron(self.temporal_precision_matrix, self.spatial_precision_matrix)\n    else:\n        self.precision_matrix = self.temporal_precision_matrix\n    if (self.n_parameter == 1) and sparse.issparse(self.precision_matrix):\n        self.precision_matrix = self.precision_matrix.toarray()\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_spatial_precision_matrix","title":"<code>make_spatial_precision_matrix()</code>","text":"<p>Create the spatial precision matrix for the model.</p> <p>The spatial precision matrix is simply calculated as the inverse of a squared exponential covariance matrix calculated using the sensor locations.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_spatial_precision_matrix(self):\n    \"\"\"Create the spatial precision matrix for the model.\n\n    The spatial precision matrix is simply calculated as the inverse of a squared exponential covariance matrix\n    calculated using the sensor locations.\n\n    \"\"\"\n    location_array = self.location.to_array()\n    spatial_covariance_matrix = np.exp(\n        -(1 / (2 * np.power(self.spatial_correlation_param, 2)))\n        * (\n            np.power(location_array[:, [0]] - location_array[:, [0]].T, 2)\n            + np.power(location_array[:, [1]] - location_array[:, [1]].T, 2)\n            + np.power(location_array[:, [2]] - location_array[:, [2]].T, 2)\n        )\n    )\n    self.spatial_precision_matrix = np.linalg.inv(\n        spatial_covariance_matrix + (1e-6) * np.eye(spatial_covariance_matrix.shape[0])\n    )\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.get_locations_from_sensors","title":"<code>get_locations_from_sensors(sensor_object)</code>","text":"<p>Extract the location information from the sensor object.</p> <p>Attaches a Coordinate.ENU object as the self.location attribute, with all the sensor locations stored on the same object.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def get_locations_from_sensors(self, sensor_object: SensorGroup):\n    \"\"\"Extract the location information from the sensor object.\n\n    Attaches a Coordinate.ENU object as the self.location attribute, with all the sensor locations stored on the\n    same object.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    self.location = deepcopy(sensor_object[list(sensor_object.keys())[0]].location.to_enu())\n    self.location.east = np.full(shape=(self.n_location,), fill_value=np.nan)\n    self.location.north = np.full(shape=(self.n_location,), fill_value=np.nan)\n    self.location.up = np.full(shape=(self.n_location,), fill_value=np.nan)\n    for k, sensor in enumerate(sensor_object.values()):\n        if isinstance(sensor, Beam):\n            self.location.east[k] = np.mean(sensor.location.to_enu().east, axis=0)\n            self.location.north[k] = np.mean(sensor.location.to_enu().north, axis=0)\n            self.location.up[k] = np.mean(sensor.location.to_enu().up, axis=0)\n        else:\n            self.location.east[k] = sensor.location.to_enu().east.item()\n            self.location.north[k] = sensor.location.to_enu().north.item()\n            self.location.up[k] = sensor.location.to_enu().up.item()\n</code></pre>"},{"location":"pyelq/component/component/","title":"Overview","text":""},{"location":"pyelq/component/component/#component-classes","title":"Component classes","text":"<p>An overview of the component classes:</p> <ul> <li> <p>Background Model</p> </li> <li> <p>Error Model</p> </li> <li> <p>Offset</p> </li> <li> <p>Source Model</p> </li> </ul>"},{"location":"pyelq/component/component/#component-superclass","title":"Component superclass","text":"<p>Superclass for model components.</p>"},{"location":"pyelq/component/component/#pyelq.component.component.Component","title":"<code>Component</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class defining methods and rules for model elements.</p> <p>The bulk of attributes will be defined in the subclasses inheriting from this superclass.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@dataclass\nclass Component(ABC):\n    \"\"\"Abstract class defining methods and rules for model elements.\n\n    The bulk of attributes will be defined in the subclasses inheriting from this superclass.\n\n    \"\"\"\n\n    @abstractmethod\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n\n    @abstractmethod\n    def make_model(self, model: list) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to [].\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_state(self, state: dict) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to {}.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n\n    @abstractmethod\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>  <code>abstractmethod</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.make_model","title":"<code>make_model(model)</code>  <code>abstractmethod</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to [].</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef make_model(self, model: list) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to [].\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.make_sampler","title":"<code>make_sampler(model, sampler_list)</code>  <code>abstractmethod</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to [].</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.make_state","title":"<code>make_state(state)</code>  <code>abstractmethod</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to {}.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef make_state(self, state: dict) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to {}.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.from_mcmc","title":"<code>from_mcmc(store)</code>  <code>abstractmethod</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/error_model/","title":"Error Model","text":""},{"location":"pyelq/component/error_model/#error-model","title":"Error Model","text":"<p>Error model module.</p>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel","title":"<code>ErrorModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code></p> <p>Measurement precision model component for the model.</p> <p>Attributes:</p> Name Type Description <code>n_sensor</code> <code>int</code> <p>number of sensors in the sensor object used for analysis.</p> <code>precision_index</code> <code>ndarray</code> <p>index mapping precision parameters onto observations. Will be set up differently for different model types.</p> <code>precision_parameter</code> <code>Parameter</code> <p>parameter object which constructs the full measurement error precision matrix from the components stored in state. Will be passed to the distribution for the observed when the full model is constructed.</p> <code>prior_precision_shape</code> <code>Union[ndarray, float]</code> <p>prior shape parameters for the precision model. Set up differently per model type.</p> <code>prior_precision_rate</code> <code>Union[ndarray, float]</code> <p>prior rate parameters for the precision model. Set up differently per model type.</p> <code>initial_precision</code> <code>Union[ndarray, float]</code> <p>initial value for the precision to be passed to the analysis routine. Set up differently per model type.</p> <code>precision</code> <code>ndarray</code> <p>array of sampled measurement error precision values, populated in self.from_mcmc() after the MCMC run is completed.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>@dataclass\nclass ErrorModel(Component):\n    \"\"\"Measurement precision model component for the model.\n\n    Attributes:\n        n_sensor (int): number of sensors in the sensor object used for analysis.\n        precision_index (np.ndarray): index mapping precision parameters onto observations. Will be set up differently\n            for different model types.\n        precision_parameter (parameter.Parameter): parameter object which constructs the full measurement error\n            precision matrix from the components stored in state. Will be passed to the distribution for the observed\n            when the full model is constructed.\n        prior_precision_shape (Union[np.ndarray, float]): prior shape parameters for the precision model. Set up\n            differently per model type.\n        prior_precision_rate (Union[np.ndarray, float]): prior rate parameters for the precision model. Set up\n            differently per model type.\n        initial_precision (Union[np.ndarray, float]): initial value for the precision to be passed to the analysis\n            routine. Set up differently per model type.\n        precision (np.ndarray): array of sampled measurement error precision values, populated in self.from_mcmc() after\n            the MCMC run is completed.\n\n    \"\"\"\n\n    n_sensor: int = field(init=False)\n    precision_index: np.ndarray = field(init=False)\n    precision_parameter: parameter.Parameter = field(init=False)\n    prior_precision_shape: Union[np.ndarray, float] = field(init=False)\n    prior_precision_rate: Union[np.ndarray, float] = field(init=False)\n    initial_precision: Union[np.ndarray, float] = field(init=False)\n    precision: np.ndarray = field(init=False)\n\n    def initialise(\n        self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n    ):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n            gas_species (GasSpecies): gas species information. Defaults to None.\n\n        \"\"\"\n        self.n_sensor = sensor_object.nof_sensors\n\n    def make_model(self, model: list = None) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to None.\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n        if model is None:\n            model = []\n        model.append(Gamma(\"tau\", shape=\"a_tau\", rate=\"b_tau\"))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalGamma(\"tau\", model))\n        return sampler_list\n\n    def make_state(self, state: dict = None) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to None.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n        if state is None:\n            state = {}\n        state[\"a_tau\"] = self.prior_precision_shape.flatten()\n        state[\"b_tau\"] = self.prior_precision_rate.flatten()\n        state[\"precision_index\"] = self.precision_index\n        state[\"tau\"] = self.initial_precision.flatten()\n        return state\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.precision = store[\"tau\"]\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.initialise","title":"<code>initialise(sensor_object, meteorology=None, gas_species=None)</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data. Defaults to None.</p> <code>None</code> <code>gas_species</code> <code>GasSpecies</code> <p>gas species information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def initialise(\n    self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n        gas_species (GasSpecies): gas species information. Defaults to None.\n\n    \"\"\"\n    self.n_sensor = sensor_object.nof_sensors\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.make_model","title":"<code>make_model(model=None)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def make_model(self, model: list = None) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to None.\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n    if model is None:\n        model = []\n    model.append(Gamma(\"tau\", shape=\"a_tau\", rate=\"b_tau\"))\n    return model\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.make_sampler","title":"<code>make_sampler(model, sampler_list=None)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalGamma(\"tau\", model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.make_state","title":"<code>make_state(state=None)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def make_state(self, state: dict = None) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to None.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n    if state is None:\n        state = {}\n    state[\"a_tau\"] = self.prior_precision_shape.flatten()\n    state[\"b_tau\"] = self.prior_precision_rate.flatten()\n    state[\"precision_index\"] = self.precision_index\n    state[\"tau\"] = self.initial_precision.flatten()\n    return state\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.precision = store[\"tau\"]\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor","title":"<code>BySensor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ErrorModel</code></p> <p>Version of measurement precision where each sensor object has a different precision.</p> <p>Attributes:</p> Name Type Description <code>prior_precision_shape</code> <code>Union[ndarray, float]</code> <p>prior shape parameters for the precision model, can be specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same parameter value for each sensor. Defaults to 1e-3.</p> <code>prior_precision_rate</code> <code>Union[ndarray, float]</code> <p>prior rate parameters for the precision model, can be specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same parameter value for each sensor. Defaults to 1e-3.</p> <code>initial_precision</code> <code>Union[ndarray, float]</code> <p>initial value for the precision parameters, can be specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same parameter value for each sensor. Defaults to 1.</p> <code>precision_index</code> <code>ndarray</code> <p>index mapping precision parameters onto observations. Parameters 1:n_sensor are mapped as the measurement error precisions of the corresponding sensors.</p> <code>precision_parameter</code> <code>MixtureParameterMatrix</code> <p>parameter specification for this model, maps the current value of the parameter in the state dict onto the concentration data precisions.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>@dataclass\nclass BySensor(ErrorModel):\n    \"\"\"Version of measurement precision where each sensor object has a different precision.\n\n    Attributes:\n        prior_precision_shape (Union[np.ndarray, float]): prior shape parameters for the precision model, can be\n            specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in\n            the same parameter value for each sensor. Defaults to 1e-3.\n        prior_precision_rate (Union[np.ndarray, float]): prior rate parameters for the precision model, can be\n            specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in\n            the same parameter value for each sensor. Defaults to 1e-3.\n        initial_precision (Union[np.ndarray, float]): initial value for the precision parameters, can be specified\n            either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same\n            parameter value for each sensor. Defaults to 1.\n        precision_index (np.ndarray): index mapping precision parameters onto observations. Parameters 1:n_sensor are\n            mapped as the measurement error precisions of the corresponding sensors.\n        precision_parameter (Parameter.MixtureParameterMatrix): parameter specification for this model, maps the\n            current value of the parameter in the state dict onto the concentration data precisions.\n\n    \"\"\"\n\n    prior_precision_shape: Union[np.ndarray, float] = 1e-3\n    prior_precision_rate: Union[np.ndarray, float] = 1e-3\n    initial_precision: Union[np.ndarray, float] = 1.0\n\n    def initialise(\n        self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n    ):\n        \"\"\"Set up the error model using sensor properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n            gas_species (GasSpecies): gas species information. Defaults to None.\n\n        \"\"\"\n        super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n        self.prior_precision_shape = self.prior_precision_shape * np.ones((self.n_sensor,))\n        self.prior_precision_rate = self.prior_precision_rate * np.ones((self.n_sensor,))\n        self.initial_precision = self.initial_precision * np.ones((self.n_sensor,))\n        self.precision_index = sensor_object.sensor_index\n        self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n\n    def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the error model values for every sensor with respect to the MCMC iterations.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_iterations'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n        )\n\n        return plot\n\n    def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the distribution of the error model values after the burn in for every sensor.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_distributions'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n        )\n\n        return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor.initialise","title":"<code>initialise(sensor_object, meteorology=None, gas_species=None)</code>","text":"<p>Set up the error model using sensor properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data. Defaults to None.</p> <code>None</code> <code>gas_species</code> <code>GasSpecies</code> <p>gas species information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def initialise(\n    self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n):\n    \"\"\"Set up the error model using sensor properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n        gas_species (GasSpecies): gas species information. Defaults to None.\n\n    \"\"\"\n    super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n    self.prior_precision_shape = self.prior_precision_shape * np.ones((self.n_sensor,))\n    self.prior_precision_rate = self.prior_precision_rate * np.ones((self.n_sensor,))\n    self.initial_precision = self.initial_precision * np.ones((self.n_sensor,))\n    self.precision_index = sensor_object.sensor_index\n    self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor.plot_iterations","title":"<code>plot_iterations(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the error model values for every sensor with respect to the MCMC iterations.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_iterations'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the error model values for every sensor with respect to the MCMC iterations.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_iterations'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor.plot_distributions","title":"<code>plot_distributions(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the distribution of the error model values after the burn in for every sensor.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_distributions'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the distribution of the error model values after the burn in for every sensor.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_distributions'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease","title":"<code>ByRelease</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ErrorModel</code></p> <p>ByRelease error model, special case of the measurement precision model.</p> <p>Version of the measurement precision model where each sensor object has a different precision, and there are different precisions for periods inside and outside controlled release periods. For all parameters: the first element corresponds to the case where the sources are OFF; the second element corresponds to the case where the sources are ON.</p> <p>Attributes:</p> Name Type Description <code>prior_precision_shape</code> <code>ndarray</code> <p>prior shape parameters for the precision model, can be specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification will result in the same prior specification for the off/on precisions for each sensor. Defaults to np.array([1e-3, 1e-3]).</p> <code>prior_precision_rate</code> <code>ndarray</code> <p>prior rate parameters for the precision model, can be specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification will result in the same prior specification for the off/on precisions for each sensor. Defaults to np.array([1e-3, 1e-3]).</p> <code>initial_precision</code> <code>ndarray</code> <p>initial value for the precision parameters, can be specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification will result in the same prior specification for the off/on precisions for each sensor. Defaults to np.array([1.0, 1.0]).</p> <code>precision_index</code> <code>ndarray</code> <p>index mapping precision parameters onto observations. Parameters 1:n_sensor are mapped onto each sensor for the periods where the sources are OFF; parameters (n_sensor + 1):(2 * n_sensor) are mapped onto each sensor for the periods where the sources are ON.</p> <code>precision_parameter</code> <code>MixtureParameterMatrix</code> <p>parameter specification for this model, maps the current value of the parameter in the state dict onto the concentration data precisions.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>@dataclass\nclass ByRelease(ErrorModel):\n    \"\"\"ByRelease error model, special case of the measurement precision model.\n\n    Version of the measurement precision model where each sensor object has a different precision, and there are\n    different precisions for periods inside and outside controlled release periods. For all parameters: the first\n    element corresponds to the case where the sources are OFF; the second element corresponds to the case where the\n    sources are ON.\n\n    Attributes:\n        prior_precision_shape (np.ndarray): prior shape parameters for the precision model, can be\n            specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification\n            will result in the same prior specification for the off/on precisions for each sensor. Defaults to\n            np.array([1e-3, 1e-3]).\n        prior_precision_rate (np.ndarray): prior rate parameters for the precision model, can be\n            specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification\n            will result in the same prior specification for the off/on precisions for each sensor. Defaults to\n            np.array([1e-3, 1e-3]).\n        initial_precision (np.ndarray): initial value for the precision parameters, can be\n            specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification\n            will result in the same prior specification for the off/on precisions for each sensor. Defaults to\n            np.array([1.0, 1.0]).\n        precision_index (np.ndarray): index mapping precision parameters onto observations. Parameters 1:n_sensor are\n            mapped onto each sensor for the periods where the sources are OFF; parameters (n_sensor + 1):(2 * n_sensor)\n            are mapped onto each sensor for the periods where the sources are ON.\n        precision_parameter (Parameter.MixtureParameterMatrix): parameter specification for this model, maps the\n            current value of the parameter in the state dict onto the concentration data precisions.\n\n    \"\"\"\n\n    prior_precision_shape: np.ndarray = field(default_factory=lambda: np.array([1e-3, 1e-3], ndmin=2).T)\n    prior_precision_rate: np.ndarray = field(default_factory=lambda: np.array([1e-3, 1e-3], ndmin=2).T)\n    initial_precision: np.ndarray = field(default_factory=lambda: np.array([1.0, 1.0], ndmin=2).T)\n\n    def initialise(\n        self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n    ):\n        \"\"\"Set up the error model using sensor properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n            gas_species (GasSpecies): gas species information. Defaults to None.\n\n        \"\"\"\n        super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n        self.prior_precision_shape = self.prior_precision_shape * np.ones((2, self.n_sensor))\n        self.prior_precision_rate = self.prior_precision_rate * np.ones((2, self.n_sensor))\n        self.initial_precision = self.initial_precision * np.ones((2, self.n_sensor))\n        self.precision_index = sensor_object.sensor_index + sensor_object.source_on * self.n_sensor\n        self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n\n    def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plot the estimated error model parameters against iterations of the MCMC chain.\n\n        Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n        sources are on/off). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n        call to its plot function.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_iterations'\n\n        \"\"\"\n        figure_keys = [\"error_model_off_iterations\", \"error_model_on_iterations\"]\n        figure_titles = [\n            \"Estimated error parameter values: sources off\",\n            \"Estimated error parameter values: sources on\",\n        ]\n        precision_arrays = [\n            self.precision[: sensor_object.nof_sensors, :],\n            self.precision[sensor_object.nof_sensors :, :],\n        ]\n        for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n            error_model = BySensor()\n            error_model.precision = array\n            plot = error_model.plot_iterations(plot, sensor_object, burn_in_value)\n            plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_iterations\")\n            plot.figure_dict[key].update_layout(title=title)\n        return plot\n\n    def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plot the estimated distributions of error model parameters.\n\n        Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n        sources are off/on). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n        call to its plot function.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_distributions'\n\n        \"\"\"\n        figure_keys = [\"error_model_off_distributions\", \"error_model_on_distributions\"]\n        figure_titles = [\n            \"Estimated error parameter distribution: sources off\",\n            \"Estimated error parameter distribution: sources on\",\n        ]\n        precision_arrays = [\n            self.precision[: sensor_object.nof_sensors, :],\n            self.precision[sensor_object.nof_sensors :, :],\n        ]\n        for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n            error_model = BySensor()\n            error_model.precision = array\n            plot = error_model.plot_distributions(plot, sensor_object, burn_in_value)\n            plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_distributions\")\n            plot.figure_dict[key].update_layout(title=title)\n        return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease.initialise","title":"<code>initialise(sensor_object, meteorology=None, gas_species=None)</code>","text":"<p>Set up the error model using sensor properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data. Defaults to None.</p> <code>None</code> <code>gas_species</code> <code>GasSpecies</code> <p>gas species information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def initialise(\n    self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n):\n    \"\"\"Set up the error model using sensor properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n        gas_species (GasSpecies): gas species information. Defaults to None.\n\n    \"\"\"\n    super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n    self.prior_precision_shape = self.prior_precision_shape * np.ones((2, self.n_sensor))\n    self.prior_precision_rate = self.prior_precision_rate * np.ones((2, self.n_sensor))\n    self.initial_precision = self.initial_precision * np.ones((2, self.n_sensor))\n    self.precision_index = sensor_object.sensor_index + sensor_object.source_on * self.n_sensor\n    self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease.plot_iterations","title":"<code>plot_iterations(plot, sensor_object, burn_in_value)</code>","text":"<p>Plot the estimated error model parameters against iterations of the MCMC chain.</p> <p>Works by simply creating a separate plot for each of the two categories of precision parameter (when the sources are on/off). Creates a BySensor() object for each of the off/on precision cases, and then makes a call to its plot function.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_iterations'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plot the estimated error model parameters against iterations of the MCMC chain.\n\n    Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n    sources are on/off). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n    call to its plot function.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_iterations'\n\n    \"\"\"\n    figure_keys = [\"error_model_off_iterations\", \"error_model_on_iterations\"]\n    figure_titles = [\n        \"Estimated error parameter values: sources off\",\n        \"Estimated error parameter values: sources on\",\n    ]\n    precision_arrays = [\n        self.precision[: sensor_object.nof_sensors, :],\n        self.precision[sensor_object.nof_sensors :, :],\n    ]\n    for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n        error_model = BySensor()\n        error_model.precision = array\n        plot = error_model.plot_iterations(plot, sensor_object, burn_in_value)\n        plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_iterations\")\n        plot.figure_dict[key].update_layout(title=title)\n    return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease.plot_distributions","title":"<code>plot_distributions(plot, sensor_object, burn_in_value)</code>","text":"<p>Plot the estimated distributions of error model parameters.</p> <p>Works by simply creating a separate plot for each of the two categories of precision parameter (when the sources are off/on). Creates a BySensor() object for each of the off/on precision cases, and then makes a call to its plot function.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_distributions'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plot the estimated distributions of error model parameters.\n\n    Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n    sources are off/on). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n    call to its plot function.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_distributions'\n\n    \"\"\"\n    figure_keys = [\"error_model_off_distributions\", \"error_model_on_distributions\"]\n    figure_titles = [\n        \"Estimated error parameter distribution: sources off\",\n        \"Estimated error parameter distribution: sources on\",\n    ]\n    precision_arrays = [\n        self.precision[: sensor_object.nof_sensors, :],\n        self.precision[sensor_object.nof_sensors :, :],\n    ]\n    for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n        error_model = BySensor()\n        error_model.precision = array\n        plot = error_model.plot_distributions(plot, sensor_object, burn_in_value)\n        plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_distributions\")\n        plot.figure_dict[key].update_layout(title=title)\n    return plot\n</code></pre>"},{"location":"pyelq/component/offset/","title":"Offset","text":""},{"location":"pyelq/component/offset/#offset","title":"Offset","text":"<p>Offset module.</p>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor","title":"<code>PerSensor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code></p> <p>Offset implementation which assumes an additive offset between sensors.</p> <p>The offset is which is constant in space and time and accounts for calibration differences between sensors. To maintain parameter identifiability, the offset for the first sensor (with index 0) is assumed to be 0, and other sensor offsets are defined relative to this beam.</p> <p>Attributes:</p> Name Type Description <code>n_sensor</code> <code>int</code> <p>number of sensors in the sensor object used for analysis.</p> <code>offset</code> <code>ndarray</code> <p>array of sampled offset values, populated in self.from_mcmc() after the MCMC run is completed.</p> <code>precision_scalar</code> <code>ndarray</code> <p>array of sampled offset precision values, populated in self.from_mcmc() after the MCMC run is completed. Only populated if update_precision is True.</p> <code>indicator_basis</code> <code>csc_matrix</code> <p>[nof_observations x (nof_sensors - 1)] sparse matrix which assigns the offset parameters to the correct observations.</p> <code>update_precision</code> <code>bool</code> <p>logical indicating whether the offset prior precision parameter should be updated as part of the analysis.</p> <code>mean_offset</code> <code>float</code> <p>prior mean parameter for the offsets, assumed to be the same for each beam. Default is 0.</p> <code>prior_precision_shape</code> <code>float</code> <p>shape parameter for the prior gamma distribution for the scalar precision parameter. Default is 1e-3.</p> <code>prior_precision_rate</code> <code>float</code> <p>rate parameter for the prior gamma distribution for the scalar precision parameter(s). Default is 1e-3.</p> <code>initial_precision</code> <code>float</code> <p>initial value for the scalar precision parameter. Default is 1.0.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>@dataclass\nclass PerSensor(Component):\n    \"\"\"Offset implementation which assumes an additive offset between sensors.\n\n    The offset is which is constant in space and time and accounts for calibration differences between sensors.\n    To maintain parameter identifiability, the offset for the first sensor (with index 0) is assumed to be 0, and other\n    sensor offsets are defined relative to this beam.\n\n    Attributes:\n        n_sensor (int): number of sensors in the sensor object used for analysis.\n        offset (np.ndarray): array of sampled offset values, populated in self.from_mcmc() after the MCMC run is\n            completed.\n        precision_scalar (np.ndarray): array of sampled offset precision values, populated in self.from_mcmc() after\n            the MCMC run is completed. Only populated if update_precision is True.\n        indicator_basis (sparse.csc_matrix): [nof_observations x (nof_sensors - 1)] sparse matrix which assigns the\n            offset parameters to the correct observations.\n        update_precision (bool): logical indicating whether the offset prior precision parameter should be updated as\n            part of the analysis.\n        mean_offset (float): prior mean parameter for the offsets, assumed to be the same for each beam. Default is 0.\n        prior_precision_shape (float): shape parameter for the prior gamma distribution for the scalar precision\n            parameter. Default is 1e-3.\n        prior_precision_rate (float): rate parameter for the prior gamma distribution for the scalar precision\n            parameter(s). Default is 1e-3.\n        initial_precision (float): initial value for the scalar precision parameter. Default is 1.0.\n\n    \"\"\"\n\n    n_sensor: int = field(init=False)\n    offset: np.ndarray = field(init=False)\n    precision_scalar: np.ndarray = field(init=False)\n    indicator_basis: sparse.csc_matrix = field(init=False)\n    update_precision: bool = False\n    mean_offset: float = 0.0\n    prior_precision_shape: float = 1e-3\n    prior_precision_rate: float = 1e-3\n    initial_precision: float = 1.0\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data wind data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n        self.n_sensor = len(sensor_object)\n        self.indicator_basis = sparse.csc_matrix(\n            np.equal(sensor_object.sensor_index[:, np.newaxis], np.array(range(1, self.n_sensor)))\n        )\n\n    def make_model(self, model: list = None) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to [].\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n        if model is None:\n            model = []\n        off_precision_predictor = parameter.ScaledMatrix(matrix=\"P_d\", scalar=\"lambda_d\")\n        model.append(Normal(\"d\", mean=\"mu_d\", precision=off_precision_predictor))\n        if self.update_precision:\n            model.append(Gamma(\"lambda_d\", shape=\"a_lam_d\", rate=\"b_lam_d\"))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalNormal(\"d\", model))\n        if self.update_precision:\n            sampler_list.append(NormalGamma(\"lambda_d\", model))\n        return sampler_list\n\n    def make_state(self, state: dict = None) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to {}.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n        if state is None:\n            state = {}\n        state[\"mu_d\"] = np.ones((self.n_sensor - 1, 1)) * self.mean_offset\n        state[\"d\"] = np.zeros((self.n_sensor - 1, 1))\n        state[\"B_d\"] = self.indicator_basis\n        state[\"P_d\"] = sparse.eye(self.n_sensor - 1, format=\"csc\")\n        state[\"lambda_d\"] = self.initial_precision\n        if self.update_precision:\n            state[\"a_lam_d\"] = self.prior_precision_shape\n            state[\"b_lam_d\"] = self.prior_precision_rate\n        return state\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.offset = store[\"d\"]\n        if self.update_precision:\n            self.precision_scalar = store[\"lambda_d\"]\n\n    def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the offset values for every sensor with respect to the MCMC iterations.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'offset_iterations'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n        )\n\n        return plot\n\n    def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the distribution of the offset values after the burn in for every sensor.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n            burn_in_value (int): Burn in value to use for plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'offset_distributions'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n        )\n\n        return plot\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data wind data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data wind data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n    self.n_sensor = len(sensor_object)\n    self.indicator_basis = sparse.csc_matrix(\n        np.equal(sensor_object.sensor_index[:, np.newaxis], np.array(range(1, self.n_sensor)))\n    )\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.make_model","title":"<code>make_model(model=None)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to [].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def make_model(self, model: list = None) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to [].\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n    if model is None:\n        model = []\n    off_precision_predictor = parameter.ScaledMatrix(matrix=\"P_d\", scalar=\"lambda_d\")\n    model.append(Normal(\"d\", mean=\"mu_d\", precision=off_precision_predictor))\n    if self.update_precision:\n        model.append(Gamma(\"lambda_d\", shape=\"a_lam_d\", rate=\"b_lam_d\"))\n    return model\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.make_sampler","title":"<code>make_sampler(model, sampler_list=None)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to [].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalNormal(\"d\", model))\n    if self.update_precision:\n        sampler_list.append(NormalGamma(\"lambda_d\", model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.make_state","title":"<code>make_state(state=None)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to {}.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def make_state(self, state: dict = None) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to {}.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n    if state is None:\n        state = {}\n    state[\"mu_d\"] = np.ones((self.n_sensor - 1, 1)) * self.mean_offset\n    state[\"d\"] = np.zeros((self.n_sensor - 1, 1))\n    state[\"B_d\"] = self.indicator_basis\n    state[\"P_d\"] = sparse.eye(self.n_sensor - 1, format=\"csc\")\n    state[\"lambda_d\"] = self.initial_precision\n    if self.update_precision:\n        state[\"a_lam_d\"] = self.prior_precision_shape\n        state[\"b_lam_d\"] = self.prior_precision_rate\n    return state\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.offset = store[\"d\"]\n    if self.update_precision:\n        self.precision_scalar = store[\"lambda_d\"]\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.plot_iterations","title":"<code>plot_iterations(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the offset values for every sensor with respect to the MCMC iterations.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the offset_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'offset_iterations'</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the offset values for every sensor with respect to the MCMC iterations.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'offset_iterations'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.plot_distributions","title":"<code>plot_distributions(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the distribution of the offset values after the burn in for every sensor.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the offset_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to use for plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'offset_distributions'</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the distribution of the offset values after the burn in for every sensor.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n        burn_in_value (int): Burn in value to use for plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'offset_distributions'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/source_model/","title":"Source Model","text":""},{"location":"pyelq/component/source_model/#source-model","title":"Source Model","text":"<p>Component Class and subclasses for source model.</p> A SourceModel instance inherits from 3 super-classes <ul> <li>Component: this is the general superclass for ELQModel components, which prototypes generic methods.</li> <li>A type of SourceGrouping: this class type implements an allocation of sources to different categories (e.g. slab     or spike), and sets up a sampler for estimating the classification of each source within the source map.     Inheriting from the NullGrouping class ensures that the allocation of all sources is fixed during the inversion,     and is not updated.</li> <li>A type of SourceDistribution: this class type implements a particular type of response distribution (mostly     Normal, but also allows for cases where we have e.g. exp(log_s) or a non-Gaussian prior).</li> </ul>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.ParameterMapping","title":"<code>ParameterMapping</code>  <code>dataclass</code>","text":"<p>Class for defining mapping variable/parameter labels needed for creating an analysis.</p> <p>In instances where we want to include multiple source_model instances in an MCMC analysis, we can apply a suffix to all of the parameter names in the mapping dictionary. This allows us to create separate variables for different source map types, so that these can be associated with different sampler types in the MCMC analysis.</p> <p>Attributes:</p> Name Type Description <code>map</code> <code>dict</code> <p>dictionary containing mapping between variable types and MCMC parameters.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass ParameterMapping:\n    \"\"\"Class for defining mapping variable/parameter labels needed for creating an analysis.\n\n    In instances where we want to include multiple source_model instances in an MCMC analysis, we can apply a suffix to\n    all of the parameter names in the mapping dictionary. This allows us to create separate variables for different\n    source map types, so that these can be associated with different sampler types in the MCMC analysis.\n\n    Attributes:\n        map (dict): dictionary containing mapping between variable types and MCMC parameters.\n\n    \"\"\"\n\n    map: dict = field(\n        default_factory=lambda: {\n            \"source\": \"s\",\n            \"coupling_matrix\": \"A\",\n            \"emission_rate_mean\": \"mu_s\",\n            \"emission_rate_precision\": \"lambda_s\",\n            \"allocation\": \"alloc_s\",\n            \"source_prob\": \"s_prob\",\n            \"precision_prior_shape\": \"a_lam_s\",\n            \"precision_prior_rate\": \"b_lam_s\",\n            \"source_location\": \"z_src\",\n            \"number_sources\": \"n_src\",\n            \"number_source_rate\": \"rho\",\n        }\n    )\n\n    def append_string(self, string: str = None):\n        \"\"\"Apply the supplied string as a suffix to all of the values in the mapping dictionary.\n\n        For example: {'source': 's'} would become {'source': 's_fixed'} when string = 'fixed' is passed as the argument.\n        If string is None, nothing is appended.\n\n        Args:\n            string (str): string to append to the variable names.\n\n        \"\"\"\n        for key, value in self.map.items():\n            self.map[key] = value + \"_\" + string\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.ParameterMapping.append_string","title":"<code>append_string(string=None)</code>","text":"<p>Apply the supplied string as a suffix to all of the values in the mapping dictionary.</p> <p>For example: {'source': 's'} would become {'source': 's_fixed'} when string = 'fixed' is passed as the argument. If string is None, nothing is appended.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>string to append to the variable names.</p> <code>None</code> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def append_string(self, string: str = None):\n    \"\"\"Apply the supplied string as a suffix to all of the values in the mapping dictionary.\n\n    For example: {'source': 's'} would become {'source': 's_fixed'} when string = 'fixed' is passed as the argument.\n    If string is None, nothing is appended.\n\n    Args:\n        string (str): string to append to the variable names.\n\n    \"\"\"\n    for key, value in self.map.items():\n        self.map[key] = value + \"_\" + string\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping","title":"<code>SourceGrouping</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParameterMapping</code></p> <p>Superclass for source grouping approach.</p> <p>Source grouping method determines the group allocation of each source in the model, e.g: slab and spike distribution makes an on/off allocation for each source.</p> <p>Attributes:</p> Name Type Description <code>nof_sources</code> <code>int</code> <p>number of sources in the model.</p> <code>emission_rate_mean</code> <code>Union[float, ndarray]</code> <p>prior mean parameter for the emission rate distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SourceGrouping(ParameterMapping):\n    \"\"\"Superclass for source grouping approach.\n\n    Source grouping method determines the group allocation of each source in the model, e.g: slab and spike\n    distribution makes an on/off allocation for each source.\n\n    Attributes:\n        nof_sources (int): number of sources in the model.\n        emission_rate_mean (Union[float, np.ndarray]): prior mean parameter for the emission rate distribution.\n\n    \"\"\"\n\n    nof_sources: int = field(init=False)\n    emission_rate_mean: Union[float, np.ndarray] = field(init=False)\n\n    @abstractmethod\n    def make_allocation_model(self, model: list) -&gt; list:\n        \"\"\"Initialise the source allocation part of the model, and the parameters of the source response distribution.\n\n        Args:\n            model (list): overall model, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with allocation distribution.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the allocation part of the sampler.\n\n        Args:\n            model (Model): overall model, consisting of list of distributions.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the source allocation.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_allocation_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the allocation part of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source grouping.\n\n        \"\"\"\n\n    @abstractmethod\n    def from_mcmc_group(self, store: dict):\n        \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.make_allocation_model","title":"<code>make_allocation_model(model)</code>  <code>abstractmethod</code>","text":"<p>Initialise the source allocation part of the model, and the parameters of the source response distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>overall model, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with allocation distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_allocation_model(self, model: list) -&gt; list:\n    \"\"\"Initialise the source allocation part of the model, and the parameters of the source response distribution.\n\n    Args:\n        model (list): overall model, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with allocation distribution.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.make_allocation_sampler","title":"<code>make_allocation_sampler(model, sampler_list)</code>  <code>abstractmethod</code>","text":"<p>Initialise the allocation part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model, consisting of list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the source allocation.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the allocation part of the sampler.\n\n    Args:\n        model (Model): overall model, consisting of list of distributions.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the source allocation.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.make_allocation_state","title":"<code>make_allocation_state(state)</code>  <code>abstractmethod</code>","text":"<p>Initialise the allocation part of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source grouping.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_allocation_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the allocation part of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source grouping.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.from_mcmc_group","title":"<code>from_mcmc_group(store)</code>  <code>abstractmethod</code>","text":"<p>Extract posterior allocation samples from the MCMC sampler, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef from_mcmc_group(self, store: dict):\n    \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping","title":"<code>NullGrouping</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceGrouping</code></p> <p>Null grouping: the grouping of the sources will not change during the course of the inversion.</p> Note that this is intended to support two distinct cases <p>1) The case where the source map is fixed, and a given prior mean and prior precision value are assigned to     each source (can be a common value for all sources, or can be a distinct allocation to each element of the     source map). 2) The case where the dimensionality of the source map is changing during the inversion, and a common prior     mean and precision term are used for all sources.</p> <p>number_on_sources (np.ndarray): number of sources switched on in the solution, per iteration. Extracted as a     property from the MCMC samples in self.from_mcmc_group().</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass NullGrouping(SourceGrouping):\n    \"\"\"Null grouping: the grouping of the sources will not change during the course of the inversion.\n\n    Note that this is intended to support two distinct cases:\n        1) The case where the source map is fixed, and a given prior mean and prior precision value are assigned to\n            each source (can be a common value for all sources, or can be a distinct allocation to each element of the\n            source map).\n        2) The case where the dimensionality of the source map is changing during the inversion, and a common prior\n            mean and precision term are used for all sources.\n\n    Attributes:\n    number_on_sources (np.ndarray): number of sources switched on in the solution, per iteration. Extracted as a\n        property from the MCMC samples in self.from_mcmc_group().\n\n    \"\"\"\n\n    number_on_sources: np.ndarray = field(init=False)\n\n    def make_allocation_model(self, model: list) -&gt; list:\n        \"\"\"Initialise the source allocation part of the model.\n\n        In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n        returns the existing model un-modified).\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with allocation distribution.\n\n        \"\"\"\n        return model\n\n    def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the allocation part of the sampler.\n\n        In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n        returns the existing sampler list un-modified).\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the source allocation.\n\n        \"\"\"\n        return sampler_list\n\n    def make_allocation_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the allocation part of the state.\n\n        The prior mean parameter and the fixed source allocation are added to the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source grouping.\n\n        \"\"\"\n        state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n        state[self.map[\"allocation\"]] = np.zeros((self.nof_sources, 1), dtype=\"int\")\n        return state\n\n    def from_mcmc_group(self, store: dict):\n        \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n        Gets the number of sources present in each iteration of the MCMC sampler, and attaches this as a class property.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n        self.number_on_sources = np.count_nonzero(np.logical_not(np.isnan(store[self.map[\"source\"]])), axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.make_allocation_model","title":"<code>make_allocation_model(model)</code>","text":"<p>Initialise the source allocation part of the model.</p> <p>In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply returns the existing model un-modified).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with allocation distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_model(self, model: list) -&gt; list:\n    \"\"\"Initialise the source allocation part of the model.\n\n    In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n    returns the existing model un-modified).\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with allocation distribution.\n\n    \"\"\"\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.make_allocation_sampler","title":"<code>make_allocation_sampler(model, sampler_list)</code>","text":"<p>Initialise the allocation part of the sampler.</p> <p>In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply returns the existing sampler list un-modified).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the source allocation.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the allocation part of the sampler.\n\n    In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n    returns the existing sampler list un-modified).\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the source allocation.\n\n    \"\"\"\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.make_allocation_state","title":"<code>make_allocation_state(state)</code>","text":"<p>Initialise the allocation part of the state.</p> <p>The prior mean parameter and the fixed source allocation are added to the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source grouping.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the allocation part of the state.\n\n    The prior mean parameter and the fixed source allocation are added to the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source grouping.\n\n    \"\"\"\n    state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n    state[self.map[\"allocation\"]] = np.zeros((self.nof_sources, 1), dtype=\"int\")\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.from_mcmc_group","title":"<code>from_mcmc_group(store)</code>","text":"<p>Extract posterior allocation samples from the MCMC sampler, attach them to the class.</p> <p>Gets the number of sources present in each iteration of the MCMC sampler, and attaches this as a class property.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc_group(self, store: dict):\n    \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n    Gets the number of sources present in each iteration of the MCMC sampler, and attaches this as a class property.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n    self.number_on_sources = np.count_nonzero(np.logical_not(np.isnan(store[self.map[\"source\"]])), axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike","title":"<code>SlabAndSpike</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceGrouping</code></p> <p>Slab and spike source model, special case for the source grouping.</p> <p>Slab and spike: the prior for the emission rates is a two-component mixture, and the allocation is to be estimated as part of the inversion.</p> <p>Attributes:</p> Name Type Description <code>slab_probability</code> <code>float</code> <p>prior probability of allocation to the slab component. Defaults to 0.05.</p> <code>allocation</code> <code>ndarray</code> <p>set of allocation samples, with shape=(n_sources, n_iterations). Attached to the class by self.from_mcmc_group().</p> <code>number_on_sources</code> <code>ndarray</code> <p>number of sources switched on in the solution, per iteration.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SlabAndSpike(SourceGrouping):\n    \"\"\"Slab and spike source model, special case for the source grouping.\n\n    Slab and spike: the prior for the emission rates is a two-component mixture, and the allocation is to be\n    estimated as part of the inversion.\n\n    Attributes:\n        slab_probability (float): prior probability of allocation to the slab component. Defaults to 0.05.\n        allocation (np.ndarray): set of allocation samples, with shape=(n_sources, n_iterations). Attached to\n            the class by self.from_mcmc_group().\n        number_on_sources (np.ndarray): number of sources switched on in the solution, per iteration.\n\n    \"\"\"\n\n    slab_probability: float = 0.05\n    allocation: np.ndarray = field(init=False)\n    number_on_sources: np.ndarray = field(init=False)\n\n    def make_allocation_model(self, model: list) -&gt; list:\n        \"\"\"Initialise the source allocation part of the model.\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with allocation distribution.\n\n        \"\"\"\n        model.append(Categorical(self.map[\"allocation\"], prob=self.map[\"source_prob\"]))\n        return model\n\n    def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the allocation part of the sampler.\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the source allocation.\n\n        \"\"\"\n        sampler_list.append(\n            MixtureAllocation(param=self.map[\"allocation\"], model=model, response_param=self.map[\"source\"])\n        )\n        return sampler_list\n\n    def make_allocation_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the allocation part of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source grouping.\n\n        \"\"\"\n        state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n        state[self.map[\"source_prob\"]] = np.tile(\n            np.array([self.slab_probability, 1 - self.slab_probability]), (self.nof_sources, 1)\n        )\n        state[self.map[\"allocation\"]] = np.ones((self.nof_sources, 1), dtype=\"int\")\n        return state\n\n    def from_mcmc_group(self, store: dict):\n        \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n        self.allocation = store[self.map[\"allocation\"]]\n        self.number_on_sources = self.allocation.shape[0] - np.sum(self.allocation, axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.make_allocation_model","title":"<code>make_allocation_model(model)</code>","text":"<p>Initialise the source allocation part of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with allocation distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_model(self, model: list) -&gt; list:\n    \"\"\"Initialise the source allocation part of the model.\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with allocation distribution.\n\n    \"\"\"\n    model.append(Categorical(self.map[\"allocation\"], prob=self.map[\"source_prob\"]))\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.make_allocation_sampler","title":"<code>make_allocation_sampler(model, sampler_list)</code>","text":"<p>Initialise the allocation part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the source allocation.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the allocation part of the sampler.\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the source allocation.\n\n    \"\"\"\n    sampler_list.append(\n        MixtureAllocation(param=self.map[\"allocation\"], model=model, response_param=self.map[\"source\"])\n    )\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.make_allocation_state","title":"<code>make_allocation_state(state)</code>","text":"<p>Initialise the allocation part of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source grouping.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the allocation part of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source grouping.\n\n    \"\"\"\n    state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n    state[self.map[\"source_prob\"]] = np.tile(\n        np.array([self.slab_probability, 1 - self.slab_probability]), (self.nof_sources, 1)\n    )\n    state[self.map[\"allocation\"]] = np.ones((self.nof_sources, 1), dtype=\"int\")\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.from_mcmc_group","title":"<code>from_mcmc_group(store)</code>","text":"<p>Extract posterior allocation samples from the MCMC sampler, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc_group(self, store: dict):\n    \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n    self.allocation = store[self.map[\"allocation\"]]\n    self.number_on_sources = self.allocation.shape[0] - np.sum(self.allocation, axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution","title":"<code>SourceDistribution</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParameterMapping</code></p> <p>Superclass for source emission rate distribution.</p> <p>Source distribution determines the type of prior to be used for the source emission rates, and the transformation linking the source parameters and the data.</p> <p>Elements related to transformation of source parameters are also specified at the model level.</p> <p>Attributes:</p> Name Type Description <code>nof_sources</code> <code>int</code> <p>number of sources in the model.</p> <code>emission_rate</code> <code>ndarray</code> <p>set of emission rate samples, with shape=(n_sources, n_iterations). Attached to the class by self.from_mcmc_dist().</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SourceDistribution(ParameterMapping):\n    \"\"\"Superclass for source emission rate distribution.\n\n    Source distribution determines the type of prior to be used for the source emission rates, and the transformation\n    linking the source parameters and the data.\n\n    Elements related to transformation of source parameters are also specified at the model level.\n\n    Attributes:\n        nof_sources (int): number of sources in the model.\n        emission_rate (np.ndarray): set of emission rate samples, with shape=(n_sources, n_iterations). Attached to\n            the class by self.from_mcmc_dist().\n\n    \"\"\"\n\n    nof_sources: int = field(init=False)\n    emission_rate: np.ndarray = field(init=False)\n\n    @abstractmethod\n    def make_source_model(self, model: list) -&gt; list:\n        \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with distributions related to source prior.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_source_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the source prior distribution part of the sampler.\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the emission rate parameters.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_source_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the emission rate parts of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source emission rates.\n\n        \"\"\"\n\n    @abstractmethod\n    def from_mcmc_dist(self, store: dict):\n        \"\"\"Extract posterior emission rate samples from the MCMC, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.make_source_model","title":"<code>make_source_model(model)</code>  <code>abstractmethod</code>","text":"<p>Add distributional component to the overall model corresponding to the source emission rate distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with distributions related to source prior.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_source_model(self, model: list) -&gt; list:\n    \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with distributions related to source prior.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.make_source_sampler","title":"<code>make_source_sampler(model, sampler_list)</code>  <code>abstractmethod</code>","text":"<p>Initialise the source prior distribution part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the emission rate parameters.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_source_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the source prior distribution part of the sampler.\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the emission rate parameters.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.make_source_state","title":"<code>make_source_state(state)</code>  <code>abstractmethod</code>","text":"<p>Initialise the emission rate parts of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source emission rates.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_source_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the emission rate parts of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source emission rates.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.from_mcmc_dist","title":"<code>from_mcmc_dist(store)</code>  <code>abstractmethod</code>","text":"<p>Extract posterior emission rate samples from the MCMC, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef from_mcmc_dist(self, store: dict):\n    \"\"\"Extract posterior emission rate samples from the MCMC, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse","title":"<code>NormalResponse</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceDistribution</code></p> <p>(Truncated) Gaussian prior for sources.</p> <p>No transformation applied to parameters, i.e.: - Prior distribution: s ~ N(mu, 1/precision) - Likelihood contribution: y = A*s + b + ...</p> <p>Attributes:</p> Name Type Description <code>truncation</code> <code>bool</code> <p>indication of whether the emission rate prior should be truncated at 0. Defaults to True.</p> <code>emission_rate_lb</code> <code>Union[float, ndarray]</code> <p>lower bound for the source emission rates. Defaults to 0.</p> <code>emission_rate_mean</code> <code>Union[float, ndarray]</code> <p>prior mean for the emission rate distribution. Defaults to 0.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass NormalResponse(SourceDistribution):\n    \"\"\"(Truncated) Gaussian prior for sources.\n\n    No transformation applied to parameters, i.e.:\n    - Prior distribution: s ~ N(mu, 1/precision)\n    - Likelihood contribution: y = A*s + b + ...\n\n    Attributes:\n        truncation (bool): indication of whether the emission rate prior should be truncated at 0. Defaults to True.\n        emission_rate_lb (Union[float, np.ndarray]): lower bound for the source emission rates. Defaults to 0.\n        emission_rate_mean (Union[float, np.ndarray]): prior mean for the emission rate distribution. Defaults to 0.\n\n    \"\"\"\n\n    truncation: bool = True\n    emission_rate_lb: Union[float, np.ndarray] = 0\n    emission_rate_mean: Union[float, np.ndarray] = 0\n\n    def make_source_model(self, model: list) -&gt; list:\n        \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: model, updated with distributions related to source prior.\n\n        \"\"\"\n        domain_response_lower = None\n        if self.truncation:\n            domain_response_lower = self.emission_rate_lb\n\n        model.append(\n            mcmcNormal(\n                self.map[\"source\"],\n                mean=parameter.MixtureParameterVector(\n                    param=self.map[\"emission_rate_mean\"], allocation=self.map[\"allocation\"]\n                ),\n                precision=parameter.MixtureParameterMatrix(\n                    param=self.map[\"emission_rate_precision\"], allocation=self.map[\"allocation\"]\n                ),\n                domain_response_lower=domain_response_lower,\n            )\n        )\n        return model\n\n    def make_source_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Initialise the source prior distribution part of the sampler.\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the emission rate parameters.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalNormal(self.map[\"source\"], model))\n        return sampler_list\n\n    def make_source_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the emission rate part of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with initial emission rate vector.\n\n        \"\"\"\n        state[self.map[\"source\"]] = np.zeros((self.nof_sources, 1))\n        return state\n\n    def from_mcmc_dist(self, store: dict):\n        \"\"\"Extract posterior emission rate samples from the MCMC sampler, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n        self.emission_rate = store[self.map[\"source\"]]\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.make_source_model","title":"<code>make_source_model(model)</code>","text":"<p>Add distributional component to the overall model corresponding to the source emission rate distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model, updated with distributions related to source prior.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_source_model(self, model: list) -&gt; list:\n    \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: model, updated with distributions related to source prior.\n\n    \"\"\"\n    domain_response_lower = None\n    if self.truncation:\n        domain_response_lower = self.emission_rate_lb\n\n    model.append(\n        mcmcNormal(\n            self.map[\"source\"],\n            mean=parameter.MixtureParameterVector(\n                param=self.map[\"emission_rate_mean\"], allocation=self.map[\"allocation\"]\n            ),\n            precision=parameter.MixtureParameterMatrix(\n                param=self.map[\"emission_rate_precision\"], allocation=self.map[\"allocation\"]\n            ),\n            domain_response_lower=domain_response_lower,\n        )\n    )\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.make_source_sampler","title":"<code>make_source_sampler(model, sampler_list=None)</code>","text":"<p>Initialise the source prior distribution part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the emission rate parameters.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_source_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Initialise the source prior distribution part of the sampler.\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the emission rate parameters.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalNormal(self.map[\"source\"], model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.make_source_state","title":"<code>make_source_state(state)</code>","text":"<p>Initialise the emission rate part of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with initial emission rate vector.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_source_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the emission rate part of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with initial emission rate vector.\n\n    \"\"\"\n    state[self.map[\"source\"]] = np.zeros((self.nof_sources, 1))\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.from_mcmc_dist","title":"<code>from_mcmc_dist(store)</code>","text":"<p>Extract posterior emission rate samples from the MCMC sampler, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc_dist(self, store: dict):\n    \"\"\"Extract posterior emission rate samples from the MCMC sampler, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n    self.emission_rate = store[self.map[\"source\"]]\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel","title":"<code>SourceModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code>, <code>SourceGrouping</code>, <code>SourceDistribution</code></p> <p>Superclass for the specification of the source model in an inversion run.</p> <p>Various different types of model. A SourceModel is an optional component of a model, and thus inherits from Component.</p> A subclass instance of SourceModel must inherit from <ul> <li>an INSTANCE of SourceDistribution, which specifies a prior emission rate distribution for all sources in the     source map.</li> <li>an INSTANCE of SourceGrouping, which specifies a type of mixture prior specification for the sources (for     which the allocation is to be estimated as part of the inversion).</li> </ul> <p>If the flag reversible_jump == True, then the number of sources and their locations are also estimated as part of the inversion, in addition to the emission rates. If this flag is set to true, the sensor_object, meteorology and gas_species objects are all attached to the class, as they will be required in the repeated computation of updates to the coupling matrix during the inversion.</p> <p>Attributes:</p> Name Type Description <code>dispersion_model</code> <code>GaussianPlume</code> <p>dispersion model used to generate the couplings between source locations and sensor observations.</p> <code>coupling</code> <code>ndarray</code> <p>coupling matrix generated using dispersion_model.</p> <code>sensor_object</code> <code>SensorGroup</code> <p>stores sensor information for reversible jump coupling updates.</p> <code>meteorology</code> <code>MeteorologyGroup</code> <p>stores meteorology information for reversible jump coupling updates.</p> <code>gas_species</code> <code>GasSpecies</code> <p>stores gas species information for reversible jump coupling updates.</p> <code>reversible_jump</code> <code>bool</code> <p>logical indicating whether the reversible jump algorithm for estimation of the number of sources and their locations should be run. Defaults to False.</p> <code>distribution_number_sources</code> <code>str</code> <p>distribution for the number of sources in the solution. Can be either \"Poisson\" or \"Uniform\". Defaults to \"Poisson\".</p> <code>random_walk_step_size</code> <code>ndarray</code> <p>(3 x 1) array specifying the standard deviations of the distributions from which the random walk sampler draws new source locations. Defaults to np.array([1.0, 1.0, 0.1]).</p> <code>site_limits</code> <code>ndarray</code> <p>(3 x 2) array specifying the lower (column 0) and upper (column 1) limits of the analysis site. Only relevant for cases where reversible_jump == True (where sources are free to move in the solution).</p> <code>rate_num_sources</code> <code>int</code> <p>specification for the parameter for the Poisson prior distribution for the total number of sources. Only relevant for cases where reversible_jump == True (where the number of sources in the solution can change). Unused in the case of a Uniform prior (self.distribution_number_sources == \"Uniform\").</p> <code>n_sources_max</code> <code>int</code> <p>maximum number of sources that can feature in the solution. Only relevant for cases where reversible_jump == True (where the number of sources in the solution can change).</p> <code>emission_proposal_std</code> <code>float</code> <p>standard deviation of the truncated Gaussian distribution used to propose the new source emission rate in case of a birth move.</p> <code>update_precision</code> <code>bool</code> <p>logical indicating whether the prior precision parameter for emission rates should be updated as part of the inversion. Defaults to false.</p> <code>prior_precision_shape</code> <code>Union[float, ndarray]</code> <p>shape parameters for the prior Gamma distribution for the source precision parameter.</p> <code>prior_precision_rate</code> <code>Union[float, ndarray]</code> <p>rate parameters for the prior Gamma distribution for the source precision parameter.</p> <code>initial_precision</code> <code>Union[float, ndarray]</code> <p>initial value for the source emission rate precision parameter.</p> <code>precision_scalar</code> <code>ndarray</code> <p>precision values generated by MCMC inversion.</p> <code>all_source_locations</code> <code>ENU</code> <p>ENU object containing the locations of after the mcmc has been run, therefore in the situation where the reversible_jump == True, this will be the final locations of the sources in the solution over all iterations. For the case where reversible_jump == False, this will be the locations of the sources in the source map and will not change during the course of the inversion.</p> <code>individual_source_labels</code> <code>list</code> <p>list of labels for each source in the source map, defaults to None.</p> <code>coverage_detection</code> <code>float</code> <p>sensor detection threshold (in ppm) to be used for coverage calculations.</p> <code>coverage_test_source</code> <code>float</code> <p>test source (in kg/hr) which we wish to be able to see in coverage calculation.</p> <code>threshold_function</code> <code>Callable</code> <p>Callable function which returns a single value that defines the threshold for the coupling in a lambda function form. Examples: lambda x: np.quantile(x, 0.95, axis=0), lambda x: np.max(x, axis=0), lambda x: np.mean(x, axis=0). Defaults to np.quantile.</p> <code>label_string</code> <code>str</code> <p>string to append to the parameter mapping, e.g. for fixed sources, defaults to None.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SourceModel(Component, SourceGrouping, SourceDistribution):\n    \"\"\"Superclass for the specification of the source model in an inversion run.\n\n    Various different types of model. A SourceModel is an optional component of a model, and thus inherits\n    from Component.\n\n    A subclass instance of SourceModel must inherit from:\n        - an INSTANCE of SourceDistribution, which specifies a prior emission rate distribution for all sources in the\n            source map.\n        - an INSTANCE of SourceGrouping, which specifies a type of mixture prior specification for the sources (for\n            which the allocation is to be estimated as part of the inversion).\n\n    If the flag reversible_jump == True, then the number of sources and their locations are also estimated as part of\n    the inversion, in addition to the emission rates. If this flag is set to true, the sensor_object, meteorology and\n    gas_species objects are all attached to the class, as they will be required in the repeated computation of updates\n    to the coupling matrix during the inversion.\n\n    Attributes:\n        dispersion_model (GaussianPlume): dispersion model used to generate the couplings between source locations and\n            sensor observations.\n        coupling (np.ndarray): coupling matrix generated using dispersion_model.\n\n        sensor_object (SensorGroup): stores sensor information for reversible jump coupling updates.\n        meteorology (MeteorologyGroup): stores meteorology information for reversible jump coupling updates.\n        gas_species (GasSpecies): stores gas species information for reversible jump coupling updates.\n\n        reversible_jump (bool): logical indicating whether the reversible jump algorithm for estimation of the number\n            of sources and their locations should be run. Defaults to False.\n        distribution_number_sources (str): distribution for the number of sources in the solution. Can be either\n            \"Poisson\" or \"Uniform\". Defaults to \"Poisson\".\n        random_walk_step_size (np.ndarray): (3 x 1) array specifying the standard deviations of the distributions\n            from which the random walk sampler draws new source locations. Defaults to np.array([1.0, 1.0, 0.1]).\n        site_limits (np.ndarray): (3 x 2) array specifying the lower (column 0) and upper (column 1) limits of the\n            analysis site. Only relevant for cases where reversible_jump == True (where sources are free to move in\n            the solution).\n        rate_num_sources (int): specification for the parameter for the Poisson prior distribution for the total number\n            of sources. Only relevant for cases where reversible_jump == True (where the number of sources in the\n            solution can change). Unused in the case of a Uniform prior (self.distribution_number_sources == \"Uniform\").\n        n_sources_max (int): maximum number of sources that can feature in the solution. Only relevant for cases where\n            reversible_jump == True (where the number of sources in the solution can change).\n        emission_proposal_std (float): standard deviation of the truncated Gaussian distribution used to propose the\n            new source emission rate in case of a birth move.\n\n        update_precision (bool): logical indicating whether the prior precision parameter for emission rates should be\n            updated as part of the inversion. Defaults to false.\n        prior_precision_shape (Union[float, np.ndarray]): shape parameters for the prior Gamma distribution for the\n            source precision parameter.\n        prior_precision_rate (Union[float, np.ndarray]): rate parameters for the prior Gamma distribution for the\n            source precision parameter.\n        initial_precision (Union[float, np.ndarray]): initial value for the source emission rate precision parameter.\n        precision_scalar (np.ndarray): precision values generated by MCMC inversion.\n\n        all_source_locations (ENU): ENU object containing the locations of after the mcmc has been run, therefore in\n            the situation where the reversible_jump == True, this will be the final locations of the sources in the\n            solution over all iterations. For the case where reversible_jump == False, this will be the locations of\n            the sources in the source map and will not change during the course of the inversion.\n        individual_source_labels (list, optional): list of labels for each source in the source map, defaults to None.\n\n        coverage_detection (float): sensor detection threshold (in ppm) to be used for coverage calculations.\n        coverage_test_source (float): test source (in kg/hr) which we wish to be able to see in coverage calculation.\n\n        threshold_function (Callable): Callable function which returns a single value that defines the threshold\n            for the coupling in a lambda function form. Examples: lambda x: np.quantile(x, 0.95, axis=0),\n            lambda x: np.max(x, axis=0), lambda x: np.mean(x, axis=0). Defaults to np.quantile.\n        label_string (str): string to append to the parameter mapping, e.g. for fixed sources, defaults to None.\n\n    \"\"\"\n\n    dispersion_model: GaussianPlume = field(init=False, default=None)\n    coupling: np.ndarray = field(init=False)\n\n    sensor_object: SensorGroup = field(init=False, default=None)\n    meteorology: Meteorology = field(init=False, default=None)\n    gas_species: GasSpecies = field(init=False, default=None)\n\n    reversible_jump: bool = False\n    distribution_number_sources: str = \"Poisson\"\n    random_walk_step_size: np.ndarray = field(default_factory=lambda: np.array([1.0, 1.0, 0.1], ndmin=2).T)\n    site_limits: np.ndarray = None\n    rate_num_sources: int = 5\n    n_sources_max: int = 20\n    emission_proposal_std: float = 0.5\n\n    update_precision: bool = False\n    prior_precision_shape: Union[float, np.ndarray] = 1e-3\n    prior_precision_rate: Union[float, np.ndarray] = 1e-3\n    initial_precision: Union[float, np.ndarray] = 1.0\n    precision_scalar: np.ndarray = field(init=False)\n\n    all_source_locations: np.ndarray = field(init=False)\n    individual_source_labels: Optional[list] = None\n\n    coverage_detection: float = 0.1\n    coverage_test_source: float = 6.0\n\n    threshold_function: callable = lambda x: np.quantile(x, 0.95, axis=0)\n\n    label_string: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Post-initialisation of the class.\n\n        This function is called after the class has been initialised, and is used to set up the mapping dictionary for\n        the class by applying the append_string function to the mapping dictionary.\n\n        \"\"\"\n        if self.label_string is not None:\n            self.append_string(self.label_string)\n\n    @property\n    def nof_sources(self):\n        \"\"\"Get number of sources in the source map.\"\"\"\n        return self.dispersion_model.source_map.nof_sources\n\n    @property\n    def coverage_threshold(self):\n        \"\"\"Compute coverage threshold from detection threshold and test source strength.\"\"\"\n        return self.coverage_test_source / self.coverage_detection\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n        \"\"\"Set up the source model.\n\n        Extract required information from the sensor, meteorology and gas species objects:\n            - Attach coupling calculated using self.dispersion_model.\n            - (If self.reversible_jump == True) Attach objects to source model which will be used in RJMCMC sampler,\n                they will be required when we need to update the couplings when new source locations are proposed when\n                we move/birth/death.\n\n        Args:\n            sensor_object (SensorGroup): object containing sensor data.\n            meteorology (MeteorologyGroup): object containing meteorology data.\n            gas_species (GasSpecies): object containing gas species information.\n\n        \"\"\"\n        self.initialise_dispersion_model(sensor_object)\n        self.coupling = self.dispersion_model.compute_coupling(\n            sensor_object, meteorology, gas_species, output_stacked=True\n        )\n\n        self.sensor_object = sensor_object\n        self.screen_coverage()\n        if self.reversible_jump:\n            self.meteorology = meteorology\n            self.gas_species = gas_species\n\n    def initialise_dispersion_model(self, sensor_object: SensorGroup):\n        \"\"\"Initialise the dispersion model.\n\n        If a dispersion_model has already been attached to this instance, then this function takes no action.\n\n        If a dispersion_model has not already been attached to the instance, then this function adds a GaussianPlume\n        dispersion model, with a default source map that has limits set based on the sensor locations.\n\n        Args:\n            sensor_object (SensorGroup): object containing sensor data.\n\n        \"\"\"\n        if self.dispersion_model is None:\n            source_map = SourceMap()\n            sensor_locations = sensor_object.location.to_enu()\n            location_object = ENU(\n                ref_latitude=sensor_locations.ref_latitude,\n                ref_longitude=sensor_locations.ref_longitude,\n                ref_altitude=sensor_locations.ref_altitude,\n            )\n            source_map.generate_sources(\n                coordinate_object=location_object,\n                sourcemap_limits=np.array(\n                    [\n                        [np.min(sensor_locations.east), np.max(sensor_locations.east)],\n                        [np.min(sensor_locations.north), np.max(sensor_locations.north)],\n                        [np.min(sensor_locations.up), np.max(sensor_locations.up)],\n                    ]\n                ),\n                sourcemap_type=\"grid\",\n            )\n            self.dispersion_model = GaussianPlume(source_map)\n\n    def screen_coverage(self):\n        \"\"\"Screen the initial source map for coverage.\"\"\"\n        in_coverage_area = self.compute_coverage(self.coupling)\n        self.coupling = self.coupling[:, in_coverage_area]\n        all_locations = self.dispersion_model.source_map.location.to_array()\n        screened_locations = all_locations[in_coverage_area, :]\n        self.dispersion_model.source_map.location.from_array(screened_locations)\n\n    def compute_coverage(self, couplings: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"Returns a logical vector that indicates which sources in the couplings are, or are not, within the coverage.\n\n        The 'coverage' is the area inside which all sources are well covered by wind data. E.g. If wind exclusively\n        blows towards East, then all sources to the East of any sensor are 'invisible', and are not within the coverage.\n\n        Couplings are returned in hr/kg. Some threshold function defines the largest allowed coupling value. This is\n        used to calculate estimated emission rates in kg/hr. Any emissions which are greater than the value of\n        'self.coverage_threshold' are defined as not within the coverage.\n\n        If sensor_object.source_on is being used only the parts where th coupling is computed are used in the coverage\n        check. This avoids threshold_function being affected by large amounts of zero values.\n\n        Args:\n            couplings (np.ndarray): Array of coupling values. Dimensions: n_data points x n_sources.\n            kwargs (dict, optional): Keyword arguments required for the threshold function.\n\n        Returns:\n            coverage (np.ndarray): A logical array specifying which sources are within the coverage.\n\n        \"\"\"\n        if self.sensor_object.source_on is not None:\n            couplings = deepcopy(couplings)\n            index_keep = self.sensor_object.source_on &gt; 0\n            couplings = couplings[index_keep]\n\n        coupling_threshold = self.threshold_function(couplings, **kwargs)\n        no_warning_threshold = np.where(coupling_threshold &lt;= 1e-100, 1, coupling_threshold)\n        no_warning_estimated_emission_rates = np.where(coupling_threshold &lt;= 1e-100, np.inf, 1 / no_warning_threshold)\n        coverage = no_warning_estimated_emission_rates &lt; self.coverage_threshold\n\n        return coverage\n\n    def update_coupling_column(self, state: dict, update_column: int) -&gt; dict:\n        \"\"\"Update the coupling, based on changes to the source locations as part of inversion.\n\n        To be used in two different situations:\n            - movement of source locations (e.g. Metropolis Hastings, random walk).\n            - adding of new source locations (e.g. reversible jump birth move).\n        If [update_column &lt; A.shape[1]]: an existing column of the A matrix is updated.\n        If [update_column == A.shape[1]]: a new column is appended to the right-hand side of the A matrix\n        (corresponding to a new source).\n\n        A central assumption of this function is that the sensor information and meteorology information\n        have already been interpolated onto the same space/time points.\n\n        If an update_column is supplied, the coupling for that source location only is calculated to save on\n        computation time. If update_column is None, then we just re-compute the whole coupling matrix.\n\n        Args:\n            state (dict): dictionary containing state parameters.\n            update_column (int): index of the coupling column to be updated.\n\n        Returns:\n            state (dict): state dictionary containing updated coupling information.\n\n        \"\"\"\n        self.dispersion_model.source_map.location.from_array(state[self.map[\"source_location\"]][:, [update_column]].T)\n        new_coupling = self.dispersion_model.compute_coupling(\n            self.sensor_object, self.meteorology, self.gas_species, output_stacked=True, run_interpolation=False\n        )\n\n        if update_column == state[self.map[\"coupling_matrix\"]].shape[1]:\n            state[self.map[\"coupling_matrix\"]] = np.concatenate(\n                (state[self.map[\"coupling_matrix\"]], new_coupling), axis=1\n            )\n        elif update_column &lt; state[self.map[\"coupling_matrix\"]].shape[1]:\n            state[self.map[\"coupling_matrix\"]][:, [update_column]] = new_coupling\n        else:\n            raise ValueError(\"Invalid column specification for updating.\")\n        return state\n\n    def birth_function(self, current_state: dict, prop_state: dict) -&gt; Tuple[dict, float, float]:\n        \"\"\"Update MCMC state based on source birth proposal.\n\n        Proposed state updated as follows:\n            1- Add column to coupling matrix for new source location.\n            2- If required, adjust other components of the state which correspond to the sources.\n        The source emission rate vector will be adjusted using the standardised functionality\n        in the openMCMC package.\n\n        After the coupling has been updated, a coverage test is applied for the new source\n        location. If the max coupling is too small, a large contribution is added to the\n        log-proposal density for the new state, to force the sampler to reject it.\n\n        A central assumption of this function is that the sensor information and meteorology information\n        have already been interpolated onto the same space/time points.\n\n        This function assumes that the new source location has been added as the final column of\n        the source location matrix, and so will correspondingly append the new coupling column to the right\n        hand side of the current state coupling, and append an emission rate as the last element of the\n        current state emission rate vector.\n\n        Args:\n            current_state (dict): dictionary containing parameters of the current state.\n            prop_state (dict): dictionary containing the parameters of the proposed state.\n\n        Returns:\n            prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n            logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n                (i.e. log[p(proposed | current)])\n            logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n                (i.e. log[p(current | proposed)])\n\n        \"\"\"\n        prop_state = self.update_coupling_column(prop_state, int(prop_state[self.map[\"number_sources\"]].item()) - 1)\n        prop_state[self.map[\"allocation\"]] = np.concatenate(\n            (prop_state[self.map[\"allocation\"]], np.array([0], ndmin=2)), axis=0\n        )\n        in_cov_area = self.compute_coverage(prop_state[self.map[\"coupling_matrix\"]][:, -1])\n        if not in_cov_area:\n            logp_pr_g_cr = 1e10\n        else:\n            logp_pr_g_cr = 0.0\n        logp_cr_g_pr = 0.0\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def death_function(self, current_state: dict, prop_state: dict, deletion_index: int) -&gt; Tuple[dict, float, float]:\n        \"\"\"Update MCMC state based on source death proposal.\n\n        Proposed state updated as follows:\n            1- Remove column from coupling for deleted source.\n            2- If required, adjust other components of the state which correspond to the sources.\n        The source emission rate vector will be adjusted using the standardised functionality in the general_mcmc repo.\n\n        A central assumption of this function is that the sensor information and meteorology information have already\n        been interpolated onto the same space/time points.\n\n        Args:\n            current_state (dict): dictionary containing parameters of the current state.\n            prop_state (dict): dictionary containing the parameters of the proposed state.\n            deletion_index (int): index of the source to be deleted in the overall set of sources.\n\n        Returns:\n            prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n            logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n                (i.e. log[p(proposed | current)])\n            logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n                (i.e. log[p(current | proposed)])\n\n        \"\"\"\n        prop_state[self.map[\"coupling_matrix\"]] = np.delete(\n            prop_state[self.map[\"coupling_matrix\"]], obj=deletion_index, axis=1\n        )\n        prop_state[self.map[\"allocation\"]] = np.delete(prop_state[self.map[\"allocation\"]], obj=deletion_index, axis=0)\n        logp_pr_g_cr = 0.0\n        logp_cr_g_pr = 0.0\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def move_function(self, prop_state: dict, update_column: int) -&gt; Tuple[dict, float, float]:\n        \"\"\"Re-compute the coupling after a source location move.\n\n        Function first updates the coupling column, and then checks whether the location passes a coverage test. If the\n        location does not have good enough coverage, we return a high log-probability of the move to reject.\n\n        Args:\n            prop_state (dict): dictionary containing parameters of the proposed state.\n            update_column (int): index of the coupling column to be updated.\n\n        Returns:\n            prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n            logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n                (i.e. log[p(proposed | current)])\n            logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n                (i.e. log[p(current | proposed)])\n\n        \"\"\"\n        prop_state = self.update_coupling_column(prop_state, update_column)\n        in_cov_area = self.compute_coverage(prop_state[self.map[\"coupling_matrix\"]][:, update_column])\n\n        if not in_cov_area:\n            logp_pr_g_cr = 1e10\n        else:\n            logp_pr_g_cr = 0.0\n        logp_cr_g_pr = 0.0\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def make_model(self, model: list) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list): Current list of model elements.\n\n        Returns:\n            list: model list updated with source-related distributions.\n\n        \"\"\"\n        model = self.make_allocation_model(model)\n        model = self.make_source_model(model)\n        if self.update_precision:\n            model.append(\n                Gamma(\n                    self.map[\"emission_rate_precision\"],\n                    shape=self.map[\"precision_prior_shape\"],\n                    rate=self.map[\"precision_prior_rate\"],\n                )\n            )\n        if self.reversible_jump:\n            model.append(\n                Uniform(\n                    response=self.map[\"source_location\"],\n                    domain_response_lower=self.site_limits[:, [0]],\n                    domain_response_upper=self.site_limits[:, [1]],\n                )\n            )\n            if self.distribution_number_sources == \"Uniform\":\n                model.append(\n                    Uniform(\n                        response=self.map[\"number_sources\"],\n                        domain_response_lower=1,\n                        domain_response_upper=self.n_sources_max,\n                    )\n                )\n            elif self.distribution_number_sources == \"Poisson\":\n                model.append(Poisson(response=self.map[\"number_sources\"], rate=self.map[\"number_source_rate\"]))\n            else:\n                raise ValueError(\"Invalid distribution type for number of sources.\")\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list): Current list of samplers.\n\n        Returns:\n            list: sampler list updated with source-related samplers.\n\n        \"\"\"\n        sampler_list = self.make_source_sampler(model, sampler_list)\n        sampler_list = self.make_allocation_sampler(model, sampler_list)\n        if self.update_precision:\n            sampler_list.append(NormalGamma(self.map[\"emission_rate_precision\"], model))\n        if self.reversible_jump:\n            sampler_list = self.make_sampler_rjmcmc(model, sampler_list)\n        return sampler_list\n\n    def make_state(self, state: dict) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict): current state vector.\n\n        Returns:\n            dict: current state vector with source-related parameters added.\n\n        \"\"\"\n        state = self.make_allocation_state(state)\n        state = self.make_source_state(state)\n        state[self.map[\"coupling_matrix\"]] = self.coupling\n        state[self.map[\"emission_rate_precision\"]] = np.array(self.initial_precision, ndmin=1)\n        if self.update_precision:\n            state[self.map[\"precision_prior_shape\"]] = np.ones_like(self.initial_precision) * self.prior_precision_shape\n            state[self.map[\"precision_prior_rate\"]] = np.ones_like(self.initial_precision) * self.prior_precision_rate\n        if self.reversible_jump:\n            state[self.map[\"source_location\"]] = self.dispersion_model.source_map.location.to_array().T\n            state[self.map[\"number_sources\"]] = np.array(\n                state[self.map[\"source_location\"]].shape[1], ndmin=2, dtype=int\n            )\n            state[self.map[\"number_source_rate\"]] = self.rate_num_sources\n        return state\n\n    def make_sampler_rjmcmc(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Create the parts of the sampler related to the reversible jump MCMC scheme.\n\n        RJ MCMC scheme:\n            - create the RandomWalkLoop sampler object which updates the source locations one-at-a-time.\n            - create the ReversibleJump sampler which proposes birth/death moves to add/remove sources from the source\n                map.\n\n        Args:\n            model (Model): model object containing probability density objects for all uncertain\n                parameters.\n            sampler_list (list): list of existing samplers.\n\n        Returns:\n            sampler_list (list): list of samplers updated with samplers corresponding to RJMCMC routine.\n\n        \"\"\"\n        for sampler in sampler_list:\n            if sampler.param == self.map[\"source\"]:\n                sampler.max_variable_size = self.n_sources_max\n\n        sampler_list.append(\n            RandomWalkLoop(\n                self.map[\"source_location\"],\n                model,\n                step=self.random_walk_step_size,\n                max_variable_size=(3, self.n_sources_max),\n                domain_limits=self.site_limits,\n                state_update_function=self.move_function,\n            )\n        )\n        matching_params = {\n            \"variable\": self.map[\"source\"],\n            \"matrix\": self.map[\"coupling_matrix\"],\n            \"scale\": 1.0,\n            \"limits\": [0.0, 1e6],\n        }\n        sampler_list.append(\n            ReversibleJump(\n                self.map[\"number_sources\"],\n                model,\n                step=np.array([1.0], ndmin=2),\n                associated_params=self.map[\"source_location\"],\n                n_max=self.n_sources_max,\n                state_birth_function=self.birth_function,\n                state_death_function=self.death_function,\n                matching_params=matching_params,\n            )\n        )\n        return sampler_list\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        For the reversible jump case we extract all estimated source locations\n        per iteration. For the fixed sources case we grab the source locations\n        from the inputted sourcemap and repeat those for all iterations.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.from_mcmc_group(store)\n        self.from_mcmc_dist(store)\n        if self.individual_source_labels is None:\n            self.individual_source_labels = list(np.repeat(None, store[self.map[\"source\"]].shape[0]))\n\n        if self.update_precision:\n            self.precision_scalar = store[self.map[\"emission_rate_precision\"]]\n\n        if self.reversible_jump:\n            reference_latitude = self.dispersion_model.source_map.location.ref_latitude\n            reference_longitude = self.dispersion_model.source_map.location.ref_longitude\n            ref_altitude = self.dispersion_model.source_map.location.ref_altitude\n            self.all_source_locations = ENU(\n                ref_latitude=reference_latitude,\n                ref_longitude=reference_longitude,\n                ref_altitude=ref_altitude,\n                east=store[self.map[\"source_location\"]][0, :, :],\n                north=store[self.map[\"source_location\"]][1, :, :],\n                up=store[self.map[\"source_location\"]][2, :, :],\n            )\n\n        else:\n            location_temp = self.dispersion_model.source_map.location.to_enu()\n            location_temp.east = np.repeat(location_temp.east[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n            location_temp.north = np.repeat(location_temp.north[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n            location_temp.up = np.repeat(location_temp.up[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n            self.all_source_locations = location_temp\n\n    def plot_iterations(self, plot: \"Plot\", burn_in_value: int, y_axis_type: str = \"linear\") -&gt; \"Plot\":\n        \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n        Args:\n            burn_in_value (int): Burn in value to show in plot.\n            y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary.\n\n        Returns:\n            plot (Plot): Plot object to which the figures added in the figure dictionary with\n                keys 'estimated_values_plot'/'log_estimated_values_plot' and 'number_of_sources_plot'\n\n        \"\"\"\n        plot.plot_emission_rate_estimates(source_model_object=self, burn_in=burn_in_value, y_axis_type=y_axis_type)\n        plot.plot_single_trace(object_to_plot=self)\n        return plot\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.nof_sources","title":"<code>nof_sources</code>  <code>property</code>","text":"<p>Get number of sources in the source map.</p>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.coverage_threshold","title":"<code>coverage_threshold</code>  <code>property</code>","text":"<p>Compute coverage threshold from detection threshold and test source strength.</p>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialisation of the class.</p> <p>This function is called after the class has been initialised, and is used to set up the mapping dictionary for the class by applying the append_string function to the mapping dictionary.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post-initialisation of the class.\n\n    This function is called after the class has been initialised, and is used to set up the mapping dictionary for\n    the class by applying the append_string function to the mapping dictionary.\n\n    \"\"\"\n    if self.label_string is not None:\n        self.append_string(self.label_string)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Set up the source model.</p> <p>Extract required information from the sensor, meteorology and gas species objects:     - Attach coupling calculated using self.dispersion_model.     - (If self.reversible_jump == True) Attach objects to source model which will be used in RJMCMC sampler,         they will be required when we need to update the couplings when new source locations are proposed when         we move/birth/death.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>object containing sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>object containing meteorology data.</p> required <code>gas_species</code> <code>GasSpecies</code> <p>object containing gas species information.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n    \"\"\"Set up the source model.\n\n    Extract required information from the sensor, meteorology and gas species objects:\n        - Attach coupling calculated using self.dispersion_model.\n        - (If self.reversible_jump == True) Attach objects to source model which will be used in RJMCMC sampler,\n            they will be required when we need to update the couplings when new source locations are proposed when\n            we move/birth/death.\n\n    Args:\n        sensor_object (SensorGroup): object containing sensor data.\n        meteorology (MeteorologyGroup): object containing meteorology data.\n        gas_species (GasSpecies): object containing gas species information.\n\n    \"\"\"\n    self.initialise_dispersion_model(sensor_object)\n    self.coupling = self.dispersion_model.compute_coupling(\n        sensor_object, meteorology, gas_species, output_stacked=True\n    )\n\n    self.sensor_object = sensor_object\n    self.screen_coverage()\n    if self.reversible_jump:\n        self.meteorology = meteorology\n        self.gas_species = gas_species\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.initialise_dispersion_model","title":"<code>initialise_dispersion_model(sensor_object)</code>","text":"<p>Initialise the dispersion model.</p> <p>If a dispersion_model has already been attached to this instance, then this function takes no action.</p> <p>If a dispersion_model has not already been attached to the instance, then this function adds a GaussianPlume dispersion model, with a default source map that has limits set based on the sensor locations.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>object containing sensor data.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def initialise_dispersion_model(self, sensor_object: SensorGroup):\n    \"\"\"Initialise the dispersion model.\n\n    If a dispersion_model has already been attached to this instance, then this function takes no action.\n\n    If a dispersion_model has not already been attached to the instance, then this function adds a GaussianPlume\n    dispersion model, with a default source map that has limits set based on the sensor locations.\n\n    Args:\n        sensor_object (SensorGroup): object containing sensor data.\n\n    \"\"\"\n    if self.dispersion_model is None:\n        source_map = SourceMap()\n        sensor_locations = sensor_object.location.to_enu()\n        location_object = ENU(\n            ref_latitude=sensor_locations.ref_latitude,\n            ref_longitude=sensor_locations.ref_longitude,\n            ref_altitude=sensor_locations.ref_altitude,\n        )\n        source_map.generate_sources(\n            coordinate_object=location_object,\n            sourcemap_limits=np.array(\n                [\n                    [np.min(sensor_locations.east), np.max(sensor_locations.east)],\n                    [np.min(sensor_locations.north), np.max(sensor_locations.north)],\n                    [np.min(sensor_locations.up), np.max(sensor_locations.up)],\n                ]\n            ),\n            sourcemap_type=\"grid\",\n        )\n        self.dispersion_model = GaussianPlume(source_map)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.screen_coverage","title":"<code>screen_coverage()</code>","text":"<p>Screen the initial source map for coverage.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def screen_coverage(self):\n    \"\"\"Screen the initial source map for coverage.\"\"\"\n    in_coverage_area = self.compute_coverage(self.coupling)\n    self.coupling = self.coupling[:, in_coverage_area]\n    all_locations = self.dispersion_model.source_map.location.to_array()\n    screened_locations = all_locations[in_coverage_area, :]\n    self.dispersion_model.source_map.location.from_array(screened_locations)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.compute_coverage","title":"<code>compute_coverage(couplings, **kwargs)</code>","text":"<p>Returns a logical vector that indicates which sources in the couplings are, or are not, within the coverage.</p> <p>The 'coverage' is the area inside which all sources are well covered by wind data. E.g. If wind exclusively blows towards East, then all sources to the East of any sensor are 'invisible', and are not within the coverage.</p> <p>Couplings are returned in hr/kg. Some threshold function defines the largest allowed coupling value. This is used to calculate estimated emission rates in kg/hr. Any emissions which are greater than the value of 'self.coverage_threshold' are defined as not within the coverage.</p> <p>If sensor_object.source_on is being used only the parts where th coupling is computed are used in the coverage check. This avoids threshold_function being affected by large amounts of zero values.</p> <p>Parameters:</p> Name Type Description Default <code>couplings</code> <code>ndarray</code> <p>Array of coupling values. Dimensions: n_data points x n_sources.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments required for the threshold function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>coverage</code> <code>ndarray</code> <p>A logical array specifying which sources are within the coverage.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def compute_coverage(self, couplings: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Returns a logical vector that indicates which sources in the couplings are, or are not, within the coverage.\n\n    The 'coverage' is the area inside which all sources are well covered by wind data. E.g. If wind exclusively\n    blows towards East, then all sources to the East of any sensor are 'invisible', and are not within the coverage.\n\n    Couplings are returned in hr/kg. Some threshold function defines the largest allowed coupling value. This is\n    used to calculate estimated emission rates in kg/hr. Any emissions which are greater than the value of\n    'self.coverage_threshold' are defined as not within the coverage.\n\n    If sensor_object.source_on is being used only the parts where th coupling is computed are used in the coverage\n    check. This avoids threshold_function being affected by large amounts of zero values.\n\n    Args:\n        couplings (np.ndarray): Array of coupling values. Dimensions: n_data points x n_sources.\n        kwargs (dict, optional): Keyword arguments required for the threshold function.\n\n    Returns:\n        coverage (np.ndarray): A logical array specifying which sources are within the coverage.\n\n    \"\"\"\n    if self.sensor_object.source_on is not None:\n        couplings = deepcopy(couplings)\n        index_keep = self.sensor_object.source_on &gt; 0\n        couplings = couplings[index_keep]\n\n    coupling_threshold = self.threshold_function(couplings, **kwargs)\n    no_warning_threshold = np.where(coupling_threshold &lt;= 1e-100, 1, coupling_threshold)\n    no_warning_estimated_emission_rates = np.where(coupling_threshold &lt;= 1e-100, np.inf, 1 / no_warning_threshold)\n    coverage = no_warning_estimated_emission_rates &lt; self.coverage_threshold\n\n    return coverage\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.update_coupling_column","title":"<code>update_coupling_column(state, update_column)</code>","text":"<p>Update the coupling, based on changes to the source locations as part of inversion.</p> To be used in two different situations <ul> <li>movement of source locations (e.g. Metropolis Hastings, random walk).</li> <li>adding of new source locations (e.g. reversible jump birth move).</li> </ul> <p>If [update_column &lt; A.shape[1]]: an existing column of the A matrix is updated. If [update_column == A.shape[1]]: a new column is appended to the right-hand side of the A matrix (corresponding to a new source).</p> <p>A central assumption of this function is that the sensor information and meteorology information have already been interpolated onto the same space/time points.</p> <p>If an update_column is supplied, the coupling for that source location only is calculated to save on computation time. If update_column is None, then we just re-compute the whole coupling matrix.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing state parameters.</p> required <code>update_column</code> <code>int</code> <p>index of the coupling column to be updated.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>state dictionary containing updated coupling information.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def update_coupling_column(self, state: dict, update_column: int) -&gt; dict:\n    \"\"\"Update the coupling, based on changes to the source locations as part of inversion.\n\n    To be used in two different situations:\n        - movement of source locations (e.g. Metropolis Hastings, random walk).\n        - adding of new source locations (e.g. reversible jump birth move).\n    If [update_column &lt; A.shape[1]]: an existing column of the A matrix is updated.\n    If [update_column == A.shape[1]]: a new column is appended to the right-hand side of the A matrix\n    (corresponding to a new source).\n\n    A central assumption of this function is that the sensor information and meteorology information\n    have already been interpolated onto the same space/time points.\n\n    If an update_column is supplied, the coupling for that source location only is calculated to save on\n    computation time. If update_column is None, then we just re-compute the whole coupling matrix.\n\n    Args:\n        state (dict): dictionary containing state parameters.\n        update_column (int): index of the coupling column to be updated.\n\n    Returns:\n        state (dict): state dictionary containing updated coupling information.\n\n    \"\"\"\n    self.dispersion_model.source_map.location.from_array(state[self.map[\"source_location\"]][:, [update_column]].T)\n    new_coupling = self.dispersion_model.compute_coupling(\n        self.sensor_object, self.meteorology, self.gas_species, output_stacked=True, run_interpolation=False\n    )\n\n    if update_column == state[self.map[\"coupling_matrix\"]].shape[1]:\n        state[self.map[\"coupling_matrix\"]] = np.concatenate(\n            (state[self.map[\"coupling_matrix\"]], new_coupling), axis=1\n        )\n    elif update_column &lt; state[self.map[\"coupling_matrix\"]].shape[1]:\n        state[self.map[\"coupling_matrix\"]][:, [update_column]] = new_coupling\n    else:\n        raise ValueError(\"Invalid column specification for updating.\")\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.birth_function","title":"<code>birth_function(current_state, prop_state)</code>","text":"<p>Update MCMC state based on source birth proposal.</p> Proposed state updated as follows <p>1- Add column to coupling matrix for new source location. 2- If required, adjust other components of the state which correspond to the sources.</p> <p>The source emission rate vector will be adjusted using the standardised functionality in the openMCMC package.</p> <p>After the coupling has been updated, a coverage test is applied for the new source location. If the max coupling is too small, a large contribution is added to the log-proposal density for the new state, to force the sampler to reject it.</p> <p>A central assumption of this function is that the sensor information and meteorology information have already been interpolated onto the same space/time points.</p> <p>This function assumes that the new source location has been added as the final column of the source location matrix, and so will correspondingly append the new coupling column to the right hand side of the current state coupling, and append an emission rate as the last element of the current state emission rate vector.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing parameters of the current state.</p> required <code>prop_state</code> <code>dict</code> <p>dictionary containing the parameters of the proposed state.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state, with coupling matrix and source emission rate vector updated.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-transition density of the proposed state given the current state (i.e. log[p(proposed | current)])</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-transition density of the current state given the proposed state (i.e. log[p(current | proposed)])</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def birth_function(self, current_state: dict, prop_state: dict) -&gt; Tuple[dict, float, float]:\n    \"\"\"Update MCMC state based on source birth proposal.\n\n    Proposed state updated as follows:\n        1- Add column to coupling matrix for new source location.\n        2- If required, adjust other components of the state which correspond to the sources.\n    The source emission rate vector will be adjusted using the standardised functionality\n    in the openMCMC package.\n\n    After the coupling has been updated, a coverage test is applied for the new source\n    location. If the max coupling is too small, a large contribution is added to the\n    log-proposal density for the new state, to force the sampler to reject it.\n\n    A central assumption of this function is that the sensor information and meteorology information\n    have already been interpolated onto the same space/time points.\n\n    This function assumes that the new source location has been added as the final column of\n    the source location matrix, and so will correspondingly append the new coupling column to the right\n    hand side of the current state coupling, and append an emission rate as the last element of the\n    current state emission rate vector.\n\n    Args:\n        current_state (dict): dictionary containing parameters of the current state.\n        prop_state (dict): dictionary containing the parameters of the proposed state.\n\n    Returns:\n        prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n        logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n            (i.e. log[p(proposed | current)])\n        logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n            (i.e. log[p(current | proposed)])\n\n    \"\"\"\n    prop_state = self.update_coupling_column(prop_state, int(prop_state[self.map[\"number_sources\"]].item()) - 1)\n    prop_state[self.map[\"allocation\"]] = np.concatenate(\n        (prop_state[self.map[\"allocation\"]], np.array([0], ndmin=2)), axis=0\n    )\n    in_cov_area = self.compute_coverage(prop_state[self.map[\"coupling_matrix\"]][:, -1])\n    if not in_cov_area:\n        logp_pr_g_cr = 1e10\n    else:\n        logp_pr_g_cr = 0.0\n    logp_cr_g_pr = 0.0\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.death_function","title":"<code>death_function(current_state, prop_state, deletion_index)</code>","text":"<p>Update MCMC state based on source death proposal.</p> Proposed state updated as follows <p>1- Remove column from coupling for deleted source. 2- If required, adjust other components of the state which correspond to the sources.</p> <p>The source emission rate vector will be adjusted using the standardised functionality in the general_mcmc repo.</p> <p>A central assumption of this function is that the sensor information and meteorology information have already been interpolated onto the same space/time points.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing parameters of the current state.</p> required <code>prop_state</code> <code>dict</code> <p>dictionary containing the parameters of the proposed state.</p> required <code>deletion_index</code> <code>int</code> <p>index of the source to be deleted in the overall set of sources.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state, with coupling matrix and source emission rate vector updated.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-transition density of the proposed state given the current state (i.e. log[p(proposed | current)])</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-transition density of the current state given the proposed state (i.e. log[p(current | proposed)])</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def death_function(self, current_state: dict, prop_state: dict, deletion_index: int) -&gt; Tuple[dict, float, float]:\n    \"\"\"Update MCMC state based on source death proposal.\n\n    Proposed state updated as follows:\n        1- Remove column from coupling for deleted source.\n        2- If required, adjust other components of the state which correspond to the sources.\n    The source emission rate vector will be adjusted using the standardised functionality in the general_mcmc repo.\n\n    A central assumption of this function is that the sensor information and meteorology information have already\n    been interpolated onto the same space/time points.\n\n    Args:\n        current_state (dict): dictionary containing parameters of the current state.\n        prop_state (dict): dictionary containing the parameters of the proposed state.\n        deletion_index (int): index of the source to be deleted in the overall set of sources.\n\n    Returns:\n        prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n        logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n            (i.e. log[p(proposed | current)])\n        logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n            (i.e. log[p(current | proposed)])\n\n    \"\"\"\n    prop_state[self.map[\"coupling_matrix\"]] = np.delete(\n        prop_state[self.map[\"coupling_matrix\"]], obj=deletion_index, axis=1\n    )\n    prop_state[self.map[\"allocation\"]] = np.delete(prop_state[self.map[\"allocation\"]], obj=deletion_index, axis=0)\n    logp_pr_g_cr = 0.0\n    logp_cr_g_pr = 0.0\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.move_function","title":"<code>move_function(prop_state, update_column)</code>","text":"<p>Re-compute the coupling after a source location move.</p> <p>Function first updates the coupling column, and then checks whether the location passes a coverage test. If the location does not have good enough coverage, we return a high log-probability of the move to reject.</p> <p>Parameters:</p> Name Type Description Default <code>prop_state</code> <code>dict</code> <p>dictionary containing parameters of the proposed state.</p> required <code>update_column</code> <code>int</code> <p>index of the coupling column to be updated.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state, with coupling matrix and source emission rate vector updated.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-transition density of the proposed state given the current state (i.e. log[p(proposed | current)])</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-transition density of the current state given the proposed state (i.e. log[p(current | proposed)])</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def move_function(self, prop_state: dict, update_column: int) -&gt; Tuple[dict, float, float]:\n    \"\"\"Re-compute the coupling after a source location move.\n\n    Function first updates the coupling column, and then checks whether the location passes a coverage test. If the\n    location does not have good enough coverage, we return a high log-probability of the move to reject.\n\n    Args:\n        prop_state (dict): dictionary containing parameters of the proposed state.\n        update_column (int): index of the coupling column to be updated.\n\n    Returns:\n        prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n        logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n            (i.e. log[p(proposed | current)])\n        logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n            (i.e. log[p(current | proposed)])\n\n    \"\"\"\n    prop_state = self.update_coupling_column(prop_state, update_column)\n    in_cov_area = self.compute_coverage(prop_state[self.map[\"coupling_matrix\"]][:, update_column])\n\n    if not in_cov_area:\n        logp_pr_g_cr = 1e10\n    else:\n        logp_pr_g_cr = 0.0\n    logp_cr_g_pr = 0.0\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_model","title":"<code>make_model(model)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model list updated with source-related distributions.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_model(self, model: list) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list): Current list of model elements.\n\n    Returns:\n        list: model list updated with source-related distributions.\n\n    \"\"\"\n    model = self.make_allocation_model(model)\n    model = self.make_source_model(model)\n    if self.update_precision:\n        model.append(\n            Gamma(\n                self.map[\"emission_rate_precision\"],\n                shape=self.map[\"precision_prior_shape\"],\n                rate=self.map[\"precision_prior_rate\"],\n            )\n        )\n    if self.reversible_jump:\n        model.append(\n            Uniform(\n                response=self.map[\"source_location\"],\n                domain_response_lower=self.site_limits[:, [0]],\n                domain_response_upper=self.site_limits[:, [1]],\n            )\n        )\n        if self.distribution_number_sources == \"Uniform\":\n            model.append(\n                Uniform(\n                    response=self.map[\"number_sources\"],\n                    domain_response_lower=1,\n                    domain_response_upper=self.n_sources_max,\n                )\n            )\n        elif self.distribution_number_sources == \"Poisson\":\n            model.append(Poisson(response=self.map[\"number_sources\"], rate=self.map[\"number_source_rate\"]))\n        else:\n            raise ValueError(\"Invalid distribution type for number of sources.\")\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_sampler","title":"<code>make_sampler(model, sampler_list)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler list updated with source-related samplers.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list): Current list of samplers.\n\n    Returns:\n        list: sampler list updated with source-related samplers.\n\n    \"\"\"\n    sampler_list = self.make_source_sampler(model, sampler_list)\n    sampler_list = self.make_allocation_sampler(model, sampler_list)\n    if self.update_precision:\n        sampler_list.append(NormalGamma(self.map[\"emission_rate_precision\"], model))\n    if self.reversible_jump:\n        sampler_list = self.make_sampler_rjmcmc(model, sampler_list)\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_state","title":"<code>make_state(state)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with source-related parameters added.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_state(self, state: dict) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict): current state vector.\n\n    Returns:\n        dict: current state vector with source-related parameters added.\n\n    \"\"\"\n    state = self.make_allocation_state(state)\n    state = self.make_source_state(state)\n    state[self.map[\"coupling_matrix\"]] = self.coupling\n    state[self.map[\"emission_rate_precision\"]] = np.array(self.initial_precision, ndmin=1)\n    if self.update_precision:\n        state[self.map[\"precision_prior_shape\"]] = np.ones_like(self.initial_precision) * self.prior_precision_shape\n        state[self.map[\"precision_prior_rate\"]] = np.ones_like(self.initial_precision) * self.prior_precision_rate\n    if self.reversible_jump:\n        state[self.map[\"source_location\"]] = self.dispersion_model.source_map.location.to_array().T\n        state[self.map[\"number_sources\"]] = np.array(\n            state[self.map[\"source_location\"]].shape[1], ndmin=2, dtype=int\n        )\n        state[self.map[\"number_source_rate\"]] = self.rate_num_sources\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_sampler_rjmcmc","title":"<code>make_sampler_rjmcmc(model, sampler_list)</code>","text":"<p>Create the parts of the sampler related to the reversible jump MCMC scheme.</p> RJ MCMC scheme <ul> <li>create the RandomWalkLoop sampler object which updates the source locations one-at-a-time.</li> <li>create the ReversibleJump sampler which proposes birth/death moves to add/remove sources from the source     map.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>model object containing probability density objects for all uncertain parameters.</p> required <code>sampler_list</code> <code>list</code> <p>list of existing samplers.</p> required <p>Returns:</p> Name Type Description <code>sampler_list</code> <code>list</code> <p>list of samplers updated with samplers corresponding to RJMCMC routine.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_sampler_rjmcmc(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Create the parts of the sampler related to the reversible jump MCMC scheme.\n\n    RJ MCMC scheme:\n        - create the RandomWalkLoop sampler object which updates the source locations one-at-a-time.\n        - create the ReversibleJump sampler which proposes birth/death moves to add/remove sources from the source\n            map.\n\n    Args:\n        model (Model): model object containing probability density objects for all uncertain\n            parameters.\n        sampler_list (list): list of existing samplers.\n\n    Returns:\n        sampler_list (list): list of samplers updated with samplers corresponding to RJMCMC routine.\n\n    \"\"\"\n    for sampler in sampler_list:\n        if sampler.param == self.map[\"source\"]:\n            sampler.max_variable_size = self.n_sources_max\n\n    sampler_list.append(\n        RandomWalkLoop(\n            self.map[\"source_location\"],\n            model,\n            step=self.random_walk_step_size,\n            max_variable_size=(3, self.n_sources_max),\n            domain_limits=self.site_limits,\n            state_update_function=self.move_function,\n        )\n    )\n    matching_params = {\n        \"variable\": self.map[\"source\"],\n        \"matrix\": self.map[\"coupling_matrix\"],\n        \"scale\": 1.0,\n        \"limits\": [0.0, 1e6],\n    }\n    sampler_list.append(\n        ReversibleJump(\n            self.map[\"number_sources\"],\n            model,\n            step=np.array([1.0], ndmin=2),\n            associated_params=self.map[\"source_location\"],\n            n_max=self.n_sources_max,\n            state_birth_function=self.birth_function,\n            state_death_function=self.death_function,\n            matching_params=matching_params,\n        )\n    )\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>For the reversible jump case we extract all estimated source locations per iteration. For the fixed sources case we grab the source locations from the inputted sourcemap and repeat those for all iterations.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    For the reversible jump case we extract all estimated source locations\n    per iteration. For the fixed sources case we grab the source locations\n    from the inputted sourcemap and repeat those for all iterations.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.from_mcmc_group(store)\n    self.from_mcmc_dist(store)\n    if self.individual_source_labels is None:\n        self.individual_source_labels = list(np.repeat(None, store[self.map[\"source\"]].shape[0]))\n\n    if self.update_precision:\n        self.precision_scalar = store[self.map[\"emission_rate_precision\"]]\n\n    if self.reversible_jump:\n        reference_latitude = self.dispersion_model.source_map.location.ref_latitude\n        reference_longitude = self.dispersion_model.source_map.location.ref_longitude\n        ref_altitude = self.dispersion_model.source_map.location.ref_altitude\n        self.all_source_locations = ENU(\n            ref_latitude=reference_latitude,\n            ref_longitude=reference_longitude,\n            ref_altitude=ref_altitude,\n            east=store[self.map[\"source_location\"]][0, :, :],\n            north=store[self.map[\"source_location\"]][1, :, :],\n            up=store[self.map[\"source_location\"]][2, :, :],\n        )\n\n    else:\n        location_temp = self.dispersion_model.source_map.location.to_enu()\n        location_temp.east = np.repeat(location_temp.east[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n        location_temp.north = np.repeat(location_temp.north[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n        location_temp.up = np.repeat(location_temp.up[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n        self.all_source_locations = location_temp\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.plot_iterations","title":"<code>plot_iterations(plot, burn_in_value, y_axis_type='linear')</code>","text":"<p>Plot the emission rate estimates source model object against MCMC iteration.</p> <p>Parameters:</p> Name Type Description Default <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>y_axis_type</code> <code>str</code> <p>String to indicate whether the y-axis should be linear of log scale.</p> <code>'linear'</code> <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary.</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which the figures added in the figure dictionary with keys 'estimated_values_plot'/'log_estimated_values_plot' and 'number_of_sources_plot'</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", burn_in_value: int, y_axis_type: str = \"linear\") -&gt; \"Plot\":\n    \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n    Args:\n        burn_in_value (int): Burn in value to show in plot.\n        y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary.\n\n    Returns:\n        plot (Plot): Plot object to which the figures added in the figure dictionary with\n            keys 'estimated_values_plot'/'log_estimated_values_plot' and 'number_of_sources_plot'\n\n    \"\"\"\n    plot.plot_emission_rate_estimates(source_model_object=self, burn_in=burn_in_value, y_axis_type=y_axis_type)\n    plot.plot_single_trace(object_to_plot=self)\n    return plot\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.Normal","title":"<code>Normal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceModel</code>, <code>NullGrouping</code>, <code>NormalResponse</code></p> <p>Normal model, with null allocation.</p> <p>(Truncated) Gaussian prior for emission rates, no grouping/allocation; no transformation applied to emission rate parameters.</p> Can be used in the following cases <ul> <li>Fixed set of sources (grid or specific locations), all with the same Gaussian prior distribution.</li> <li>Variable number of sources, with a common prior distribution, estimated using reversible jump MCMC.</li> <li>Fixed set of sources with a bespoke prior per source (using the allocation to map prior parameters onto     sources).</li> </ul> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass Normal(SourceModel, NullGrouping, NormalResponse):\n    \"\"\"Normal model, with null allocation.\n\n    (Truncated) Gaussian prior for emission rates, no grouping/allocation; no transformation applied to emission rate\n    parameters.\n\n    Can be used in the following cases:\n        - Fixed set of sources (grid or specific locations), all with the same Gaussian prior distribution.\n        - Variable number of sources, with a common prior distribution, estimated using reversible jump MCMC.\n        - Fixed set of sources with a bespoke prior per source (using the allocation to map prior parameters onto\n            sources).\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalSlabAndSpike","title":"<code>NormalSlabAndSpike</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceModel</code>, <code>SlabAndSpike</code>, <code>NormalResponse</code></p> <p>Normal Slab and Spike model.</p> <p>(Truncated) Gaussian prior for emission rates, slab and spike prior, with allocation estimation; no transformation applied to emission rate parameters.</p> <p>Attributes:</p> Name Type Description <code>initial_precision</code> <code>ndarray</code> <p>initial precision parameter for a slab and spike case. shape=(2, 1).</p> <code>emission_rate_mean</code> <code>ndarray</code> <p>emission rate prior mean for a slab and spike case. shape=(2, 1).</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass NormalSlabAndSpike(SourceModel, SlabAndSpike, NormalResponse):\n    \"\"\"Normal Slab and Spike model.\n\n    (Truncated) Gaussian prior for emission rates, slab and spike prior, with allocation estimation; no transformation\n    applied to emission rate parameters.\n\n    Attributes:\n        initial_precision (np.ndarray): initial precision parameter for a slab and spike case. shape=(2, 1).\n        emission_rate_mean (np.ndarray): emission rate prior mean for a slab and spike case. shape=(2, 1).\n\n    \"\"\"\n\n    initial_precision: np.ndarray = field(default_factory=lambda: np.array([1 / (10**2), 1 / (0.01**2)], ndmin=2).T)\n    emission_rate_mean: np.ndarray = field(default_factory=lambda: np.array([0, 0], ndmin=2).T)\n</code></pre>"},{"location":"pyelq/data_access/data_access/","title":"Data Access","text":""},{"location":"pyelq/data_access/data_access/#data-access","title":"Data access","text":"<p>Data access module.</p> <p>Superclass containing some common attributes and helper functions used in multiple data access classes</p>"},{"location":"pyelq/data_access/data_access/#pyelq.data_access.data_access.DataAccess","title":"<code>DataAccess</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>DataAccess superclass containing some common attributes and functionalities.</p> <p>This superclass is used to show the type of methods to implement when creating a new data access class. The data access classes are used to convert raw data into well-defined classes and objects which can be used by the rest of the package.</p> <p>Attributes:</p> Name Type Description <code>latitude_bounds</code> <code>tuple</code> <p>Tuple specifying (latitude_min, latitude_max)</p> <code>longitude_bounds</code> <code>tuple</code> <p>Tuple specifying (longitude_min, longitude_max)</p> <code>date_bounds</code> <code>tuple</code> <p>Tuple specifying (datetime_min, datetime_max)</p> Source code in <code>src/pyelq/data_access/data_access.py</code> <pre><code>@dataclass\nclass DataAccess(ABC):\n    \"\"\"DataAccess superclass containing some common attributes and functionalities.\n\n    This superclass is used to show the type of methods to implement when creating a new data access class. The data\n    access classes are used to convert raw data into well-defined classes and objects which can be used by the rest of\n    the package.\n\n    Attributes:\n        latitude_bounds (tuple, optional): Tuple specifying (latitude_min, latitude_max)\n        longitude_bounds (tuple, optional): Tuple specifying (longitude_min, longitude_max)\n        date_bounds (tuple, optional): Tuple specifying (datetime_min, datetime_max)\n\n    \"\"\"\n\n    latitude_bounds: tuple = (None, None)\n    longitude_bounds: tuple = (None, None)\n    date_bounds: tuple = (None, None)\n\n    @abstractmethod\n    def to_sensor(self, *args: Any, **kwargs: dict) -&gt; Union[Sensor, SensorGroup]:\n        \"\"\"Abstract method to convert raw data into a Sensor or SensorGroup object.\n\n        This method should be implemented to convert the raw data into a Sensor or SensorGroup object.\n\n        Args:\n            *args (Any): Variable length argument list of any type.\n            **kwargs (dict): Arbitrary keyword arguments\n\n        \"\"\"\n\n    @abstractmethod\n    def to_meteorology(self, *args: Any, **kwargs: dict) -&gt; Union[Meteorology, MeteorologyGroup]:\n        \"\"\"Abstract method to convert raw data into a Meteorology or MeteorologyGroup object.\n\n        This method should be implemented to convert the raw data into a Meteorology or MeteorologyGroup object.\n\n        Args:\n            *args (Any): Variable length argument list of any type.\n            **kwargs (dict): Arbitrary keyword arguments\n\n        \"\"\"\n\n    def _query_aoi(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Helper function to perform area of interest query on data.\n\n        Args:\n            data (pd.Dataframe): Pandas dataframe to perform the query on\n\n        \"\"\"\n        aoi_query_string = \"\"\n        if self.latitude_bounds[0] is not None:\n            aoi_query_string += f\" &amp; latitude&gt;={self.latitude_bounds[0]}\"\n        if self.latitude_bounds[1] is not None:\n            aoi_query_string += f\" &amp; latitude&lt;={self.latitude_bounds[1]}\"\n        if self.longitude_bounds[0] is not None:\n            aoi_query_string += f\" &amp; longitude&gt;={self.longitude_bounds[0]}\"\n        if self.longitude_bounds[1] is not None:\n            aoi_query_string += f\" &amp; longitude&lt;={self.longitude_bounds[1]}\"\n        if len(aoi_query_string) &gt; 0:\n            aoi_query_string = aoi_query_string[3:]\n            return data.query(aoi_query_string).copy()\n        return data\n\n    def _query_time(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Helper function to perform time query on data.\n\n        Args:\n            data (pd.Dataframe): Pandas dataframe to perform the query on\n\n        \"\"\"\n        time_query_string = \"\"\n        if self.date_bounds[0] is not None:\n            timestamp_min = dt.datetime.timestamp(self.date_bounds[0])\n            time_query_string += f\" &amp; timestamp&gt;={timestamp_min}\"\n        if self.date_bounds[1] is not None:\n            timestamp_max = dt.datetime.timestamp(self.date_bounds[1])\n            time_query_string += f\" &amp; timestamp&lt;={timestamp_max}\"\n        if len(time_query_string) &gt; 0:\n            time_query_string = time_query_string[3:]\n            return data.query(time_query_string).copy()\n        return data\n</code></pre>"},{"location":"pyelq/data_access/data_access/#pyelq.data_access.data_access.DataAccess.to_sensor","title":"<code>to_sensor(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to convert raw data into a Sensor or SensorGroup object.</p> <p>This method should be implemented to convert the raw data into a Sensor or SensorGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list of any type.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Arbitrary keyword arguments</p> <code>{}</code> Source code in <code>src/pyelq/data_access/data_access.py</code> <pre><code>@abstractmethod\ndef to_sensor(self, *args: Any, **kwargs: dict) -&gt; Union[Sensor, SensorGroup]:\n    \"\"\"Abstract method to convert raw data into a Sensor or SensorGroup object.\n\n    This method should be implemented to convert the raw data into a Sensor or SensorGroup object.\n\n    Args:\n        *args (Any): Variable length argument list of any type.\n        **kwargs (dict): Arbitrary keyword arguments\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/data_access/data_access/#pyelq.data_access.data_access.DataAccess.to_meteorology","title":"<code>to_meteorology(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to convert raw data into a Meteorology or MeteorologyGroup object.</p> <p>This method should be implemented to convert the raw data into a Meteorology or MeteorologyGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list of any type.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Arbitrary keyword arguments</p> <code>{}</code> Source code in <code>src/pyelq/data_access/data_access.py</code> <pre><code>@abstractmethod\ndef to_meteorology(self, *args: Any, **kwargs: dict) -&gt; Union[Meteorology, MeteorologyGroup]:\n    \"\"\"Abstract method to convert raw data into a Meteorology or MeteorologyGroup object.\n\n    This method should be implemented to convert the raw data into a Meteorology or MeteorologyGroup object.\n\n    Args:\n        *args (Any): Variable length argument list of any type.\n        **kwargs (dict): Arbitrary keyword arguments\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/dispersion_model/dispersion_model/","title":"Overview","text":""},{"location":"pyelq/dispersion_model/dispersion_model/#dispersion-model","title":"Dispersion Model","text":"<p>DispersionModel module.</p> <p>The super class for the Gaussian Plume and Finite Volume dispersion models used in pyELQ.</p> <p>The Mathematics of Atmospheric Dispersion Modeling, John M. Stockie, DOI. 10.1137/10080991X</p>"},{"location":"pyelq/dispersion_model/dispersion_model/#pyelq.dispersion_model.dispersion_model.DispersionModel","title":"<code>DispersionModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the dispersion model class.</p> <p>Attributes:</p> Name Type Description <code>source_map</code> <code>Sourcemap</code> <p>SourceMap object used for the dispersion model.</p> <code>minimum_contribution</code> <code>float</code> <p>All elements in the plume coupling smaller than this number will be set to 0. Helps to speed up matrix multiplications/matrix inverses, also helps with stability.</p> Source code in <code>src/pyelq/dispersion_model/dispersion_model.py</code> <pre><code>@dataclass\nclass DispersionModel(ABC):\n    \"\"\"Defines the dispersion model class.\n\n    Attributes:\n        source_map (Sourcemap): SourceMap object used for the dispersion model.\n        minimum_contribution (float): All elements in the plume coupling smaller than this number will be set\n            to 0. Helps to speed up matrix multiplications/matrix inverses, also helps with stability.\n\n    \"\"\"\n\n    source_map: SourceMap\n    minimum_contribution: float = 0\n\n    def calculate_gas_density(\n        self,\n        meteorology: Meteorology,\n        sensor_object: Sensor,\n        gas_object: Union[GasSpecies, None],\n        run_interpolation: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Helper function to calculate the gas density using ideal gas law.\n\n        https://en.wikipedia.org/wiki/Ideal_gas\n\n        When a gas object is passed as input we calculate the density according to that gas. We check if the\n        meteorology object has a temperature and/or pressure value and use those accordingly. Otherwise, we use Standard\n        Temperature and Pressure (STP).\n\n        If run_interpolation is True, we interpolate the temperature and pressure values to the source locations/times\n        such that this is consistent with the other calculations, i.e. we only do spatial interpolation when the sensor\n        is a Satellite object and temporal interpolation otherwise.\n\n        When no gas_object is passed in we just set the gas density value to 1.\n\n        Args:\n            meteorology (Meteorology): Meteorology object potentially containing temperature or pressure values\n            sensor_object (Sensor): Sensor object containing information about where to interpolate to\n            gas_object (Union[GasSpecies, None]): Gas species object which actually calculates the correct density\n            run_interpolation (bool): Flag indicating whether to run interpolation, defaults to True.\n\n        Returns:\n            gas_density (np.ndarray): Numpy array of shape [1 x nof_sources] (Satellite sensor)\n                or [nof_observations x 1] (otherwise) containing the gas density values to use\n\n        \"\"\"\n        if not isinstance(gas_object, GasSpecies):\n            if isinstance(sensor_object, Satellite):\n                return np.ones((1, self.source_map.nof_sources))\n            return np.ones((sensor_object.nof_observations, 1))\n\n        if meteorology.temperature is None:\n            temperature = np.array([[273.15]])\n\n        elif run_interpolation:\n            temperature = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"temperature\", sensor_object=sensor_object\n            )\n        else:\n            temperature = meteorology.temperature\n\n        if meteorology.pressure is None:\n            pressure = np.array([[101.325]])\n        elif run_interpolation:\n            pressure = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"pressure\", sensor_object=sensor_object\n            )\n        else:\n            pressure = meteorology.pressure\n\n        gas_density = gas_object.gas_density(temperature=temperature, pressure=pressure)\n        return gas_density\n\n    def interpolate_all_meteorology(\n        self,\n        sensor_object: Sensor,\n        meteorology: Meteorology,\n        gas_object: Union[GasSpecies, None],\n        run_interpolation: bool,\n    ):\n        \"\"\"Function which carries out interpolation of all meteorological information.\n\n        The flag run_interpolation determines whether the interpolation should be carried out. If this is set to be\n        False, the meteorological parameters are simply set to the values stored on the meteorology object (i.e. we\n        assume that the meteorology has already been interpolated). This functionality is required to avoid wasted\n        computation in the case of e.g. a reversible jump run.\n\n        Args:\n            sensor_object (Sensor): object containing locations/times onto which met information should\n                be interpolated.\n            meteorology (Meteorology): object containing meteorology information for interpolation.\n            gas_object (Union[GasSpecies, None]): object containing gas information.\n            run_interpolation (bool): logical indicating whether the meteorology information needs to be interpolated.\n\n        Returns:\n            gas_density (np.ndarray): numpy array of shape [n_data x 1] of gas densities.\n            u_interpolated (np.ndarray): numpy array of shape [n_data x 1] of northerly wind components.\n            v_interpolated (np.ndarray): numpy array of shape [n_data x 1] of easterly wind components.\n            wind_turbulence_horizontal (np.ndarray): numpy array of shape [n_data x 1] of horizontal turbulence\n                parameters.\n            wind_turbulence_vertical (np.ndarray): numpy array of shape [n_data x 1] of vertical turbulence\n                parameters.\n\n        \"\"\"\n        if run_interpolation:\n            gas_density = self.calculate_gas_density(\n                meteorology=meteorology, sensor_object=sensor_object, gas_object=gas_object\n            )\n            u_interpolated = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"u_component\", sensor_object=sensor_object\n            )\n            v_interpolated = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"v_component\", sensor_object=sensor_object\n            )\n            wind_turbulence_horizontal = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"wind_turbulence_horizontal\", sensor_object=sensor_object\n            )\n            wind_turbulence_vertical = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"wind_turbulence_vertical\", sensor_object=sensor_object\n            )\n        else:\n            if gas_object is None:\n                gas_density = np.ones((meteorology.nof_observations, 1))\n            else:\n                gas_density = gas_object.gas_density(temperature=meteorology.temperature, pressure=meteorology.pressure)\n                gas_density = gas_density.reshape((gas_density.size, 1))\n            u_interpolated = meteorology.u_component.reshape((meteorology.u_component.size, 1))\n            v_interpolated = meteorology.v_component.reshape((meteorology.v_component.size, 1))\n            wind_turbulence_horizontal = meteorology.wind_turbulence_horizontal.reshape(\n                (meteorology.wind_turbulence_horizontal.size, 1)\n            )\n            wind_turbulence_vertical = meteorology.wind_turbulence_vertical.reshape(\n                (meteorology.wind_turbulence_vertical.size, 1)\n            )\n\n        return gas_density, u_interpolated, v_interpolated, wind_turbulence_horizontal, wind_turbulence_vertical\n\n    def interpolate_meteorology(\n        self, meteorology: Meteorology, variable_name: str, sensor_object: Sensor\n    ) -&gt; Union[np.ndarray, None]:\n        \"\"\"Helper function to interpolate meteorology variables.\n\n        This function interpolates meteorological variables to times in Sensor or Sources in sourcemap. It also\n        calculates the wind speed and mathematical angle between the u- and v-components which in turn gets used in the\n        calculation of the Gaussian plume.\n\n        When the input sensor object is a Satellite type we use spatial interpolation using the interpolation method\n        from the coordinate system class as this takes care of the coordinate systems.\n        When the input sensor object is of another time we use temporal interpolation (assumption is spatial uniformity\n        for all observations over a small(er) area).\n\n        Args:\n            meteorology (Meteorology): Meteorology object containing u- and v-components of wind including their\n                spatial location\n            variable_name (str): String name of an attribute in the meteorology input object which needs to be\n                interpolated\n            sensor_object (Sensor): Sensor object containing information about where to interpolate to\n\n        Returns:\n            variable_interpolated (np.ndarray): Interpolated values\n\n        \"\"\"\n        variable = getattr(meteorology, variable_name)\n        if variable is None:\n            return None\n\n        if isinstance(sensor_object, Satellite):\n            variable_interpolated = meteorology.location.interpolate(variable, self.source_map.location)\n            variable_interpolated = variable_interpolated.reshape(1, self.source_map.nof_sources)\n        else:\n            variable_interpolated = sti.interpolate(\n                time_in=meteorology.time, values_in=variable, time_out=sensor_object.time\n            )\n            variable_interpolated = variable_interpolated.reshape(sensor_object.nof_observations, 1)\n        return variable_interpolated\n</code></pre>"},{"location":"pyelq/dispersion_model/dispersion_model/#pyelq.dispersion_model.dispersion_model.DispersionModel.calculate_gas_density","title":"<code>calculate_gas_density(meteorology, sensor_object, gas_object, run_interpolation=True)</code>","text":"<p>Helper function to calculate the gas density using ideal gas law.</p> <p>https://en.wikipedia.org/wiki/Ideal_gas</p> <p>When a gas object is passed as input we calculate the density according to that gas. We check if the meteorology object has a temperature and/or pressure value and use those accordingly. Otherwise, we use Standard Temperature and Pressure (STP).</p> <p>If run_interpolation is True, we interpolate the temperature and pressure values to the source locations/times such that this is consistent with the other calculations, i.e. we only do spatial interpolation when the sensor is a Satellite object and temporal interpolation otherwise.</p> <p>When no gas_object is passed in we just set the gas density value to 1.</p> <p>Parameters:</p> Name Type Description Default <code>meteorology</code> <code>Meteorology</code> <p>Meteorology object potentially containing temperature or pressure values</p> required <code>sensor_object</code> <code>Sensor</code> <p>Sensor object containing information about where to interpolate to</p> required <code>gas_object</code> <code>Union[GasSpecies, None]</code> <p>Gas species object which actually calculates the correct density</p> required <code>run_interpolation</code> <code>bool</code> <p>Flag indicating whether to run interpolation, defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>gas_density</code> <code>ndarray</code> <p>Numpy array of shape [1 x nof_sources] (Satellite sensor) or [nof_observations x 1] (otherwise) containing the gas density values to use</p> Source code in <code>src/pyelq/dispersion_model/dispersion_model.py</code> <pre><code>def calculate_gas_density(\n    self,\n    meteorology: Meteorology,\n    sensor_object: Sensor,\n    gas_object: Union[GasSpecies, None],\n    run_interpolation: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Helper function to calculate the gas density using ideal gas law.\n\n    https://en.wikipedia.org/wiki/Ideal_gas\n\n    When a gas object is passed as input we calculate the density according to that gas. We check if the\n    meteorology object has a temperature and/or pressure value and use those accordingly. Otherwise, we use Standard\n    Temperature and Pressure (STP).\n\n    If run_interpolation is True, we interpolate the temperature and pressure values to the source locations/times\n    such that this is consistent with the other calculations, i.e. we only do spatial interpolation when the sensor\n    is a Satellite object and temporal interpolation otherwise.\n\n    When no gas_object is passed in we just set the gas density value to 1.\n\n    Args:\n        meteorology (Meteorology): Meteorology object potentially containing temperature or pressure values\n        sensor_object (Sensor): Sensor object containing information about where to interpolate to\n        gas_object (Union[GasSpecies, None]): Gas species object which actually calculates the correct density\n        run_interpolation (bool): Flag indicating whether to run interpolation, defaults to True.\n\n    Returns:\n        gas_density (np.ndarray): Numpy array of shape [1 x nof_sources] (Satellite sensor)\n            or [nof_observations x 1] (otherwise) containing the gas density values to use\n\n    \"\"\"\n    if not isinstance(gas_object, GasSpecies):\n        if isinstance(sensor_object, Satellite):\n            return np.ones((1, self.source_map.nof_sources))\n        return np.ones((sensor_object.nof_observations, 1))\n\n    if meteorology.temperature is None:\n        temperature = np.array([[273.15]])\n\n    elif run_interpolation:\n        temperature = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"temperature\", sensor_object=sensor_object\n        )\n    else:\n        temperature = meteorology.temperature\n\n    if meteorology.pressure is None:\n        pressure = np.array([[101.325]])\n    elif run_interpolation:\n        pressure = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"pressure\", sensor_object=sensor_object\n        )\n    else:\n        pressure = meteorology.pressure\n\n    gas_density = gas_object.gas_density(temperature=temperature, pressure=pressure)\n    return gas_density\n</code></pre>"},{"location":"pyelq/dispersion_model/dispersion_model/#pyelq.dispersion_model.dispersion_model.DispersionModel.interpolate_all_meteorology","title":"<code>interpolate_all_meteorology(sensor_object, meteorology, gas_object, run_interpolation)</code>","text":"<p>Function which carries out interpolation of all meteorological information.</p> <p>The flag run_interpolation determines whether the interpolation should be carried out. If this is set to be False, the meteorological parameters are simply set to the values stored on the meteorology object (i.e. we assume that the meteorology has already been interpolated). This functionality is required to avoid wasted computation in the case of e.g. a reversible jump run.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>object containing locations/times onto which met information should be interpolated.</p> required <code>meteorology</code> <code>Meteorology</code> <p>object containing meteorology information for interpolation.</p> required <code>gas_object</code> <code>Union[GasSpecies, None]</code> <p>object containing gas information.</p> required <code>run_interpolation</code> <code>bool</code> <p>logical indicating whether the meteorology information needs to be interpolated.</p> required <p>Returns:</p> Name Type Description <code>gas_density</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of gas densities.</p> <code>u_interpolated</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of northerly wind components.</p> <code>v_interpolated</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of easterly wind components.</p> <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of horizontal turbulence parameters.</p> <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of vertical turbulence parameters.</p> Source code in <code>src/pyelq/dispersion_model/dispersion_model.py</code> <pre><code>def interpolate_all_meteorology(\n    self,\n    sensor_object: Sensor,\n    meteorology: Meteorology,\n    gas_object: Union[GasSpecies, None],\n    run_interpolation: bool,\n):\n    \"\"\"Function which carries out interpolation of all meteorological information.\n\n    The flag run_interpolation determines whether the interpolation should be carried out. If this is set to be\n    False, the meteorological parameters are simply set to the values stored on the meteorology object (i.e. we\n    assume that the meteorology has already been interpolated). This functionality is required to avoid wasted\n    computation in the case of e.g. a reversible jump run.\n\n    Args:\n        sensor_object (Sensor): object containing locations/times onto which met information should\n            be interpolated.\n        meteorology (Meteorology): object containing meteorology information for interpolation.\n        gas_object (Union[GasSpecies, None]): object containing gas information.\n        run_interpolation (bool): logical indicating whether the meteorology information needs to be interpolated.\n\n    Returns:\n        gas_density (np.ndarray): numpy array of shape [n_data x 1] of gas densities.\n        u_interpolated (np.ndarray): numpy array of shape [n_data x 1] of northerly wind components.\n        v_interpolated (np.ndarray): numpy array of shape [n_data x 1] of easterly wind components.\n        wind_turbulence_horizontal (np.ndarray): numpy array of shape [n_data x 1] of horizontal turbulence\n            parameters.\n        wind_turbulence_vertical (np.ndarray): numpy array of shape [n_data x 1] of vertical turbulence\n            parameters.\n\n    \"\"\"\n    if run_interpolation:\n        gas_density = self.calculate_gas_density(\n            meteorology=meteorology, sensor_object=sensor_object, gas_object=gas_object\n        )\n        u_interpolated = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"u_component\", sensor_object=sensor_object\n        )\n        v_interpolated = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"v_component\", sensor_object=sensor_object\n        )\n        wind_turbulence_horizontal = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"wind_turbulence_horizontal\", sensor_object=sensor_object\n        )\n        wind_turbulence_vertical = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"wind_turbulence_vertical\", sensor_object=sensor_object\n        )\n    else:\n        if gas_object is None:\n            gas_density = np.ones((meteorology.nof_observations, 1))\n        else:\n            gas_density = gas_object.gas_density(temperature=meteorology.temperature, pressure=meteorology.pressure)\n            gas_density = gas_density.reshape((gas_density.size, 1))\n        u_interpolated = meteorology.u_component.reshape((meteorology.u_component.size, 1))\n        v_interpolated = meteorology.v_component.reshape((meteorology.v_component.size, 1))\n        wind_turbulence_horizontal = meteorology.wind_turbulence_horizontal.reshape(\n            (meteorology.wind_turbulence_horizontal.size, 1)\n        )\n        wind_turbulence_vertical = meteorology.wind_turbulence_vertical.reshape(\n            (meteorology.wind_turbulence_vertical.size, 1)\n        )\n\n    return gas_density, u_interpolated, v_interpolated, wind_turbulence_horizontal, wind_turbulence_vertical\n</code></pre>"},{"location":"pyelq/dispersion_model/dispersion_model/#pyelq.dispersion_model.dispersion_model.DispersionModel.interpolate_meteorology","title":"<code>interpolate_meteorology(meteorology, variable_name, sensor_object)</code>","text":"<p>Helper function to interpolate meteorology variables.</p> <p>This function interpolates meteorological variables to times in Sensor or Sources in sourcemap. It also calculates the wind speed and mathematical angle between the u- and v-components which in turn gets used in the calculation of the Gaussian plume.</p> <p>When the input sensor object is a Satellite type we use spatial interpolation using the interpolation method from the coordinate system class as this takes care of the coordinate systems. When the input sensor object is of another time we use temporal interpolation (assumption is spatial uniformity for all observations over a small(er) area).</p> <p>Parameters:</p> Name Type Description Default <code>meteorology</code> <code>Meteorology</code> <p>Meteorology object containing u- and v-components of wind including their spatial location</p> required <code>variable_name</code> <code>str</code> <p>String name of an attribute in the meteorology input object which needs to be interpolated</p> required <code>sensor_object</code> <code>Sensor</code> <p>Sensor object containing information about where to interpolate to</p> required <p>Returns:</p> Name Type Description <code>variable_interpolated</code> <code>ndarray</code> <p>Interpolated values</p> Source code in <code>src/pyelq/dispersion_model/dispersion_model.py</code> <pre><code>def interpolate_meteorology(\n    self, meteorology: Meteorology, variable_name: str, sensor_object: Sensor\n) -&gt; Union[np.ndarray, None]:\n    \"\"\"Helper function to interpolate meteorology variables.\n\n    This function interpolates meteorological variables to times in Sensor or Sources in sourcemap. It also\n    calculates the wind speed and mathematical angle between the u- and v-components which in turn gets used in the\n    calculation of the Gaussian plume.\n\n    When the input sensor object is a Satellite type we use spatial interpolation using the interpolation method\n    from the coordinate system class as this takes care of the coordinate systems.\n    When the input sensor object is of another time we use temporal interpolation (assumption is spatial uniformity\n    for all observations over a small(er) area).\n\n    Args:\n        meteorology (Meteorology): Meteorology object containing u- and v-components of wind including their\n            spatial location\n        variable_name (str): String name of an attribute in the meteorology input object which needs to be\n            interpolated\n        sensor_object (Sensor): Sensor object containing information about where to interpolate to\n\n    Returns:\n        variable_interpolated (np.ndarray): Interpolated values\n\n    \"\"\"\n    variable = getattr(meteorology, variable_name)\n    if variable is None:\n        return None\n\n    if isinstance(sensor_object, Satellite):\n        variable_interpolated = meteorology.location.interpolate(variable, self.source_map.location)\n        variable_interpolated = variable_interpolated.reshape(1, self.source_map.nof_sources)\n    else:\n        variable_interpolated = sti.interpolate(\n            time_in=meteorology.time, values_in=variable, time_out=sensor_object.time\n        )\n        variable_interpolated = variable_interpolated.reshape(sensor_object.nof_observations, 1)\n    return variable_interpolated\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/","title":"Finite Volume","text":""},{"location":"pyelq/dispersion_model/finite_volume/#finite-volume-model","title":"Finite Volume Model","text":"<p>Finite Volume Dispersion Model module.</p> <p>Methods and classes for the finite volume method for the dispersion model.</p>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume","title":"<code>FiniteVolume</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DispersionModel</code></p> <p>Dispersion model object which creates a coupling matrix using a finite volume solver.</p> <p>Uses an advection-diffusion solver to create the coupling matrix between a set of source locations and a set of sensor locations.</p> <p>Attributes:</p> Name Type Description <code>dimensions</code> <code>list</code> <p>list of FiniteVolumeDimension for each grid dimension (e.g., x, y, z).</p> <code>diffusion_constants</code> <code>ndarray</code> <p>array of diffusion constants [x,y,z], units m^2/s.</p> <code>site_layout</code> <code>Union[SiteLayout, None]</code> <p>the layout of the site including cylinder coordinates and radii. (default is None). If None, no obstacles are considered in the model.</p> <code>dt</code> <code>float</code> <p>time step (s) (default is None). (If None, the time step is set using the CFL condition).</p> <code>implicit_solver</code> <code>bool</code> <p>if True, the solver uses implicit methods. (default is False).</p> <code>courant_number</code> <code>float</code> <p>Courant number which and represents the fraction of the grid cell that a fluid particle can travel in one time step. It is used in calculating dt when not specified. Default is 0.5 which means that a fluid particle can travel half the grid cell in one time step.</p> <code>burn_in_steady_state</code> <code>bool</code> <p>if True, the model runs a burn-in period to reach steady state before computing coupling. (default is True).</p> <code>use_lookup_table</code> <code>bool</code> <p>if True, uses a lookup table for coupling matrix interpolation (default is True).</p> <code>grid_coordinates</code> <code>ndarray</code> <p>shape=(total_number_cells, number_dimensions), coordinates of the grid points.</p> <code>source_grid_link</code> <code>csr_array</code> <p>is a sparse matrix linking the source map to the grid coordinates.</p> <code>cell_volume</code> <code>float</code> <p>volume of a single grid cell.</p> <code>total_number_cells</code> <code>int</code> <p>total number of cells in the grid.</p> <code>grid_size</code> <code>tuple</code> <p>size of the grid in each dimension.</p> <code>grid_centers</code> <code>list</code> <p>centers of the grid cells in each dimension.</p> <code>number_dimensions</code> <code>int</code> <p>number of dimensions in the grid.</p> <code>adv_diff_terms</code> <code>dict</code> <p>contains advection and diffusion terms for the solver matrix.</p> <code>coupling_lookup_table</code> <code>ndarray</code> <p>coupling matrix calculated for each grid cell in grid_coordinates computed when use_lookup_table=True. It is used for interpolation of coupling values for new source locations without the need to re-run the FV solver.</p> <code>forward_matrix</code> <code>dia_array</code> <p>the solver matrix for the finite volume method.</p> <code>_forward_matrix_transpose</code> <code>dia_array</code> <p>the transpose of the solver matrix for the finite volume method.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>@dataclass\nclass FiniteVolume(DispersionModel):\n    \"\"\"Dispersion model object which creates a coupling matrix using a finite volume solver.\n\n    Uses an advection-diffusion solver to create the coupling matrix between a set of source locations and a set of\n    sensor locations.\n\n    Attributes:\n        dimensions (list): list of FiniteVolumeDimension for each grid dimension (e.g., x, y, z).\n        diffusion_constants (np.ndarray): array of diffusion constants [x,y,z], units m^2/s.\n        site_layout (Union[SiteLayout, None]): the layout of the site including cylinder coordinates and radii.\n            (default is None). If None, no obstacles are considered in the model.\n        dt (float): time step (s) (default is None). (If None, the time step is set using the CFL condition).\n        implicit_solver (bool): if True, the solver uses implicit methods. (default is False).\n        courant_number (float): Courant number which and represents the fraction of the grid cell that a fluid particle\n            can travel in one time step. It is used in calculating dt when not specified. Default is 0.5 which means\n            that a fluid particle can travel half the grid cell in one time step.\n        burn_in_steady_state (bool): if True, the model runs a burn-in period to reach steady state before\n            computing coupling. (default is True).\n        use_lookup_table (bool): if True, uses a lookup table for coupling matrix interpolation (default is True).\n\n        grid_coordinates (np.ndarray): shape=(total_number_cells, number_dimensions), coordinates of the grid points.\n        source_grid_link (csr_array): is a sparse matrix linking the source map to the grid coordinates.\n        cell_volume (float): volume of a single grid cell.\n        total_number_cells (int): total number of cells in the grid.\n        grid_size (tuple): size of the grid in each dimension.\n        grid_centers (list): centers of the grid cells in each dimension.\n        number_dimensions (int): number of dimensions in the grid.\n        adv_diff_terms (dict): contains advection and diffusion terms for the solver matrix.\n        coupling_lookup_table (np.ndarray): coupling matrix calculated for each grid cell in grid_coordinates computed\n            when use_lookup_table=True. It is used for interpolation of coupling values for new source locations without\n            the need to re-run the FV solver.\n        forward_matrix (dia_array): the solver matrix for the finite volume method.\n        _forward_matrix_transpose (dia_array): the transpose of the solver matrix for the finite volume method.\n\n    \"\"\"\n\n    dimensions: list = field(default_factory=list)\n    diffusion_constants: np.ndarray = field(default_factory=lambda: np.zeros((3, 1)))\n    site_layout: Union[SiteLayout, None] = field(default=None)\n    dt: Union[float, None] = field(default=None)\n    implicit_solver: bool = field(default=False)\n    courant_number: float = field(default=0.5)\n    burn_in_steady_state: bool = field(default=True)\n    use_lookup_table: bool = field(default=True)\n\n    grid_coordinates: np.ndarray = field(init=False)\n    source_grid_link: csr_array = field(init=False)\n    cell_volume: float = field(init=False)\n    total_number_cells: int = field(init=False)\n    grid_size: tuple = field(init=False)\n    grid_centers: list = field(init=False)\n    number_dimensions: int = field(init=False)\n    adv_diff_terms: dict = field(init=False)\n    coupling_lookup_table: np.ndarray = field(init=False, default=None)\n    forward_matrix: dia_array = field(init=False, default=None)\n    _forward_matrix_transpose: dia_array = field(init=False, default=None)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post-initialization checks and setup.\n\n        Creates the grid and neighbourhood for the finite volume solver, and uses the site layout to mask any obstacles\n        from the solver grid.\n\n        \"\"\"\n        if not isinstance(self.source_map.location, ENU):\n            raise ValueError(\"source_map.location must be an ENU object.\")\n        self.number_dimensions = len(self.dimensions)\n        self._setup_grid()\n        if self.site_layout is not None:\n            self.site_layout.find_index_obstacles(self.grid_coordinates)\n        self._setup_neighbourhood()\n\n    def compute_coupling(\n        self,\n        sensor_object: SensorGroup,\n        met_windfield: MeteorologyWindfield,\n        gas_object: Union[GasSpecies, None] = None,\n        output_stacked: bool = False,\n        **kwargs,\n    ) -&gt; Union[np.ndarray, dict]:\n        \"\"\"Compute the coupling matrix for the finite volume method using a lookup table.\n\n        If self.use_lookup_table == False, or if self.coupling_lookup_table is None, the coupling matrix is computed\n        using the FV solver and stored in self.coupling_lookup_table. Otherwise, the coupling matrix is computed using a\n        lookup table approach from the previously computed coupling matrix.\n\n        Args:\n            sensor_object (SensorGroup): sensor object containing sensor observations.\n            met_windfield (MeteorologyWindfield): meteorology object containing site layout and timeseries of wind data.\n            gas_object (Union[GasSpecies, None]): optional input, a gas species object to correctly calculate the\n                gas density which is used in the conversion of the units of the Gaussian plume coupling. Defaults to\n                None.\n            output_stacked (bool): if True, the coupling is stacked across sensors into a single np.ndarray. Otherwise,\n                the coupling is returned as a dictionary with an entry per sensor. Defaults to False.\n            **kwargs: additional keyword arguments. To accommodate some arguments used in\n                GaussianPlume.compute_coupling but not required in FiniteVolume.\n\n        Returns:\n            output (Union[np.ndarray, dict]): List of arrays, single array or dictionary containing the plume coupling\n                in hr/kg. If a dictionary of sensor objects is passed in and output_stacked=False, this function returns\n                a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling\n                outputs for each sensor. If a dictionary of sensor objects is passed in and output_stacked=True, this\n                function returns an np.ndarray containing the stacked coupling matrices.\n\n        \"\"\"\n        if (met_windfield.site_layout is not None) | (self.site_layout is not None):\n            if np.any(met_windfield.site_layout.id_obstacles != self.site_layout.id_obstacles):\n                raise ValueError(\"MeteorologyWindfield site layout does not match FiniteVolume site layout.\")\n        if not isinstance(self.source_map.location, ENU):\n            raise ValueError(\"source_map.location must be an ENU object.\")\n\n        if (not self.use_lookup_table) or (self.coupling_lookup_table is None):\n            coupling_sensor = self.compute_coupling_sections(sensor_object, met_windfield, gas_object)\n            if self.use_lookup_table:\n                self.coupling_lookup_table = coupling_sensor\n\n        if self.use_lookup_table:\n            output = self.interpolate_coupling_lookup_to_source_map(sensor_object)\n        else:\n            output = coupling_sensor\n        if output_stacked:\n            output = np.concatenate(tuple(output.values()), axis=0)\n        return output\n\n    def compute_coupling_sections(\n        self, sensor_object: SensorGroup, met_windfield: MeteorologyWindfield, gas_object: GasSpecies\n    ) -&gt; dict:\n        \"\"\"Compute the coupling sections for the finite volume method.\n\n        Sections are defined by the source_on attribute of the sensor object. If source_on is None (not specified) or\n        all ones, then it is treated as a single section of data and directly moves on to computing the coupling matrix.\n\n        If there are multiple sections, then the coupling matrix is computed for each section separately and combined\n        into a single coupling matrix. This avoids computational effort computing the forward model through time steps\n        that are not required and can speed up the computational time substantially in this case. Sections are\n        defined by the source_on attribute of the sensor object which indicates which time steps the source is on where\n        0 indicates the source is off and integers starting from 1 indicate different source on sections.\n\n        To avoid additional computational effort when a source is not emitting we do not compute the forward model when\n        the source_on attribute of the sensor object is set to 0. When a source starts emitting we can either assume it\n        was already emitting and calculate an equilibrium state by setting the burn_in_steady_state attribute to True.\n        Or we assume it starts right then and set this burn_in_steady_state attribute to False. When a source stops\n        emitting there is still some gas present in the area of interest and it will take some time for this gas to\n        disperse out of the area. However we assume the solution will not improve enough during this time to warrant\n        the additional computational effort to compute the forward model for this period. Which is why we stop computing\n        the forward model again when the source_on attribute switches from 1 to 0.\n\n        Args:\n            sensor_object (SensorGroup): sensor data object.\n            meteorology_object (MeteorologyWindfield): wind field data object.\n            gas_object (GasSpecies): gas species object.\n\n        Returns:\n            coupling_sensor (dict): coupling for each sensor, keys corresponding to each sensor: e.g.\n                coupling_sensor['sensor_1'] is the coupling matrix for sensor 1.\n\n        \"\"\"\n        if sensor_object.source_on is None or np.all(sensor_object.source_on == 1):\n            return self.finite_volume_time_step_solver(sensor_object, met_windfield, gas_object)\n\n        number_of_sections = max(sensor_object.source_on)\n        coupling_sensor = {}\n        for key, sensor in sensor_object.items():\n            coupling_sensor[key] = np.full((sensor.time.shape[0], self.source_grid_link.shape[1]), fill_value=0.0)\n        for section in range(1, number_of_sections + 1):\n            subset_sensor_object = sensor_object.subset_sensor(section_index=section)\n            coupling_sensor_section = self.finite_volume_time_step_solver(\n                subset_sensor_object, met_windfield, gas_object\n            )\n            for key, sensor in sensor_object.items():\n                section_index = (sensor.source_on == section).flatten()\n                coupling_sensor[key][section_index, :] = coupling_sensor_section[key]\n        return coupling_sensor\n\n    def finite_volume_time_step_solver(\n        self,\n        sensor_object: SensorGroup,\n        met_windfield: MeteorologyWindfield,\n        gas_object: GasSpecies,\n    ) -&gt; dict:\n        \"\"\"Compute the finite volume coupling matrix, by time-stepping the solver.\n\n        This function calculates the coupling between emission sources and sensor measurements based on a spatial wind\n        field derived from meteorological data. The resulting coupling matrices model the transport of gas through a\n        discretized domain. The coupling between emissions in all solver grid cells and concentrations in the same set\n        of grid cells is calculated by time-stepping a finite volume solver for the advection-diffusion equation. In\n        time bins where sensor observations occur, the coupling between any source locations in the source map and the\n        locations where sensor observations were obtained are extracted and stored in the rows of the coupling matrix.\n\n        If dt is not specified, it will be set automatically using a CFL-like condition via self.set_delta_time_cfl().\n        If burn_in_steady_state is True, the model runs a burn-in period to reach steady state before computing any\n        coupling values. The wind field during the burn-in period is assumed to be constant and the same as the wind\n        field at the first time-step.\n\n        If the coupling matrix is unstable (norm &gt; 1e3), an error is raised suggesting to check the CFL number and dt.\n        This condition is checked every 10% of the time steps.\n\n        Args:\n            sensor_object (SensorGroup): sensor data object.\n            meteorology_object (MeteorologyWindfield): wind field data object.\n            gas_object (GasSpecies): gas species object.\n\n        Returns:\n            coupling_sensor (dict): coupling matrix for each sensor and sources defined by source_grid_link units hr/kg.\n                coupling_sensor keys corresponding to each source, e.g. coupling_sensor['sensor_1'] =\n                coupling matrix for sensor 1 with shape=(number of observations (sensor_1), number of sources).\n\n        \"\"\"\n        coupling_sensor = {}\n        for key, sensor in sensor_object.items():\n            coupling_sensor[key] = np.full((sensor.time.shape[0], self.source_grid_link.shape[1]), fill_value=0.0)\n        coupling_grid = None\n        time_bins, time_index_sensor, time_index_met = self.compute_time_bins(\n            sensor_object=sensor_object, meteorology_object=met_windfield.static_wind_field\n        )\n        sensor_object = self._prepare_sensor(sensor_object)\n        n_burn_steps = self._calculate_number_burn_steps(met_windfield.static_wind_field)\n        gas_density = self.calculate_gas_density(\n            met_windfield.static_wind_field, sensor_object, gas_object, run_interpolation=False\n        )\n        met_windfield.calculate_spatial_wind_field(time_index=0, grid_coordinates=self.grid_coordinates)\n\n        for i_time in tqdm(range(-n_burn_steps, time_bins.size), desc=\"Computing coupling matrix\"):\n            if i_time &gt; 0 and (time_index_met[i_time] != time_index_met[i_time - 1]):\n                met_windfield.calculate_spatial_wind_field(\n                    time_index=time_index_met[i_time], grid_coordinates=self.grid_coordinates\n                )\n            if gas_density.size &gt; 1:\n                gas_density_i = gas_density[time_index_met[i_time]]\n            else:\n                gas_density_i = gas_density\n            coupling_grid = self.propagate_solver_single_time_step(met_windfield, coupling_matrix=coupling_grid)\n            scaled_coupling = coupling_grid * (1e6 / (gas_density_i.item() * 3600))\n            coupling_sensor = self.interpolate_coupling_grid_to_sensor(\n                sensor_object=sensor_object,\n                scaled_coupling=scaled_coupling,\n                time_index_sensor=time_index_sensor,\n                i_time=i_time,\n                coupling_sensor=coupling_sensor,\n            )\n            if i_time % np.floor(0.1 * (time_bins.size + n_burn_steps)) == 0:\n                coupling_grid_sourcemap_norm = sp.linalg.norm(coupling_grid)\n                if coupling_grid_sourcemap_norm &gt; 1e3:\n                    raise ValueError(\n                        f\"The coupling matrix is unstable, with matrix norm: {coupling_grid_sourcemap_norm:.3g}, \"\n                        f\"check the courant_number={self.courant_number:.3f} and calculated dt=\"\n                        f\"{self.dt:.3f} s\"\n                    )\n\n        return coupling_sensor\n\n    def interpolate_coupling_lookup_to_source_map(self, sensor_object: SensorGroup) -&gt; dict:\n        \"\"\"Compute the coupling matrix by interpolation from a lookup table.\n\n        A coupling matrix from all solver grid centres to all observations is pre-computed and stored on the class.\n        Coupling columns for new source locations can then be computed by interpolation from these pre-computed values.\n\n        The coupling matrix used for lookup is taken from self.coupling_lookup_table which is a sparse matrix computed\n        in self.finite_volume_time_step_solver().\n\n        Args:\n            sensor_object (SensorGroup): sensor data object.\n\n        Returns:\n            interpolated_coupling (dict): interpolated coupling matrix for each sensor and sources (units hr/kg).\n\n        \"\"\"\n        interpolated_coupling = {}\n        source_location = self.source_map.location.to_array(dim=self.number_dimensions)\n        for key, sensor in sensor_object.items():\n            interpolated_coupling[key] = np.full((sensor.time.shape[0], source_location.shape[0]), fill_value=0.0)\n            lookup_table_values = self.coupling_lookup_table[key].T\n            interpolated_coupling[key] = self._build_interpolator(\n                lookup_table_values, locations_to_interpolate=source_location\n            ).T\n        return interpolated_coupling\n\n    def propagate_solver_single_time_step(\n        self, met_windfield: MeteorologyWindfield, coupling_matrix: Union[sp.csc_array, None] = None\n    ) -&gt; sp.csc_array:\n        \"\"\"Time-step the finite volume solver.\n\n        Time-step the finite volume solver to map the coupling matrix at time t to the coupling matrix at time (t +\n        dt).\n\n        For each time step, the forward matrix is computed based on the current wind field. The coupling matrix is then\n        evolved by a single time-step using either an implicit or explicit solver approach, depending on the value of\n        self.implicit_solver.\n\n        coupling_matrix will be a sparse csc_array with shape=(total_number_cells, number of sources)\n\n        If minimum_contribution is set, all elements in the coupling matrix smaller than this number will be set to 0.\n        This can speed up computation.\n\n        Args:\n            met_windfield (MeteorologyWindfield): meteorology object containing wind field information.\n            coupling_matrix (Union[(sparse.csc_array, None]): shape=(self.total_number_cells, number of sources).\n                coupling matrix matrix on the finite volume grid if None will get preallocated for future time steps\n\n\n        Returns:\n            coupling_matrix (sparse.csc_array): shape=(self.total_number_cells, number of sources). Coupling\n                matrix on the finite volume grid. Represents the contribution of each cell to the source term in the\n                transport equation.\n\n        \"\"\"\n        self.compute_forward_matrix(met_windfield)\n        if coupling_matrix is None:\n            coupling_matrix = sp.csc_array(self.source_grid_link.shape, dtype=self.forward_matrix.dtype)\n        scale_factor = self.dt / self.cell_volume\n        if self.implicit_solver:\n            rhs = (1.0 / scale_factor) * coupling_matrix + self.source_grid_link\n            coupling_matrix = -spsolve(self.forward_matrix.tocsc(), rhs).reshape(self.source_grid_link.shape)\n            if not sp.issparse(coupling_matrix):\n                coupling_matrix = sp.csc_array(coupling_matrix)\n        else:\n            coupling_matrix = scale_factor * (self.forward_matrix @ coupling_matrix + self.source_grid_link)\n        if self.minimum_contribution &gt; 0:\n            coupling_matrix.data[abs(coupling_matrix.data) &lt;= self.minimum_contribution] = 0\n            coupling_matrix.eliminate_zeros()\n        if self.site_layout is not None:\n            coupling_matrix[self.site_layout.id_obstacles_index, :] = 0\n        return coupling_matrix\n\n    def compute_forward_matrix(self, met_windfield: MeteorologyWindfield) -&gt; None:\n        \"\"\"Construct the forward solver matrix. This can be used to step the solution forward in time.\n\n        The matrix forward_matrix is constructed using the advection and diffusion terms computed for each face in the\n        grid.\n\n        The overall matrix equation for the FV solver is:\n            (V / dt) * [c^(n+1) - c^(n)] + F @ c^(n) - G @ c^(n) = s\n        where F is the matrix of advection term coefficients, G is the matrix of diffusion term coefficients, and s is\n        the source term.\n\n        Rearranging gives:\n            c^(n+1) = R @ c^(n) + (dt / V) * s\n        where R = I - (dt / V) * (F - G).\n\n        The diagonals of the matrix are constructed using self._construct_diagonals_advection_diffusion() and combined\n        using self._combine_advection_diffusion_terms().\n\n        On first run, the matrix is constructed using self._construct_diagonal_matrix(). On subsequent runs, the matrix\n        is updated using self._update_diagonal_matrix() which saves computational time by updating the sparse matrix in\n        place.\n\n        Args:\n            met_windfield (MeteorologyWindfield): meteorology object containing wind field information.\n\n        \"\"\"\n        self._compute_advection_diffusion_terms_by_face(met_windfield)\n        self._construct_diagonals_advection_diffusion()\n        self._combine_advection_diffusion_terms()\n        if self.forward_matrix is None:\n            self._construct_diagonal_matrix()\n        else:\n            self._update_diagonal_matrix()\n\n    def _compute_advection_diffusion_terms_by_face(self, met_windfield: MeteorologyWindfield) -&gt; None:\n        \"\"\"Compute advection and diffusion terms for each face in the grid.\n\n        Loops over each dimension and face in the grid and computes the advection using the wind vector and the\n        diffusion terms using the diffusion constants.\n\n        Args:\n            met_windfield (MeteorologyWindfield): meteorology object containing wind field information.\n\n        \"\"\"\n        for i_dim, dim in enumerate(self.dimensions):\n            if i_dim == 0:\n                wind_component = met_windfield.u_component\n            elif i_dim == 1:\n                wind_component = met_windfield.v_component\n            elif i_dim == 2:\n                wind_component = met_windfield.w_component\n            else:\n                wind_component = None\n            for face in dim.faces:\n                face.assign_advection(wind_component)\n                face.assign_diffusion(self.diffusion_constants[i_dim])\n\n    def _construct_diagonals_advection_diffusion(self) -&gt; None:\n        \"\"\"Construct the diagonals of the advection and diffusion contributions to the overall solver matrix.\n\n        In the function self._combine_advection_diffusion_terms(), the coefficients of the advection\n        terms are stored in the matrix F, and the coefficients of the diffusion terms are stored in the matrix G. This\n        function creates the diagonals of the F and G matrices.\n\n        The overall diagonals are cumulated by looping over the solver dimensions, and the cell faces in each dimension.\n\n        \"\"\"\n        num_off_diags = self.number_dimensions * 2\n        self.adv_diff_terms = {\"advection\": SolverDiagonals(), \"diffusion\": SolverDiagonals()}\n        for key, term in self.adv_diff_terms.items():\n            term = self.adv_diff_terms[key]\n            term.B_central = np.zeros((self.total_number_cells, 1))\n            term.B_neighbour = np.zeros((self.total_number_cells, num_off_diags))\n            term.b_dirichlet = np.zeros((self.total_number_cells, 1))\n            term.b_neumann = np.zeros((self.total_number_cells, 1))\n            count = 0\n            for dim in self.dimensions:\n                for face in dim.faces:\n                    face_term = face.adv_diff_terms[key]\n                    term.B_central += face_term.B_central\n                    term.B_neighbour[:, count] = face_term.B_neighbour.flatten()\n                    term.b_dirichlet += face_term.b_dirichlet\n                    term.b_neumann += face_term.b_neumann\n                    count += 1\n            term.B = np.concatenate((term.B_central, term.B_neighbour), axis=1)\n\n    def _combine_advection_diffusion_terms(self) -&gt; None:\n        \"\"\"Combine the advection and diffusion terms into the solver matrix.\n\n        The overall matrix equation for the FV solver is:\n            (V / dt) * [c^(n+1) - c^(n)] + F @ c^(n) - G @ c^(n) = s\n        where F is the matrix of advection term coefficients, G is the matrix of diffusion term coefficients, and s is\n        the source term.\n\n        Rearranging gives:\n            c^(n+1) = R @ c^(n) + (dt / V) * s\n        where R = I - (dt / V) * (F - G).\n\n        This function calculates the diagonals of the matrix R by combining the advection and diffusion terms. These\n        diagonals are stored in self.adv_diff_terms['combined'].B.\n\n        \"\"\"\n        num_diags = 1 + self.number_dimensions * 2\n        terms = self.adv_diff_terms\n        terms[\"combined\"] = SolverDiagonals()\n        terms[\"combined\"].B = np.zeros((self.total_number_cells, num_diags))\n        if self.implicit_solver:\n            terms[\"combined\"].B[:, 0] = terms[\"combined\"].B[:, 0] - self.cell_volume / self.dt\n        else:\n            terms[\"combined\"].B[:, 0] = terms[\"combined\"].B[:, 0] + self.cell_volume / self.dt\n        terms[\"combined\"].B = terms[\"combined\"].B + terms[\"advection\"].B + terms[\"diffusion\"].B\n        terms[\"combined\"].b_dirichlet = terms[\"advection\"].b_dirichlet + terms[\"diffusion\"].b_dirichlet\n        terms[\"combined\"].b_neumann = terms[\"advection\"].b_neumann + terms[\"diffusion\"].b_neumann\n        terms[\"combined\"].B[:, 0] = terms[\"combined\"].B[:, 0] + terms[\"combined\"].b_neumann.flatten()\n\n    def _construct_diagonal_matrix(self) -&gt; None:\n        \"\"\"Construct the diagonal matrix for the solver.\n\n        This method creates a sparse diagonal matrix using the diagonals and the specified grid size.\n\n        The diagonal index is constructed based on the number of dimensions and the grid size using ravel_multi_index.\n        This index is used to place the diagonal elements in the correct location within the sparse matrix. It is\n        designed to be consistent with the meshgrid with indexing=\"ij\" used to construct grid_coordinates in\n        self._setup_grid().\n\n        The transposed matrix self._forward_matrix_transpose is constructed to deal with the way the zero-padding works\n        in dia_array then transposed to self.forward_matrix which is required for forward simulation.\n\n        The matrix self._forward_matrix_transpose is also stored to allow quick updating in self._update_diagonal_matrix\n\n        \"\"\"\n        diagonal_index = np.array([0])\n        start_coord = np.zeros(self.number_dimensions, dtype=int)\n        for i in range(self.number_dimensions):\n            diag_coord = start_coord.copy()\n            diag_coord[i] += 1\n            diag_coord = np.ravel_multi_index(diag_coord, self.grid_size, mode=\"clip\")\n            diagonal_index = np.concatenate((diagonal_index, np.array([diag_coord, -diag_coord])))\n\n        self._forward_matrix_transpose = dia_array(\n            (self.adv_diff_terms[\"combined\"].B.T, diagonal_index),\n            shape=(self.total_number_cells, self.total_number_cells),\n        )\n        self.forward_matrix = self._forward_matrix_transpose.T\n\n    def _update_diagonal_matrix(self) -&gt; None:\n        \"\"\"Update the self.forward_matrix for the solver.\n\n        This method updates the self.forward_matrix  using the updated adv_diff_terms Avoid reconstructing the sparse\n        matrix and just updates the data in place for speed purposes.\n\n        Since the forward_matrix is transposed, we need to update the transposed version of the forward matrix then\n        transpose it back.\n\n        \"\"\"\n        self._forward_matrix_transpose.data = self.adv_diff_terms[\"combined\"].B.T\n        self.forward_matrix = self._forward_matrix_transpose.T\n\n    def _setup_grid(self) -&gt; None:\n        \"\"\"Initializes a structured Cartesian grid using the site limits and number of cells in each dimension.\n\n        ENU CoordinateSystem reference location is taken from self.source_map.\n\n        This method builds a multi-dimensional grid by discretizing the spatial domain into equally spaced cells\n        along each axis (e.g., x, y, z).\n\n        Grid construction uses np.meshgrid with indexing=\"ij\" to be consistent with the way the diagonals are\n        constructed in self._construct_diagonal_matrix() and the way the neighbourhood is constructed in\n        self._setup_neighbourhood(). \"ij\" is the matrix indexing convention, which means that the first dimension\n        corresponds to rows and the second dimension corresponds to columns.\n\n        Volume and Area Calculations:\n            - self.cell_volume stores the volume of a single grid cell (product of widths).\n            - For each dimension, self.cell_face_area is computed as the ratio of cell volume to that dimension's width,\n                representing the area of a face perpendicular to the given axis.\n\n        \"\"\"\n        self.cell_volume = np.prod([dim.cell_width for dim in self.dimensions])\n        self.grid_centers = [dim.cell_centers for dim in self.dimensions]\n        for dim in self.dimensions:\n            for face in dim.faces:\n                face.cell_volume = self.cell_volume\n                face.cell_face_area = self.cell_volume / dim.cell_width\n        grid_coordinates = np.meshgrid(*[dim.cell_centers for dim in self.dimensions], indexing=\"ij\")\n        self.grid_size = grid_coordinates[0].shape\n        self.grid_coordinates = ENU(\n            ref_longitude=self.source_map.location.ref_longitude,\n            ref_latitude=self.source_map.location.ref_latitude,\n            ref_altitude=self.source_map.location.ref_altitude,\n        )\n        self.grid_coordinates.east = grid_coordinates[0].reshape(-1, 1)\n        if self.number_dimensions &gt; 1:\n            self.grid_coordinates.north = grid_coordinates[1].reshape(-1, 1)\n        if self.number_dimensions &gt; 2:\n            self.grid_coordinates.up = grid_coordinates[2].reshape(-1, 1)\n        self.total_number_cells = self.grid_coordinates.east.shape[0]\n        self._setup_source_link()\n\n    def _setup_source_link(self) -&gt; None:\n        \"\"\"Setup the source link between the source map and the grid coordinates.\n\n        This method creates a sparse matrix that links the source map to the grid coordinates.\n\n        Used in the coupling matrix to link the source map to the grid coordinates.\n\n        If there are no sources in the source map or if use_lookup_table is True, the source map locations are set to\n        the grid coordinates and the source_grid_link is set to an identity matrix.\n\n        If there are sources in the source map and use_lookup_table is False, a KDTree is used to find the nearest grid\n        point for each source location. The source_grid_link is then created as a sparse matrix with ones at the\n        locations of the nearest grid points and zeros elsewhere.\n\n        self.source_grid_link is a sparse matrix linking the source map to the grid coordinates.\n\n        \"\"\"\n        if self.use_lookup_table or self.source_map.nof_sources == 0:\n            if self.source_map.nof_sources == 0:\n                self.source_map.location = self.grid_coordinates\n            self.source_grid_link = sp.eye_array(self.total_number_cells, format=\"csr\")\n        else:\n            n_sources = self.source_map.nof_sources\n            tree = KDTree(self.grid_coordinates.to_array(dim=self.number_dimensions))\n            source_index = tree.query(self.source_map.location.to_array(dim=self.number_dimensions), k=1)[1]\n            self.source_grid_link = sp.csr_array(\n                (np.ones(n_sources), (source_index, np.array(range(n_sources)))),\n                shape=(self.total_number_cells, n_sources),\n            )\n\n    def _setup_neighbourhood(self) -&gt; None:\n        \"\"\"Initializes the neighborhood relationships for each cell in the grid across all dimensions.\n\n        For a given dim and face, to find the neighbor indices for each cell, the index is unwrapped and converted to\n        multi-dimensional indices using np.unravel_index.\n            e.g. if grid_size = (10,10) then for the cell index 27,\n                index_center = np.unravel_index(27, (10,10))  = (2,7)\n            we find the neighbor index by shifting the multi-dimensional indices by the face shift:\n            for left face in x-dimension, shift = -1, so the new multi-dimensional indices are (1, 7))\n            then we convert back to the unwrapped index using\n                index_neighbour = np.ravel_multi_index((1,7), (10,10)) = 17\n            for right face in y-dimension, shift = 1, so the new multi-dimensional indices are (2, 8))\n            then we convert back to the unwrapped index using\n                index_neighbour = np.ravel_multi_index((2,8), (10,10)) = 28\n        Cells that lie at the domain boundary (i.e., where a shift would move them outside the grid extent) are detected\n        and handled:\n            - Their neighbor index is set to `-9999` to indicate an invalid or non-existent neighbor.\n            - They are classified as external boundaries.\n\n        Cells adjacent to user-defined obstacles (as indicated by `self.site_layout.id_obstacle`) are specially treated.\n        If a neighboring cell lies within an obstacle region, it's considered an invalid neighbor for flow or\n        interaction purposes.\n\n        For external boundaries, the method assigns Dirichlet or Neumann boundary conditions depending on the\n        specification in the grid metadata for that dimension.\n\n        Each grid dimension is updated with the following information for both 'left' and 'right' directions:\n            - neighbour_index: array of neighbor indices for each cell (-9999 for out-of-bounds).\n            - boundary_condition: array indicating the type of boundary condition ('internal', 'dirichlet', 'neumann').\n            - boundary_conditions: the boundary condition type for the current direction.\n\n        \"\"\"\n        index_center = np.unravel_index(range(self.total_number_cells), self.grid_size)\n        for i, dim in enumerate(self.dimensions):\n            for face in dim.faces:\n                index_center_shift = list(index_center)\n                index_center_shift[i] = index_center_shift[i] + face.shift\n                face.neighbour_index = np.ravel_multi_index(index_center_shift, self.grid_size, mode=\"clip\")\n                face.neighbour_index = face.neighbour_index.reshape((self.total_number_cells, 1))\n                external_boundaries = np.logical_or(\n                    index_center_shift[i] &lt; 0, index_center_shift[i] &gt;= dim.number_cells\n                )\n                face.neighbour_index[external_boundaries] = -9999\n                face.set_boundary_type(external_boundaries, self.site_layout)\n\n    def compute_time_bins(\n        self, sensor_object: SensorGroup, meteorology_object: Meteorology\n    ) -&gt; Tuple[pd.DatetimeIndex, dict, np.ndarray]:\n        \"\"\"Compute discretized time bins for aligning sensor observations and meteorological data.\n\n        This method constructs a uniform time grid (bins) based on the observation time range of the given sensors.\n        The time resolution is determined by `self.dt`. If `self.dt` is not specified, it will be set automatically\n        using a CFL-like condition via `self.set_delta_time_cfl()` based on the meteorology object.\n\n        Once the time bins are established:\n            - Each sensor's observation times are digitized to determine which time bin each observation belongs to.\n            - A KDTree is used to find the closest meteorological time index corresponding to each time bin, mapping the\n            wind field to the solver grid.\n\n        Args:\n            sensor_object (SensorGroup): Sensor data object\n            meteorology_object (Meteorology): Meteorology data object.\n\n        Returns:\n            time_bins (pd.DatetimeIndex): The array of uniformly spaced time bins (based on `self.dt`).\n            time_index_sensor (dict): A dictionary mapping each sensor ID to its array of time bin indices.\n            time_index_met (np.ndarray): An array mapping each time bin to the closest meteorological time index.\n\n        \"\"\"\n        if self.dt is None:\n            self.set_delta_time_cfl(meteorology_object)\n        sensor_time = sensor_object.time.reshape(\n            -1,\n        )\n        time_bins = pd.date_range(\n            start=sensor_time.min() - pd.Timedelta(self.dt, unit=\"s\"),\n            end=sensor_time.max() + pd.Timedelta(self.dt, unit=\"s\"),\n            freq=f\"{self.dt}s\",\n            inclusive=\"both\",\n        )\n        time_index_sensor = {}\n        for key, sensor in sensor_object.items():\n            time_index_sensor[key] = np.digitize(\n                sensor.time.reshape(\n                    -1,\n                ).astype(np.int64),\n                time_bins.astype(np.int64),\n            )\n        tree = KDTree(meteorology_object.time.reshape(-1, 1).astype(np.int64))\n        _, time_index_met = tree.query(np.array(time_bins.astype(np.int64)).reshape(-1, 1), k=1)\n        return time_bins, time_index_sensor, time_index_met\n\n    def set_delta_time_cfl(self, meteorology_object: Meteorology) -&gt; None:\n        \"\"\"Use CFL condition to set the time step.\n\n        The CFL condition is a stability criterion for numerical methods used in solving partial differential equations.\n        It ensures that the numerical solution remains stable and converges to the true solution.\n\n        The CFL condition for advection is given by:\n            dt &lt;= min(dx / |u|)\n        for all dimensions, where dx is the grid spacing and u is the velocity. This method calculates the maximum\n        velocity in each dimension and sets the time step accordingly.\n\n        The diffusion term is also considered in the CFL condition:\n            dt &lt;= (dx^2) / (2 * K)\n        for all dimensions, where K is self.diffusion_constants.\n\n        dt is set to the minimum of the advection and diffusion time steps multiplied by self.courant_number.\n\n        dt is rounded to the nearest 0.1s due to usage in pd.date_range in other parts of the code.\n\n        Args:\n            meteorology_object (Meteorology): meteorology object containing timeseries of wind data.\n\n        \"\"\"\n        if meteorology_object.wind_speed is None:\n            meteorology_object.calculate_wind_speed_from_uv()\n        u_max = np.max(meteorology_object.wind_speed)\n        dx = np.min([dim.cell_width for dim in self.dimensions])\n\n        dt_adv = self.courant_number * dx / u_max\n        dt_diff = (self.courant_number * dx**2) / (2 * np.max(self.diffusion_constants))\n        self.dt = np.round(np.minimum(dt_adv, dt_diff), decimals=1)\n\n    def interpolate_coupling_grid_to_sensor(\n        self,\n        sensor_object: SensorGroup,\n        scaled_coupling: sp.csr_array,\n        time_index_sensor: np.ndarray,\n        i_time: int,\n        coupling_sensor: dict,\n    ) -&gt; dict:\n        \"\"\"Interpolate coupling grid values to sensor locations.\n\n        Calculate the coupling for each sensor at a given time step. This function interpolates plume coupling values\n        from the coupling matrix to each sensor's location for a specific time step, and updates the output dictionary\n        with the results.\n\n        Args:\n            sensor_object (SensorGroup): object containing sensor data.\n            scaled_coupling (sp.csr_array): The sparse matrix representing coupling values between sources and grid\n                cells for the current time step.\n            time_index_sensor (np.ndarray): An array mapping each sensor to its corresponding time step index.\n            i_time (int): The index of the current time step.\n            coupling_sensor (dict): The output dictionary to be updated with coupling values for each sensor.\n\n        Returns:\n            coupling_sensor (dict): The updated output dictionary with interpolated coupling values at each sensor\n                location for the current time step.\n\n        \"\"\"\n        for key, sensor in sensor_object.items():\n            observation_index = time_index_sensor[key] == i_time\n            if np.any(observation_index):\n                sensor_location = sensor.location.to_array(dim=self.number_dimensions)\n                coupling_interp = self._build_interpolator(\n                    scaled_coupling.toarray(), locations_to_interpolate=sensor_location, method=\"nearest\"\n                )\n                if isinstance(sensor, Beam):\n                    coupling_sensor[key][observation_index, :] = np.mean(coupling_interp, axis=0)\n                else:\n                    coupling_sensor[key][observation_index, :] = coupling_interp.flatten()\n        return coupling_sensor\n\n    def _build_interpolator(\n        self, tabular_values: np.ndarray, locations_to_interpolate: np.ndarray, method: str = \"linear\"\n    ) -&gt; np.ndarray:\n        \"\"\"Build an interpolator for given tabular values and interpolate at specified locations.\n\n        Interpolates values at specified locations using interpolation with the method of choosing within the grid,\n        and nearest-neighbor extrapolation for out-of-bounds points.\n\n        Args:\n            tabular_values (np.ndarray): Array of data values defined on the grid.\n            locations_to_interpolate (np.ndarray): Points at which to evaluate the interpolator, shape (M, D), where D\n                is the number of dimensions.\n            method (str): Interpolation method to use. Options are 'linear', 'nearest', etc.\n\n        Returns:\n            combined_result (np.ndarray): Interpolated values at the specified locations.\n\n        \"\"\"\n        shape = list(self.grid_size) + [-1]\n        reshaped_values = tabular_values.reshape(*shape)\n        method_interp = RegularGridInterpolator(\n            self.grid_centers,\n            reshaped_values,\n            method=method,\n            bounds_error=False,\n            fill_value=np.nan,\n        )\n        nearest_interp = RegularGridInterpolator(\n            self.grid_centers,\n            reshaped_values,\n            method=\"nearest\",\n            bounds_error=False,\n            fill_value=None,\n        )\n        method_result = method_interp(locations_to_interpolate)\n        nearest_result = nearest_interp(locations_to_interpolate)\n        combined_result = np.where(np.isnan(method_result), nearest_result, method_result)\n        return combined_result\n\n    def _prepare_sensor(self, sensor_object: SensorGroup) -&gt; SensorGroup:\n        \"\"\"Add beam knots to the sensor object for Beam sensors and convert all sensor locations to ENU coordinates.\n\n        Args:\n            sensor_object (SensorGroup): SensorGroup object containing sensor observations.\n\n        Returns:\n            sensor_object_beam_knots_added (SensorGroup): A new SensorGroup object with beam knots added for Beam\n                sensors.\n\n        \"\"\"\n        sensor_object_beam_knots_added = deepcopy(sensor_object)\n        for _, sensor in sensor_object_beam_knots_added.items():\n            sensor.location = sensor.location.to_enu(\n                ref_latitude=self.grid_coordinates.ref_latitude,\n                ref_longitude=self.grid_coordinates.ref_longitude,\n                ref_altitude=self.grid_coordinates.ref_altitude,\n            )\n            if isinstance(sensor, Beam):\n                sensor_array = sensor.make_beam_knots(\n                    ref_latitude=self.grid_coordinates.ref_latitude,\n                    ref_longitude=self.grid_coordinates.ref_longitude,\n                    ref_altitude=self.grid_coordinates.ref_altitude,\n                )\n                sensor.location.from_array(sensor_array)\n\n        return sensor_object_beam_knots_added\n\n    def _calculate_number_burn_steps(self, meteorology_object: Meteorology) -&gt; int:\n        \"\"\"Compute the number of burn-in steps for plume stabilization.\n\n        Computes the approximate amount of time required for a gas parcel to traverse the entire solver domain, based on\n        the initial wind conditions. Then, based on the model time step (self.dt), computes the approximate number of\n        time steps required for the plume to stabilize before the main analysis begins.\n\n        If burn_in_steady_state is False, the function returns 0.\n\n        burn steps are calculated as:\n        n_burn_steps = ceil(2 * max_domain_size / (max_wind_speed * dt))\n        roughly the time for a plume to travel across the domain twice. Note only consider the horizontal dimensions.\n\n        Args:\n            meteorology_object (Meteorology): Object providing wind field or other meteorological data over time.\n\n        Returns:\n            n_burn_steps (int): The number of burn steps to be used in the coupling calculations.\n\n        \"\"\"\n        if self.burn_in_steady_state is False:\n            return 0\n        meteorology_object.calculate_wind_speed_from_uv()\n        n_burn_steps = int(\n            np.ceil(\n                2\n                * np.max([(dim.limits[1] - dim.limits[0]) for dim in self.dimensions[:1]])\n                / (meteorology_object.wind_speed[0] * self.dt)\n            )\n        )\n        return n_burn_steps\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialization checks and setup.</p> <p>Creates the grid and neighbourhood for the finite volume solver, and uses the site layout to mask any obstacles from the solver grid.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post-initialization checks and setup.\n\n    Creates the grid and neighbourhood for the finite volume solver, and uses the site layout to mask any obstacles\n    from the solver grid.\n\n    \"\"\"\n    if not isinstance(self.source_map.location, ENU):\n        raise ValueError(\"source_map.location must be an ENU object.\")\n    self.number_dimensions = len(self.dimensions)\n    self._setup_grid()\n    if self.site_layout is not None:\n        self.site_layout.find_index_obstacles(self.grid_coordinates)\n    self._setup_neighbourhood()\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.compute_coupling","title":"<code>compute_coupling(sensor_object, met_windfield, gas_object=None, output_stacked=False, **kwargs)</code>","text":"<p>Compute the coupling matrix for the finite volume method using a lookup table.</p> <p>If self.use_lookup_table == False, or if self.coupling_lookup_table is None, the coupling matrix is computed using the FV solver and stored in self.coupling_lookup_table. Otherwise, the coupling matrix is computed using a lookup table approach from the previously computed coupling matrix.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor object containing sensor observations.</p> required <code>met_windfield</code> <code>MeteorologyWindfield</code> <p>meteorology object containing site layout and timeseries of wind data.</p> required <code>gas_object</code> <code>Union[GasSpecies, None]</code> <p>optional input, a gas species object to correctly calculate the gas density which is used in the conversion of the units of the Gaussian plume coupling. Defaults to None.</p> <code>None</code> <code>output_stacked</code> <code>bool</code> <p>if True, the coupling is stacked across sensors into a single np.ndarray. Otherwise, the coupling is returned as a dictionary with an entry per sensor. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>additional keyword arguments. To accommodate some arguments used in GaussianPlume.compute_coupling but not required in FiniteVolume.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Union[ndarray, dict]</code> <p>List of arrays, single array or dictionary containing the plume coupling in hr/kg. If a dictionary of sensor objects is passed in and output_stacked=False, this function returns a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling outputs for each sensor. If a dictionary of sensor objects is passed in and output_stacked=True, this function returns an np.ndarray containing the stacked coupling matrices.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def compute_coupling(\n    self,\n    sensor_object: SensorGroup,\n    met_windfield: MeteorologyWindfield,\n    gas_object: Union[GasSpecies, None] = None,\n    output_stacked: bool = False,\n    **kwargs,\n) -&gt; Union[np.ndarray, dict]:\n    \"\"\"Compute the coupling matrix for the finite volume method using a lookup table.\n\n    If self.use_lookup_table == False, or if self.coupling_lookup_table is None, the coupling matrix is computed\n    using the FV solver and stored in self.coupling_lookup_table. Otherwise, the coupling matrix is computed using a\n    lookup table approach from the previously computed coupling matrix.\n\n    Args:\n        sensor_object (SensorGroup): sensor object containing sensor observations.\n        met_windfield (MeteorologyWindfield): meteorology object containing site layout and timeseries of wind data.\n        gas_object (Union[GasSpecies, None]): optional input, a gas species object to correctly calculate the\n            gas density which is used in the conversion of the units of the Gaussian plume coupling. Defaults to\n            None.\n        output_stacked (bool): if True, the coupling is stacked across sensors into a single np.ndarray. Otherwise,\n            the coupling is returned as a dictionary with an entry per sensor. Defaults to False.\n        **kwargs: additional keyword arguments. To accommodate some arguments used in\n            GaussianPlume.compute_coupling but not required in FiniteVolume.\n\n    Returns:\n        output (Union[np.ndarray, dict]): List of arrays, single array or dictionary containing the plume coupling\n            in hr/kg. If a dictionary of sensor objects is passed in and output_stacked=False, this function returns\n            a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling\n            outputs for each sensor. If a dictionary of sensor objects is passed in and output_stacked=True, this\n            function returns an np.ndarray containing the stacked coupling matrices.\n\n    \"\"\"\n    if (met_windfield.site_layout is not None) | (self.site_layout is not None):\n        if np.any(met_windfield.site_layout.id_obstacles != self.site_layout.id_obstacles):\n            raise ValueError(\"MeteorologyWindfield site layout does not match FiniteVolume site layout.\")\n    if not isinstance(self.source_map.location, ENU):\n        raise ValueError(\"source_map.location must be an ENU object.\")\n\n    if (not self.use_lookup_table) or (self.coupling_lookup_table is None):\n        coupling_sensor = self.compute_coupling_sections(sensor_object, met_windfield, gas_object)\n        if self.use_lookup_table:\n            self.coupling_lookup_table = coupling_sensor\n\n    if self.use_lookup_table:\n        output = self.interpolate_coupling_lookup_to_source_map(sensor_object)\n    else:\n        output = coupling_sensor\n    if output_stacked:\n        output = np.concatenate(tuple(output.values()), axis=0)\n    return output\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.compute_coupling_sections","title":"<code>compute_coupling_sections(sensor_object, met_windfield, gas_object)</code>","text":"<p>Compute the coupling sections for the finite volume method.</p> <p>Sections are defined by the source_on attribute of the sensor object. If source_on is None (not specified) or all ones, then it is treated as a single section of data and directly moves on to computing the coupling matrix.</p> <p>If there are multiple sections, then the coupling matrix is computed for each section separately and combined into a single coupling matrix. This avoids computational effort computing the forward model through time steps that are not required and can speed up the computational time substantially in this case. Sections are defined by the source_on attribute of the sensor object which indicates which time steps the source is on where 0 indicates the source is off and integers starting from 1 indicate different source on sections.</p> <p>To avoid additional computational effort when a source is not emitting we do not compute the forward model when the source_on attribute of the sensor object is set to 0. When a source starts emitting we can either assume it was already emitting and calculate an equilibrium state by setting the burn_in_steady_state attribute to True. Or we assume it starts right then and set this burn_in_steady_state attribute to False. When a source stops emitting there is still some gas present in the area of interest and it will take some time for this gas to disperse out of the area. However we assume the solution will not improve enough during this time to warrant the additional computational effort to compute the forward model for this period. Which is why we stop computing the forward model again when the source_on attribute switches from 1 to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data object.</p> required <code>meteorology_object</code> <code>MeteorologyWindfield</code> <p>wind field data object.</p> required <code>gas_object</code> <code>GasSpecies</code> <p>gas species object.</p> required <p>Returns:</p> Name Type Description <code>coupling_sensor</code> <code>dict</code> <p>coupling for each sensor, keys corresponding to each sensor: e.g. coupling_sensor['sensor_1'] is the coupling matrix for sensor 1.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def compute_coupling_sections(\n    self, sensor_object: SensorGroup, met_windfield: MeteorologyWindfield, gas_object: GasSpecies\n) -&gt; dict:\n    \"\"\"Compute the coupling sections for the finite volume method.\n\n    Sections are defined by the source_on attribute of the sensor object. If source_on is None (not specified) or\n    all ones, then it is treated as a single section of data and directly moves on to computing the coupling matrix.\n\n    If there are multiple sections, then the coupling matrix is computed for each section separately and combined\n    into a single coupling matrix. This avoids computational effort computing the forward model through time steps\n    that are not required and can speed up the computational time substantially in this case. Sections are\n    defined by the source_on attribute of the sensor object which indicates which time steps the source is on where\n    0 indicates the source is off and integers starting from 1 indicate different source on sections.\n\n    To avoid additional computational effort when a source is not emitting we do not compute the forward model when\n    the source_on attribute of the sensor object is set to 0. When a source starts emitting we can either assume it\n    was already emitting and calculate an equilibrium state by setting the burn_in_steady_state attribute to True.\n    Or we assume it starts right then and set this burn_in_steady_state attribute to False. When a source stops\n    emitting there is still some gas present in the area of interest and it will take some time for this gas to\n    disperse out of the area. However we assume the solution will not improve enough during this time to warrant\n    the additional computational effort to compute the forward model for this period. Which is why we stop computing\n    the forward model again when the source_on attribute switches from 1 to 0.\n\n    Args:\n        sensor_object (SensorGroup): sensor data object.\n        meteorology_object (MeteorologyWindfield): wind field data object.\n        gas_object (GasSpecies): gas species object.\n\n    Returns:\n        coupling_sensor (dict): coupling for each sensor, keys corresponding to each sensor: e.g.\n            coupling_sensor['sensor_1'] is the coupling matrix for sensor 1.\n\n    \"\"\"\n    if sensor_object.source_on is None or np.all(sensor_object.source_on == 1):\n        return self.finite_volume_time_step_solver(sensor_object, met_windfield, gas_object)\n\n    number_of_sections = max(sensor_object.source_on)\n    coupling_sensor = {}\n    for key, sensor in sensor_object.items():\n        coupling_sensor[key] = np.full((sensor.time.shape[0], self.source_grid_link.shape[1]), fill_value=0.0)\n    for section in range(1, number_of_sections + 1):\n        subset_sensor_object = sensor_object.subset_sensor(section_index=section)\n        coupling_sensor_section = self.finite_volume_time_step_solver(\n            subset_sensor_object, met_windfield, gas_object\n        )\n        for key, sensor in sensor_object.items():\n            section_index = (sensor.source_on == section).flatten()\n            coupling_sensor[key][section_index, :] = coupling_sensor_section[key]\n    return coupling_sensor\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.finite_volume_time_step_solver","title":"<code>finite_volume_time_step_solver(sensor_object, met_windfield, gas_object)</code>","text":"<p>Compute the finite volume coupling matrix, by time-stepping the solver.</p> <p>This function calculates the coupling between emission sources and sensor measurements based on a spatial wind field derived from meteorological data. The resulting coupling matrices model the transport of gas through a discretized domain. The coupling between emissions in all solver grid cells and concentrations in the same set of grid cells is calculated by time-stepping a finite volume solver for the advection-diffusion equation. In time bins where sensor observations occur, the coupling between any source locations in the source map and the locations where sensor observations were obtained are extracted and stored in the rows of the coupling matrix.</p> <p>If dt is not specified, it will be set automatically using a CFL-like condition via self.set_delta_time_cfl(). If burn_in_steady_state is True, the model runs a burn-in period to reach steady state before computing any coupling values. The wind field during the burn-in period is assumed to be constant and the same as the wind field at the first time-step.</p> <p>If the coupling matrix is unstable (norm &gt; 1e3), an error is raised suggesting to check the CFL number and dt. This condition is checked every 10% of the time steps.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data object.</p> required <code>meteorology_object</code> <code>MeteorologyWindfield</code> <p>wind field data object.</p> required <code>gas_object</code> <code>GasSpecies</code> <p>gas species object.</p> required <p>Returns:</p> Name Type Description <code>coupling_sensor</code> <code>dict</code> <p>coupling matrix for each sensor and sources defined by source_grid_link units hr/kg. coupling_sensor keys corresponding to each source, e.g. coupling_sensor['sensor_1'] = coupling matrix for sensor 1 with shape=(number of observations (sensor_1), number of sources).</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def finite_volume_time_step_solver(\n    self,\n    sensor_object: SensorGroup,\n    met_windfield: MeteorologyWindfield,\n    gas_object: GasSpecies,\n) -&gt; dict:\n    \"\"\"Compute the finite volume coupling matrix, by time-stepping the solver.\n\n    This function calculates the coupling between emission sources and sensor measurements based on a spatial wind\n    field derived from meteorological data. The resulting coupling matrices model the transport of gas through a\n    discretized domain. The coupling between emissions in all solver grid cells and concentrations in the same set\n    of grid cells is calculated by time-stepping a finite volume solver for the advection-diffusion equation. In\n    time bins where sensor observations occur, the coupling between any source locations in the source map and the\n    locations where sensor observations were obtained are extracted and stored in the rows of the coupling matrix.\n\n    If dt is not specified, it will be set automatically using a CFL-like condition via self.set_delta_time_cfl().\n    If burn_in_steady_state is True, the model runs a burn-in period to reach steady state before computing any\n    coupling values. The wind field during the burn-in period is assumed to be constant and the same as the wind\n    field at the first time-step.\n\n    If the coupling matrix is unstable (norm &gt; 1e3), an error is raised suggesting to check the CFL number and dt.\n    This condition is checked every 10% of the time steps.\n\n    Args:\n        sensor_object (SensorGroup): sensor data object.\n        meteorology_object (MeteorologyWindfield): wind field data object.\n        gas_object (GasSpecies): gas species object.\n\n    Returns:\n        coupling_sensor (dict): coupling matrix for each sensor and sources defined by source_grid_link units hr/kg.\n            coupling_sensor keys corresponding to each source, e.g. coupling_sensor['sensor_1'] =\n            coupling matrix for sensor 1 with shape=(number of observations (sensor_1), number of sources).\n\n    \"\"\"\n    coupling_sensor = {}\n    for key, sensor in sensor_object.items():\n        coupling_sensor[key] = np.full((sensor.time.shape[0], self.source_grid_link.shape[1]), fill_value=0.0)\n    coupling_grid = None\n    time_bins, time_index_sensor, time_index_met = self.compute_time_bins(\n        sensor_object=sensor_object, meteorology_object=met_windfield.static_wind_field\n    )\n    sensor_object = self._prepare_sensor(sensor_object)\n    n_burn_steps = self._calculate_number_burn_steps(met_windfield.static_wind_field)\n    gas_density = self.calculate_gas_density(\n        met_windfield.static_wind_field, sensor_object, gas_object, run_interpolation=False\n    )\n    met_windfield.calculate_spatial_wind_field(time_index=0, grid_coordinates=self.grid_coordinates)\n\n    for i_time in tqdm(range(-n_burn_steps, time_bins.size), desc=\"Computing coupling matrix\"):\n        if i_time &gt; 0 and (time_index_met[i_time] != time_index_met[i_time - 1]):\n            met_windfield.calculate_spatial_wind_field(\n                time_index=time_index_met[i_time], grid_coordinates=self.grid_coordinates\n            )\n        if gas_density.size &gt; 1:\n            gas_density_i = gas_density[time_index_met[i_time]]\n        else:\n            gas_density_i = gas_density\n        coupling_grid = self.propagate_solver_single_time_step(met_windfield, coupling_matrix=coupling_grid)\n        scaled_coupling = coupling_grid * (1e6 / (gas_density_i.item() * 3600))\n        coupling_sensor = self.interpolate_coupling_grid_to_sensor(\n            sensor_object=sensor_object,\n            scaled_coupling=scaled_coupling,\n            time_index_sensor=time_index_sensor,\n            i_time=i_time,\n            coupling_sensor=coupling_sensor,\n        )\n        if i_time % np.floor(0.1 * (time_bins.size + n_burn_steps)) == 0:\n            coupling_grid_sourcemap_norm = sp.linalg.norm(coupling_grid)\n            if coupling_grid_sourcemap_norm &gt; 1e3:\n                raise ValueError(\n                    f\"The coupling matrix is unstable, with matrix norm: {coupling_grid_sourcemap_norm:.3g}, \"\n                    f\"check the courant_number={self.courant_number:.3f} and calculated dt=\"\n                    f\"{self.dt:.3f} s\"\n                )\n\n    return coupling_sensor\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.interpolate_coupling_lookup_to_source_map","title":"<code>interpolate_coupling_lookup_to_source_map(sensor_object)</code>","text":"<p>Compute the coupling matrix by interpolation from a lookup table.</p> <p>A coupling matrix from all solver grid centres to all observations is pre-computed and stored on the class. Coupling columns for new source locations can then be computed by interpolation from these pre-computed values.</p> <p>The coupling matrix used for lookup is taken from self.coupling_lookup_table which is a sparse matrix computed in self.finite_volume_time_step_solver().</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data object.</p> required <p>Returns:</p> Name Type Description <code>interpolated_coupling</code> <code>dict</code> <p>interpolated coupling matrix for each sensor and sources (units hr/kg).</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def interpolate_coupling_lookup_to_source_map(self, sensor_object: SensorGroup) -&gt; dict:\n    \"\"\"Compute the coupling matrix by interpolation from a lookup table.\n\n    A coupling matrix from all solver grid centres to all observations is pre-computed and stored on the class.\n    Coupling columns for new source locations can then be computed by interpolation from these pre-computed values.\n\n    The coupling matrix used for lookup is taken from self.coupling_lookup_table which is a sparse matrix computed\n    in self.finite_volume_time_step_solver().\n\n    Args:\n        sensor_object (SensorGroup): sensor data object.\n\n    Returns:\n        interpolated_coupling (dict): interpolated coupling matrix for each sensor and sources (units hr/kg).\n\n    \"\"\"\n    interpolated_coupling = {}\n    source_location = self.source_map.location.to_array(dim=self.number_dimensions)\n    for key, sensor in sensor_object.items():\n        interpolated_coupling[key] = np.full((sensor.time.shape[0], source_location.shape[0]), fill_value=0.0)\n        lookup_table_values = self.coupling_lookup_table[key].T\n        interpolated_coupling[key] = self._build_interpolator(\n            lookup_table_values, locations_to_interpolate=source_location\n        ).T\n    return interpolated_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.propagate_solver_single_time_step","title":"<code>propagate_solver_single_time_step(met_windfield, coupling_matrix=None)</code>","text":"<p>Time-step the finite volume solver.</p> <p>Time-step the finite volume solver to map the coupling matrix at time t to the coupling matrix at time (t + dt).</p> <p>For each time step, the forward matrix is computed based on the current wind field. The coupling matrix is then evolved by a single time-step using either an implicit or explicit solver approach, depending on the value of self.implicit_solver.</p> <p>coupling_matrix will be a sparse csc_array with shape=(total_number_cells, number of sources)</p> <p>If minimum_contribution is set, all elements in the coupling matrix smaller than this number will be set to 0. This can speed up computation.</p> <p>Parameters:</p> Name Type Description Default <code>met_windfield</code> <code>MeteorologyWindfield</code> <p>meteorology object containing wind field information.</p> required <code>coupling_matrix</code> <code>Union[(sparse.csc_array, None]</code> <p>shape=(self.total_number_cells, number of sources). coupling matrix matrix on the finite volume grid if None will get preallocated for future time steps</p> <code>None</code> <p>Returns:</p> Name Type Description <code>coupling_matrix</code> <code>csc_array</code> <p>shape=(self.total_number_cells, number of sources). Coupling matrix on the finite volume grid. Represents the contribution of each cell to the source term in the transport equation.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def propagate_solver_single_time_step(\n    self, met_windfield: MeteorologyWindfield, coupling_matrix: Union[sp.csc_array, None] = None\n) -&gt; sp.csc_array:\n    \"\"\"Time-step the finite volume solver.\n\n    Time-step the finite volume solver to map the coupling matrix at time t to the coupling matrix at time (t +\n    dt).\n\n    For each time step, the forward matrix is computed based on the current wind field. The coupling matrix is then\n    evolved by a single time-step using either an implicit or explicit solver approach, depending on the value of\n    self.implicit_solver.\n\n    coupling_matrix will be a sparse csc_array with shape=(total_number_cells, number of sources)\n\n    If minimum_contribution is set, all elements in the coupling matrix smaller than this number will be set to 0.\n    This can speed up computation.\n\n    Args:\n        met_windfield (MeteorologyWindfield): meteorology object containing wind field information.\n        coupling_matrix (Union[(sparse.csc_array, None]): shape=(self.total_number_cells, number of sources).\n            coupling matrix matrix on the finite volume grid if None will get preallocated for future time steps\n\n\n    Returns:\n        coupling_matrix (sparse.csc_array): shape=(self.total_number_cells, number of sources). Coupling\n            matrix on the finite volume grid. Represents the contribution of each cell to the source term in the\n            transport equation.\n\n    \"\"\"\n    self.compute_forward_matrix(met_windfield)\n    if coupling_matrix is None:\n        coupling_matrix = sp.csc_array(self.source_grid_link.shape, dtype=self.forward_matrix.dtype)\n    scale_factor = self.dt / self.cell_volume\n    if self.implicit_solver:\n        rhs = (1.0 / scale_factor) * coupling_matrix + self.source_grid_link\n        coupling_matrix = -spsolve(self.forward_matrix.tocsc(), rhs).reshape(self.source_grid_link.shape)\n        if not sp.issparse(coupling_matrix):\n            coupling_matrix = sp.csc_array(coupling_matrix)\n    else:\n        coupling_matrix = scale_factor * (self.forward_matrix @ coupling_matrix + self.source_grid_link)\n    if self.minimum_contribution &gt; 0:\n        coupling_matrix.data[abs(coupling_matrix.data) &lt;= self.minimum_contribution] = 0\n        coupling_matrix.eliminate_zeros()\n    if self.site_layout is not None:\n        coupling_matrix[self.site_layout.id_obstacles_index, :] = 0\n    return coupling_matrix\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.compute_forward_matrix","title":"<code>compute_forward_matrix(met_windfield)</code>","text":"<p>Construct the forward solver matrix. This can be used to step the solution forward in time.</p> <p>The matrix forward_matrix is constructed using the advection and diffusion terms computed for each face in the grid.</p> The overall matrix equation for the FV solver is <p>(V / dt) * [c^(n+1) - c^(n)] + F @ c^(n) - G @ c^(n) = s</p> <p>where F is the matrix of advection term coefficients, G is the matrix of diffusion term coefficients, and s is the source term.</p> Rearranging gives <p>c^(n+1) = R @ c^(n) + (dt / V) * s</p> <p>where R = I - (dt / V) * (F - G).</p> <p>The diagonals of the matrix are constructed using self._construct_diagonals_advection_diffusion() and combined using self._combine_advection_diffusion_terms().</p> <p>On first run, the matrix is constructed using self._construct_diagonal_matrix(). On subsequent runs, the matrix is updated using self._update_diagonal_matrix() which saves computational time by updating the sparse matrix in place.</p> <p>Parameters:</p> Name Type Description Default <code>met_windfield</code> <code>MeteorologyWindfield</code> <p>meteorology object containing wind field information.</p> required Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def compute_forward_matrix(self, met_windfield: MeteorologyWindfield) -&gt; None:\n    \"\"\"Construct the forward solver matrix. This can be used to step the solution forward in time.\n\n    The matrix forward_matrix is constructed using the advection and diffusion terms computed for each face in the\n    grid.\n\n    The overall matrix equation for the FV solver is:\n        (V / dt) * [c^(n+1) - c^(n)] + F @ c^(n) - G @ c^(n) = s\n    where F is the matrix of advection term coefficients, G is the matrix of diffusion term coefficients, and s is\n    the source term.\n\n    Rearranging gives:\n        c^(n+1) = R @ c^(n) + (dt / V) * s\n    where R = I - (dt / V) * (F - G).\n\n    The diagonals of the matrix are constructed using self._construct_diagonals_advection_diffusion() and combined\n    using self._combine_advection_diffusion_terms().\n\n    On first run, the matrix is constructed using self._construct_diagonal_matrix(). On subsequent runs, the matrix\n    is updated using self._update_diagonal_matrix() which saves computational time by updating the sparse matrix in\n    place.\n\n    Args:\n        met_windfield (MeteorologyWindfield): meteorology object containing wind field information.\n\n    \"\"\"\n    self._compute_advection_diffusion_terms_by_face(met_windfield)\n    self._construct_diagonals_advection_diffusion()\n    self._combine_advection_diffusion_terms()\n    if self.forward_matrix is None:\n        self._construct_diagonal_matrix()\n    else:\n        self._update_diagonal_matrix()\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.compute_time_bins","title":"<code>compute_time_bins(sensor_object, meteorology_object)</code>","text":"<p>Compute discretized time bins for aligning sensor observations and meteorological data.</p> <p>This method constructs a uniform time grid (bins) based on the observation time range of the given sensors. The time resolution is determined by <code>self.dt</code>. If <code>self.dt</code> is not specified, it will be set automatically using a CFL-like condition via <code>self.set_delta_time_cfl()</code> based on the meteorology object.</p> Once the time bins are established <ul> <li>Each sensor's observation times are digitized to determine which time bin each observation belongs to.</li> <li>A KDTree is used to find the closest meteorological time index corresponding to each time bin, mapping the wind field to the solver grid.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>Sensor data object</p> required <code>meteorology_object</code> <code>Meteorology</code> <p>Meteorology data object.</p> required <p>Returns:</p> Name Type Description <code>time_bins</code> <code>DatetimeIndex</code> <p>The array of uniformly spaced time bins (based on <code>self.dt</code>).</p> <code>time_index_sensor</code> <code>dict</code> <p>A dictionary mapping each sensor ID to its array of time bin indices.</p> <code>time_index_met</code> <code>ndarray</code> <p>An array mapping each time bin to the closest meteorological time index.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def compute_time_bins(\n    self, sensor_object: SensorGroup, meteorology_object: Meteorology\n) -&gt; Tuple[pd.DatetimeIndex, dict, np.ndarray]:\n    \"\"\"Compute discretized time bins for aligning sensor observations and meteorological data.\n\n    This method constructs a uniform time grid (bins) based on the observation time range of the given sensors.\n    The time resolution is determined by `self.dt`. If `self.dt` is not specified, it will be set automatically\n    using a CFL-like condition via `self.set_delta_time_cfl()` based on the meteorology object.\n\n    Once the time bins are established:\n        - Each sensor's observation times are digitized to determine which time bin each observation belongs to.\n        - A KDTree is used to find the closest meteorological time index corresponding to each time bin, mapping the\n        wind field to the solver grid.\n\n    Args:\n        sensor_object (SensorGroup): Sensor data object\n        meteorology_object (Meteorology): Meteorology data object.\n\n    Returns:\n        time_bins (pd.DatetimeIndex): The array of uniformly spaced time bins (based on `self.dt`).\n        time_index_sensor (dict): A dictionary mapping each sensor ID to its array of time bin indices.\n        time_index_met (np.ndarray): An array mapping each time bin to the closest meteorological time index.\n\n    \"\"\"\n    if self.dt is None:\n        self.set_delta_time_cfl(meteorology_object)\n    sensor_time = sensor_object.time.reshape(\n        -1,\n    )\n    time_bins = pd.date_range(\n        start=sensor_time.min() - pd.Timedelta(self.dt, unit=\"s\"),\n        end=sensor_time.max() + pd.Timedelta(self.dt, unit=\"s\"),\n        freq=f\"{self.dt}s\",\n        inclusive=\"both\",\n    )\n    time_index_sensor = {}\n    for key, sensor in sensor_object.items():\n        time_index_sensor[key] = np.digitize(\n            sensor.time.reshape(\n                -1,\n            ).astype(np.int64),\n            time_bins.astype(np.int64),\n        )\n    tree = KDTree(meteorology_object.time.reshape(-1, 1).astype(np.int64))\n    _, time_index_met = tree.query(np.array(time_bins.astype(np.int64)).reshape(-1, 1), k=1)\n    return time_bins, time_index_sensor, time_index_met\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.set_delta_time_cfl","title":"<code>set_delta_time_cfl(meteorology_object)</code>","text":"<p>Use CFL condition to set the time step.</p> <p>The CFL condition is a stability criterion for numerical methods used in solving partial differential equations. It ensures that the numerical solution remains stable and converges to the true solution.</p> The CFL condition for advection is given by <p>dt &lt;= min(dx / |u|)</p> <p>for all dimensions, where dx is the grid spacing and u is the velocity. This method calculates the maximum velocity in each dimension and sets the time step accordingly.</p> The diffusion term is also considered in the CFL condition <p>dt &lt;= (dx^2) / (2 * K)</p> <p>for all dimensions, where K is self.diffusion_constants.</p> <p>dt is set to the minimum of the advection and diffusion time steps multiplied by self.courant_number.</p> <p>dt is rounded to the nearest 0.1s due to usage in pd.date_range in other parts of the code.</p> <p>Parameters:</p> Name Type Description Default <code>meteorology_object</code> <code>Meteorology</code> <p>meteorology object containing timeseries of wind data.</p> required Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def set_delta_time_cfl(self, meteorology_object: Meteorology) -&gt; None:\n    \"\"\"Use CFL condition to set the time step.\n\n    The CFL condition is a stability criterion for numerical methods used in solving partial differential equations.\n    It ensures that the numerical solution remains stable and converges to the true solution.\n\n    The CFL condition for advection is given by:\n        dt &lt;= min(dx / |u|)\n    for all dimensions, where dx is the grid spacing and u is the velocity. This method calculates the maximum\n    velocity in each dimension and sets the time step accordingly.\n\n    The diffusion term is also considered in the CFL condition:\n        dt &lt;= (dx^2) / (2 * K)\n    for all dimensions, where K is self.diffusion_constants.\n\n    dt is set to the minimum of the advection and diffusion time steps multiplied by self.courant_number.\n\n    dt is rounded to the nearest 0.1s due to usage in pd.date_range in other parts of the code.\n\n    Args:\n        meteorology_object (Meteorology): meteorology object containing timeseries of wind data.\n\n    \"\"\"\n    if meteorology_object.wind_speed is None:\n        meteorology_object.calculate_wind_speed_from_uv()\n    u_max = np.max(meteorology_object.wind_speed)\n    dx = np.min([dim.cell_width for dim in self.dimensions])\n\n    dt_adv = self.courant_number * dx / u_max\n    dt_diff = (self.courant_number * dx**2) / (2 * np.max(self.diffusion_constants))\n    self.dt = np.round(np.minimum(dt_adv, dt_diff), decimals=1)\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolume.interpolate_coupling_grid_to_sensor","title":"<code>interpolate_coupling_grid_to_sensor(sensor_object, scaled_coupling, time_index_sensor, i_time, coupling_sensor)</code>","text":"<p>Interpolate coupling grid values to sensor locations.</p> <p>Calculate the coupling for each sensor at a given time step. This function interpolates plume coupling values from the coupling matrix to each sensor's location for a specific time step, and updates the output dictionary with the results.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>object containing sensor data.</p> required <code>scaled_coupling</code> <code>csr_array</code> <p>The sparse matrix representing coupling values between sources and grid cells for the current time step.</p> required <code>time_index_sensor</code> <code>ndarray</code> <p>An array mapping each sensor to its corresponding time step index.</p> required <code>i_time</code> <code>int</code> <p>The index of the current time step.</p> required <code>coupling_sensor</code> <code>dict</code> <p>The output dictionary to be updated with coupling values for each sensor.</p> required <p>Returns:</p> Name Type Description <code>coupling_sensor</code> <code>dict</code> <p>The updated output dictionary with interpolated coupling values at each sensor location for the current time step.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def interpolate_coupling_grid_to_sensor(\n    self,\n    sensor_object: SensorGroup,\n    scaled_coupling: sp.csr_array,\n    time_index_sensor: np.ndarray,\n    i_time: int,\n    coupling_sensor: dict,\n) -&gt; dict:\n    \"\"\"Interpolate coupling grid values to sensor locations.\n\n    Calculate the coupling for each sensor at a given time step. This function interpolates plume coupling values\n    from the coupling matrix to each sensor's location for a specific time step, and updates the output dictionary\n    with the results.\n\n    Args:\n        sensor_object (SensorGroup): object containing sensor data.\n        scaled_coupling (sp.csr_array): The sparse matrix representing coupling values between sources and grid\n            cells for the current time step.\n        time_index_sensor (np.ndarray): An array mapping each sensor to its corresponding time step index.\n        i_time (int): The index of the current time step.\n        coupling_sensor (dict): The output dictionary to be updated with coupling values for each sensor.\n\n    Returns:\n        coupling_sensor (dict): The updated output dictionary with interpolated coupling values at each sensor\n            location for the current time step.\n\n    \"\"\"\n    for key, sensor in sensor_object.items():\n        observation_index = time_index_sensor[key] == i_time\n        if np.any(observation_index):\n            sensor_location = sensor.location.to_array(dim=self.number_dimensions)\n            coupling_interp = self._build_interpolator(\n                scaled_coupling.toarray(), locations_to_interpolate=sensor_location, method=\"nearest\"\n            )\n            if isinstance(sensor, Beam):\n                coupling_sensor[key][observation_index, :] = np.mean(coupling_interp, axis=0)\n            else:\n                coupling_sensor[key][observation_index, :] = coupling_interp.flatten()\n    return coupling_sensor\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeDimension","title":"<code>FiniteVolumeDimension</code>  <code>dataclass</code>","text":"<p>Individual grid dimension for the finite volume method.</p> <p>Assuming that each solver dimension is a regular grid, this class stores grid properties, such as cell edges, centre points and cell widths.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str</code> <p>name of this dimension (e.g., 'x', 'y', 'z').</p> <code>number_cells</code> <code>int</code> <p>number of cells in this dimension.</p> <code>limits</code> <code>list</code> <p>limits of this dimension (e.g., [0, 100]).</p> <code>external_boundary_type</code> <code>list</code> <p>type of boundary condition for the faces in this dimension e.g., external_boundary_type=['dirichlet', 'neumann']. If only 1 type is specified, it is used for both faces of this dimension.</p> <code>cell_edges</code> <code>ndarray</code> <p>shape=(self.number_cells + 1,) edge locations for the cells in this dimension.</p> <code>cell_centers</code> <code>ndarray</code> <p>shape=(self.number_cells,) central locations of the cells in this dimension.</p> <code>cell_width</code> <code>float</code> <p>width of the cells in this dimension.</p> <code>faces</code> <code>list(FiniteVolumeFaceLeft, FiniteVolumeFaceRight</code> <p>list of objects corresponding to the left and right (-ve and +ve) faces of this dimension.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>@dataclass\nclass FiniteVolumeDimension:\n    \"\"\"Individual grid dimension for the finite volume method.\n\n    Assuming that each solver dimension is a regular grid, this class stores grid properties, such as cell edges,\n    centre points and cell widths.\n\n    Attributes:\n        label (str): name of this dimension (e.g., 'x', 'y', 'z').\n        number_cells (int): number of cells in this dimension.\n        limits (list): limits of this dimension (e.g., [0, 100]).\n        external_boundary_type (list): type of boundary condition for the faces in this dimension\n            e.g., external_boundary_type=['dirichlet', 'neumann'].\n            If only 1 type is specified, it is used for both faces of this dimension.\n\n        cell_edges (np.ndarray): shape=(self.number_cells + 1,) edge locations for the cells in this dimension.\n        cell_centers (np.ndarray): shape=(self.number_cells,) central locations of the cells in this dimension.\n        cell_width (float): width of the cells in this dimension.\n        faces (list(FiniteVolumeFaceLeft, FiniteVolumeFaceRight)): list of objects corresponding to the left and right\n            (-ve and +ve) faces of this dimension.\n\n    \"\"\"\n\n    label: str\n    number_cells: int\n    limits: list\n    external_boundary_type: list = field(default_factory=list)\n    cell_edges: np.ndarray = field(init=False)\n    cell_centers: np.ndarray = field(init=False)\n    cell_width: float = field(init=False)\n    faces: list = field(init=False)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post-initialization processing.\n\n        Validates the external boundary types and initializes the face objects for the dimension. Also calls\n        get_dimensions to calculate and store geometric properties of the dimension.\n\n        Raises:\n            ValueError: external_boundary_type must one of ['dirichlet', 'neumann'].\n            ValueError: number_cells must be at least 2.\n\n        \"\"\"\n        if not isinstance(self.external_boundary_type, list):\n            raise ValueError(\"external_boundary_type must be a list.\")\n        if self.number_cells &lt; 2:\n            raise ValueError(\"number_cells must be at least 2\")\n        if len(self.external_boundary_type) == 1:\n            self.external_boundary_type = [self.external_boundary_type[0], self.external_boundary_type[0]]\n        self.faces = [\n            FiniteVolumeFaceLeft(self.external_boundary_type[0]),\n            FiniteVolumeFaceRight(self.external_boundary_type[1]),\n        ]\n        self.get_dimensions()\n\n    def get_dimensions(self) -&gt; None:\n        \"\"\"Setup the face properties for the finite volume method.\n\n        This function calculates and stores the grid cell edges, cell centres and cell widths, and assigns the cell\n        width values to the cell faces.\n\n        \"\"\"\n        self.cell_edges = np.linspace(self.limits[0], self.limits[1], self.number_cells + 1)\n        self.cell_centers = 0.5 * (self.cell_edges[:-1] + self.cell_edges[1:])\n        self.cell_width = self.cell_edges[1] - self.cell_edges[0]\n        for face in self.faces:\n            face.cell_width = self.cell_width\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeDimension.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialization processing.</p> <p>Validates the external boundary types and initializes the face objects for the dimension. Also calls get_dimensions to calculate and store geometric properties of the dimension.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>external_boundary_type must one of ['dirichlet', 'neumann'].</p> <code>ValueError</code> <p>number_cells must be at least 2.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post-initialization processing.\n\n    Validates the external boundary types and initializes the face objects for the dimension. Also calls\n    get_dimensions to calculate and store geometric properties of the dimension.\n\n    Raises:\n        ValueError: external_boundary_type must one of ['dirichlet', 'neumann'].\n        ValueError: number_cells must be at least 2.\n\n    \"\"\"\n    if not isinstance(self.external_boundary_type, list):\n        raise ValueError(\"external_boundary_type must be a list.\")\n    if self.number_cells &lt; 2:\n        raise ValueError(\"number_cells must be at least 2\")\n    if len(self.external_boundary_type) == 1:\n        self.external_boundary_type = [self.external_boundary_type[0], self.external_boundary_type[0]]\n    self.faces = [\n        FiniteVolumeFaceLeft(self.external_boundary_type[0]),\n        FiniteVolumeFaceRight(self.external_boundary_type[1]),\n    ]\n    self.get_dimensions()\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeDimension.get_dimensions","title":"<code>get_dimensions()</code>","text":"<p>Setup the face properties for the finite volume method.</p> <p>This function calculates and stores the grid cell edges, cell centres and cell widths, and assigns the cell width values to the cell faces.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def get_dimensions(self) -&gt; None:\n    \"\"\"Setup the face properties for the finite volume method.\n\n    This function calculates and stores the grid cell edges, cell centres and cell widths, and assigns the cell\n    width values to the cell faces.\n\n    \"\"\"\n    self.cell_edges = np.linspace(self.limits[0], self.limits[1], self.number_cells + 1)\n    self.cell_centers = 0.5 * (self.cell_edges[:-1] + self.cell_edges[1:])\n    self.cell_width = self.cell_edges[1] - self.cell_edges[0]\n    for face in self.faces:\n        face.cell_width = self.cell_width\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFace","title":"<code>FiniteVolumeFace</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Face type for a grid cell in the finite volume method.</p> <p>Attributes:</p> Name Type Description <code>external_boundary_type</code> <code>str</code> <p>The type of boundary condition for the face. either 'dirichlet' or 'neumann'.</p> <code>cell_face_area</code> <code>float</code> <p>The area of the face.</p> <code>cell_volume</code> <code>float</code> <p>The volume of the face.</p> <code>cell_width</code> <code>float</code> <p>The width of the cell in the direction normal to the face.</p> <code>boundary_type</code> <code>ndarray</code> <p>shape=(total_number_cells, 1). The type of boundary condition for the face. Each entry is a string, either 'internal', 'dirichlet' or 'neumann'.</p> <code>neighbour_index</code> <code>ndarray</code> <p>shape=(total_number_cells, 1). The index of the neighboring cell across the face.</p> <code>adv_diff_terms</code> <code>dict</code> <p>The advection and diffusion terms for the face. Dictionary has two entries: \"advection\" and \"diffusion\", each containing a SolverDiagonals object.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>@dataclass\nclass FiniteVolumeFace(ABC):\n    \"\"\"Face type for a grid cell in the finite volume method.\n\n    Attributes:\n        external_boundary_type (str): The type of boundary condition for the face. either 'dirichlet' or 'neumann'.\n\n        cell_face_area (float): The area of the face.\n        cell_volume (float): The volume of the face.\n        cell_width (float): The width of the cell in the direction normal to the face.\n        boundary_type (np.ndarray): shape=(total_number_cells, 1). The type of boundary condition for the face. Each\n            entry is a string, either 'internal', 'dirichlet' or 'neumann'.\n        neighbour_index (np.ndarray): shape=(total_number_cells, 1). The index of the neighboring cell across the face.\n        adv_diff_terms (dict): The advection and diffusion terms for the face. Dictionary has two entries: \"advection\"\n            and \"diffusion\", each containing a SolverDiagonals object.\n\n    \"\"\"\n\n    external_boundary_type: str\n    cell_face_area: float = field(init=False)\n    cell_volume: float = field(init=False)\n    cell_width: float = field(init=False)\n    boundary_type: np.ndarray = field(init=False)\n    neighbour_index: np.ndarray = field(init=False)\n    adv_diff_terms: dict = field(init=False)\n\n    @property\n    @abstractmethod\n    def normal(self):\n        \"\"\"Abstract property to be defined in subclasses.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        if self.external_boundary_type not in [\"dirichlet\", \"neumann\"]:\n            raise ValueError(f\"Invalid external boundary type: {self.external_boundary_type}. \")\n        self.adv_diff_terms = {\"advection\": SolverDiagonals(), \"diffusion\": SolverDiagonals()}\n\n    def set_boundary_type(self, external_boundaries: np.ndarray, site_layout: Union[SiteLayout, None] = None) -&gt; None:\n        \"\"\"Set the boundary condition for the face based on the external boundary type.\n\n        External boundaries are set to 'dirichlet' or 'neumann' based on the specified external_boundary_type. Internal\n        boundaries are set to 'internal'.\n\n        The function also handles the case where the face is affected by an obstacle. Obstacle boundaries are set to\n        'neumann'.\n\n        Args:\n            external_boundaries (np.ndarray): shape=(total_number_cells, 1). Boolean array indicating which faces are\n                external boundaries.\n            site_layout (Union[SiteLayout, None]): SiteLayout object containing obstacle information. Defaults to None.\n\n        \"\"\"\n        self.boundary_type = np.full(self.neighbour_index.shape, \"internal\", dtype=\"&lt;U10\")\n        self.boundary_type[external_boundaries] = self.external_boundary_type\n        if site_layout is not None:\n            faces_affected_obstacle = np.isin(self.neighbour_index, np.nonzero(site_layout.id_obstacles)[0])\n            self.boundary_type[np.logical_or(faces_affected_obstacle, site_layout.id_obstacles)] = \"neumann\"\n\n    def assign_advection(self, wind_vector: np.ndarray) -&gt; None:\n        \"\"\"Assigns the advection terms for the defined set of interfaces to adv_diff_terms['advection'].\n\n        Uses an upwind scheme for the discretization of the advection term:\n        https://en.wikipedia.org/wiki/Upwind_scheme#:~:text=In#20computational#20physics#2C#20the#20term,derivatives#20in#20a#20flow#20field.\n\n        Upwind scheme for a single dimension has the following form:\n            F_i = A * [u^{+} * (c_i - c_{i-1}) + u^{-} * (c_{i+1} - c_{i})]\n        where u^{+} = -min(-u, 0) and u^{-} = max(-u, 0), A is the face area, and indices corresponding to other\n        dimensions have been dropped.\n\n        Args:\n            wind_vector (np.ndarray): shape=(total_number_cells, 1). Wind speed vector in dimension of this face\n                e.g. x, y, z.\n\n        \"\"\"\n        term = self.adv_diff_terms[\"advection\"]\n        u_norm = wind_vector * self.normal\n        term.B_central = -self.cell_face_area * -np.minimum(-u_norm, 0)\n        neighbour_advection = self.cell_face_area * np.maximum(-u_norm, 0)\n        term.B_neighbour = (self.boundary_type == \"internal\") * neighbour_advection\n        term.b_dirichlet = (self.boundary_type == \"dirichlet\") * neighbour_advection\n        term.b_neumann = (self.boundary_type == \"neumann\") * neighbour_advection\n\n    def assign_diffusion(self, diffusion_constants: float) -&gt; None:\n        \"\"\"Assigns the diffusion terms for the defined set of interfaces to adv_diff_terms['diffusion'].\n\n        If diffusion is already set this function is skipped as the diffusion term is constant.\n\n        The diffusion term for a single dimension has the following form:\n            G_i = K * A * [(c_{i+1} - c_i) / delta - (c_i - c_{i-1}) / delta]\n        where K is the diffusion constant, A is the face area, delta is the cell width, and indices corresponding to\n        other dimensions have been dropped.\n\n        Args:\n            diffusion_constants (float) : diffusion coefficient in this dimension.\n\n        \"\"\"\n        term = self.adv_diff_terms[\"diffusion\"]\n        if term.B_central is None:\n            diffusion_coefficient = self.cell_face_area * diffusion_constants / self.cell_width\n            term.B_central = -diffusion_coefficient * np.ones(self.boundary_type.shape)\n            term.B_neighbour = (self.boundary_type == \"internal\") * diffusion_coefficient\n            term.b_dirichlet = (self.boundary_type == \"dirichlet\") * diffusion_coefficient\n            term.b_neumann = (self.boundary_type == \"neumann\") * diffusion_coefficient\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFace.normal","title":"<code>normal</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Abstract property to be defined in subclasses.</p>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFace.set_boundary_type","title":"<code>set_boundary_type(external_boundaries, site_layout=None)</code>","text":"<p>Set the boundary condition for the face based on the external boundary type.</p> <p>External boundaries are set to 'dirichlet' or 'neumann' based on the specified external_boundary_type. Internal boundaries are set to 'internal'.</p> <p>The function also handles the case where the face is affected by an obstacle. Obstacle boundaries are set to 'neumann'.</p> <p>Parameters:</p> Name Type Description Default <code>external_boundaries</code> <code>ndarray</code> <p>shape=(total_number_cells, 1). Boolean array indicating which faces are external boundaries.</p> required <code>site_layout</code> <code>Union[SiteLayout, None]</code> <p>SiteLayout object containing obstacle information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def set_boundary_type(self, external_boundaries: np.ndarray, site_layout: Union[SiteLayout, None] = None) -&gt; None:\n    \"\"\"Set the boundary condition for the face based on the external boundary type.\n\n    External boundaries are set to 'dirichlet' or 'neumann' based on the specified external_boundary_type. Internal\n    boundaries are set to 'internal'.\n\n    The function also handles the case where the face is affected by an obstacle. Obstacle boundaries are set to\n    'neumann'.\n\n    Args:\n        external_boundaries (np.ndarray): shape=(total_number_cells, 1). Boolean array indicating which faces are\n            external boundaries.\n        site_layout (Union[SiteLayout, None]): SiteLayout object containing obstacle information. Defaults to None.\n\n    \"\"\"\n    self.boundary_type = np.full(self.neighbour_index.shape, \"internal\", dtype=\"&lt;U10\")\n    self.boundary_type[external_boundaries] = self.external_boundary_type\n    if site_layout is not None:\n        faces_affected_obstacle = np.isin(self.neighbour_index, np.nonzero(site_layout.id_obstacles)[0])\n        self.boundary_type[np.logical_or(faces_affected_obstacle, site_layout.id_obstacles)] = \"neumann\"\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFace.assign_advection","title":"<code>assign_advection(wind_vector)</code>","text":"<p>Assigns the advection terms for the defined set of interfaces to adv_diff_terms['advection'].</p> <p>Uses an upwind scheme for the discretization of the advection term: https://en.wikipedia.org/wiki/Upwind_scheme#:~:text=In#20computational#20physics#2C#20the#20term,derivatives#20in#20a#20flow#20field.</p> Upwind scheme for a single dimension has the following form <p>F_i = A * [u^{+} * (c_i - c_{i-1}) + u^{-} * (c_{i+1} - c_{i})]</p> <p>where u^{+} = -min(-u, 0) and u^{-} = max(-u, 0), A is the face area, and indices corresponding to other dimensions have been dropped.</p> <p>Parameters:</p> Name Type Description Default <code>wind_vector</code> <code>ndarray</code> <p>shape=(total_number_cells, 1). Wind speed vector in dimension of this face e.g. x, y, z.</p> required Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def assign_advection(self, wind_vector: np.ndarray) -&gt; None:\n    \"\"\"Assigns the advection terms for the defined set of interfaces to adv_diff_terms['advection'].\n\n    Uses an upwind scheme for the discretization of the advection term:\n    https://en.wikipedia.org/wiki/Upwind_scheme#:~:text=In#20computational#20physics#2C#20the#20term,derivatives#20in#20a#20flow#20field.\n\n    Upwind scheme for a single dimension has the following form:\n        F_i = A * [u^{+} * (c_i - c_{i-1}) + u^{-} * (c_{i+1} - c_{i})]\n    where u^{+} = -min(-u, 0) and u^{-} = max(-u, 0), A is the face area, and indices corresponding to other\n    dimensions have been dropped.\n\n    Args:\n        wind_vector (np.ndarray): shape=(total_number_cells, 1). Wind speed vector in dimension of this face\n            e.g. x, y, z.\n\n    \"\"\"\n    term = self.adv_diff_terms[\"advection\"]\n    u_norm = wind_vector * self.normal\n    term.B_central = -self.cell_face_area * -np.minimum(-u_norm, 0)\n    neighbour_advection = self.cell_face_area * np.maximum(-u_norm, 0)\n    term.B_neighbour = (self.boundary_type == \"internal\") * neighbour_advection\n    term.b_dirichlet = (self.boundary_type == \"dirichlet\") * neighbour_advection\n    term.b_neumann = (self.boundary_type == \"neumann\") * neighbour_advection\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFace.assign_diffusion","title":"<code>assign_diffusion(diffusion_constants)</code>","text":"<p>Assigns the diffusion terms for the defined set of interfaces to adv_diff_terms['diffusion'].</p> <p>If diffusion is already set this function is skipped as the diffusion term is constant.</p> The diffusion term for a single dimension has the following form <p>G_i = K * A * [(c_{i+1} - c_i) / delta - (c_i - c_{i-1}) / delta]</p> <p>where K is the diffusion constant, A is the face area, delta is the cell width, and indices corresponding to other dimensions have been dropped.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion_constants (float) </code> <p>diffusion coefficient in this dimension.</p> required Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>def assign_diffusion(self, diffusion_constants: float) -&gt; None:\n    \"\"\"Assigns the diffusion terms for the defined set of interfaces to adv_diff_terms['diffusion'].\n\n    If diffusion is already set this function is skipped as the diffusion term is constant.\n\n    The diffusion term for a single dimension has the following form:\n        G_i = K * A * [(c_{i+1} - c_i) / delta - (c_i - c_{i-1}) / delta]\n    where K is the diffusion constant, A is the face area, delta is the cell width, and indices corresponding to\n    other dimensions have been dropped.\n\n    Args:\n        diffusion_constants (float) : diffusion coefficient in this dimension.\n\n    \"\"\"\n    term = self.adv_diff_terms[\"diffusion\"]\n    if term.B_central is None:\n        diffusion_coefficient = self.cell_face_area * diffusion_constants / self.cell_width\n        term.B_central = -diffusion_coefficient * np.ones(self.boundary_type.shape)\n        term.B_neighbour = (self.boundary_type == \"internal\") * diffusion_coefficient\n        term.b_dirichlet = (self.boundary_type == \"dirichlet\") * diffusion_coefficient\n        term.b_neumann = (self.boundary_type == \"neumann\") * diffusion_coefficient\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFaceLeft","title":"<code>FiniteVolumeFaceLeft</code>  <code>dataclass</code>","text":"<p>               Bases: <code>FiniteVolumeFace</code></p> <p>Set up face properties specific to a left-facing cell (i.e. outward normal is the negative unit vector).</p> <p>Attributes:</p> Name Type Description <code>direction</code> <code>str</code> <p>direction of the face, either 'left' or 'right'.</p> <code>shift</code> <code>int</code> <p>shift in the grid index to find the neighbour cell. -1 for left face.</p> <code>normal</code> <code>int</code> <p>normal vector for the face. -1 for left face.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>@dataclass\nclass FiniteVolumeFaceLeft(FiniteVolumeFace):\n    \"\"\"Set up face properties specific to a left-facing cell (i.e. outward normal is the negative unit vector).\n\n    Attributes:\n        direction (str): direction of the face, either 'left' or 'right'.\n        shift (int): shift in the grid index to find the neighbour cell. -1 for left face.\n        normal (int): normal vector for the face. -1 for left face.\n\n    \"\"\"\n\n    direction: str = \"left\"\n    shift: int = -1\n    normal: int = -1\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.FiniteVolumeFaceRight","title":"<code>FiniteVolumeFaceRight</code>  <code>dataclass</code>","text":"<p>               Bases: <code>FiniteVolumeFace</code></p> <p>Set up face properties specific to a right-facing cell (i.e. outward normal is the positive unit vector).</p> <p>Attributes:</p> Name Type Description <code>direction</code> <code>str</code> <p>direction of the face, either 'left' or 'right'.</p> <code>shift</code> <code>int</code> <p>shift in the grid index to find the neighbour cell. +1 for right face.</p> <code>normal</code> <code>int</code> <p>normal vector for the face. +1 for right face.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>@dataclass\nclass FiniteVolumeFaceRight(FiniteVolumeFace):\n    \"\"\"Set up face properties specific to a right-facing cell (i.e. outward normal is the positive unit vector).\n\n    Attributes:\n        direction (str): direction of the face, either 'left' or 'right'.\n        shift (int): shift in the grid index to find the neighbour cell. +1 for right face.\n        normal (int): normal vector for the face. +1 for right face.\n\n    \"\"\"\n\n    direction: str = \"right\"\n    shift: int = 1\n    normal: int = 1\n</code></pre>"},{"location":"pyelq/dispersion_model/finite_volume/#pyelq.dispersion_model.finite_volume.SolverDiagonals","title":"<code>SolverDiagonals</code>  <code>dataclass</code>","text":"<p>Storage for the diagonals of the solver matrix for the finite volume method on a regular grid.</p> <p>This class holds the diagonal components to construct the solver matrix. It is used for advection, diffusion and combined terms.</p> <p>Attributes:</p> Name Type Description <code>B</code> <code>Union[ndarray, None]</code> <p>shape=(total_number_cells, 1 + number_faces). Array containing all solver diagonals, i.e. containing all diagonals from self.B_central and self.B_neighbour. The first column is the central diagonal and the remaining columns are the off-diagonal terms.</p> <code>B_central</code> <code>Union[ndarray, None]</code> <p>shape=(total_number_cells, 1). Array containing the central diagonal of the solver matrix.</p> <code>B_neighbour</code> <code>Union[ndarray, None]</code> <p>shape=(total_number_cells, number_faces). Array containing the off-diagonals of the solver matrix.</p> <code>b_dirichlet</code> <code>Union[ndarray, None]</code> <p>shape=(total_number_cells, 1). Vector containing contributions from Dirichlet boundary conditions at edge cells.</p> <code>b_neumann</code> <code>Union[ndarray, None]</code> <p>shape=(total_number_cells, 1). Vector containing contributions from Neumann boundary conditions.</p> Source code in <code>src/pyelq/dispersion_model/finite_volume.py</code> <pre><code>@dataclass\nclass SolverDiagonals:\n    \"\"\"Storage for the diagonals of the solver matrix for the finite volume method on a regular grid.\n\n    This class holds the diagonal components to construct the solver matrix. It is used for advection, diffusion and\n    combined terms.\n\n    Attributes:\n        B (Union[np.ndarray, None]): shape=(total_number_cells, 1 + number_faces). Array containing all solver\n            diagonals, i.e. containing all diagonals from self.B_central and self.B_neighbour. The first column is the\n            central diagonal and the remaining columns are the off-diagonal terms.\n        B_central (Union[np.ndarray, None]): shape=(total_number_cells, 1). Array containing the central diagonal of the\n            solver matrix.\n        B_neighbour (Union[np.ndarray, None]): shape=(total_number_cells, number_faces). Array containing the\n            off-diagonals of the solver matrix.\n        b_dirichlet (Union[np.ndarray, None]): shape=(total_number_cells, 1). Vector containing contributions from\n            Dirichlet boundary conditions at edge cells.\n        b_neumann (Union[np.ndarray, None]): shape=(total_number_cells, 1). Vector containing contributions from Neumann\n            boundary conditions.\n\n    \"\"\"\n\n    B: Union[np.ndarray, None] = field(default=None, init=False)\n    B_central: Union[np.ndarray, None] = field(default=None, init=False)\n    B_neighbour: Union[np.ndarray, None] = field(default=None, init=False)\n    b_dirichlet: Union[np.ndarray, None] = field(default=None, init=False)\n    b_neumann: Union[np.ndarray, None] = field(default=None, init=False)\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/","title":"Gaussian Plume","text":""},{"location":"pyelq/dispersion_model/gaussian_plume/#gaussian-plume","title":"Gaussian Plume","text":"<p>Gaussian Plume module.</p> <p>The class for the Gaussian Plume dispersion model used in pyELQ.</p> <p>The Mathematics of Atmospheric Dispersion Modeling, John M. Stockie, DOI. 10.1137/10080991X</p>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume","title":"<code>GaussianPlume</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DispersionModel</code></p> <p>Defines the Gaussian plume dispersion model class.</p> <p>Attributes:</p> Name Type Description <code>source_half_width</code> <code>float</code> <p>Source half width (radius) to be used in the Gaussian plume model (in meters)</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>@dataclass\nclass GaussianPlume(DispersionModel):\n    \"\"\"Defines the Gaussian plume dispersion model class.\n\n    Attributes:\n        source_half_width (float): Source half width (radius) to be used in the Gaussian plume model (in meters)\n\n    \"\"\"\n\n    source_half_width: float = 1\n\n    def compute_coupling(\n        self,\n        sensor_object: Union[SensorGroup, Sensor],\n        meteorology_object: Union[MeteorologyGroup, Meteorology],\n        gas_object: GasSpecies = None,\n        output_stacked: bool = False,\n        run_interpolation: bool = True,\n    ) -&gt; Union[list, np.ndarray, dict]:\n        \"\"\"Top level function to calculate the Gaussian plume coupling.\n\n        Calculates the coupling for either a single sensor object or a dictionary of sensor objects.\n\n        When both a SensorGroup and a MeteorologyGroup have been passed in, we assume they are consistent and contain\n        exactly the same keys for each item in both groups. Also assuming interpolation has been performed and time axes\n        are consistent, so we set run_interpolation to False\n\n        When you input a SensorGroup and a single Meteorology object we convert this object into a dictionary, so we\n        don't have to duplicate the same code.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Single sensor object or SensorGroup object which is used in the\n                calculation of the plume coupling.\n            meteorology_object (Union[MeteorologyGroup, Meteorology]): Meteorology object or MeteorologyGroup object\n                which is used in the calculation of the plume coupling.\n            gas_object (GasSpecies, optional): Optional input, a gas species object to correctly calculate the\n                gas density which is used in the conversion of the units of the Gaussian plume coupling\n            output_stacked (bool, optional): if true outputs as stacked np.array across sensors if not\n                outputs as dict\n            run_interpolation (bool, optional): logical indicating whether interpolation of the meteorological data to\n                the sensor/source is required. Defaults to True.\n\n        Returns:\n            plume_coupling (Union[list, np.ndarray, dict]): List of arrays, single array or dictionary containing the\n                plume coupling in hr/kg. When a single source object is passed in as input this function returns a list\n                or an array depending on the sensor type.\n                If a dictionary of sensor objects is passed in as input and output_stacked=False  this function returns\n                a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling\n                outputs for each sensor.\n                If a dictionary of sensor objects is passed in as input and output_stacked=True  this function returns\n                a np.array containing the stacked coupling matrices.\n\n        \"\"\"\n        if isinstance(sensor_object, SensorGroup):\n            output = {}\n            if isinstance(meteorology_object, Meteorology):\n                meteorology_object = dict.fromkeys(sensor_object.keys(), meteorology_object)\n            elif isinstance(meteorology_object, MeteorologyGroup):\n                run_interpolation = False\n\n            for sensor_key in sensor_object:\n                output[sensor_key] = self.compute_coupling_single_sensor(\n                    sensor_object=sensor_object[sensor_key],\n                    meteorology=meteorology_object[sensor_key],\n                    gas_object=gas_object,\n                    run_interpolation=run_interpolation,\n                )\n            if output_stacked:\n                output = np.concatenate(tuple(output.values()), axis=0)\n\n        elif isinstance(sensor_object, Sensor):\n            if isinstance(meteorology_object, MeteorologyGroup):\n                raise TypeError(\"Please provide a single Meteorology object when using a single Sensor object\")\n\n            output = self.compute_coupling_single_sensor(\n                sensor_object=sensor_object,\n                meteorology=meteorology_object,\n                gas_object=gas_object,\n                run_interpolation=run_interpolation,\n            )\n        else:\n            raise TypeError(\"Please provide either a Sensor or SensorGroup as input argument\")\n\n        return output\n\n    def compute_coupling_single_sensor(\n        self,\n        sensor_object: Sensor,\n        meteorology: Meteorology,\n        gas_object: Union[GasSpecies, None] = None,\n        run_interpolation: bool = True,\n    ) -&gt; Union[list, np.ndarray]:\n        \"\"\"Wrapper function to compute the gaussian plume coupling for a single sensor.\n\n        Wrapper is used to identify specific cases and calculate the Gaussian plume coupling accordingly.\n\n        When the sensor object contains the source_on attribute we set all coupling values to 0 for observations for\n        which source_on is False. Making sure the source_on is column array, aligning with the 1st dimension\n        (nof_observations) of the plume coupling array.\n\n        Args:\n            sensor_object (Sensor): Single sensor object which is used in the calculation of the plume coupling\n            meteorology (Meteorology): Meteorology object which is used in the calculation of the plume coupling\n            gas_object (GasSpecies, optional): Optionally input a gas species object to correctly calculate the\n                gas density which is used in the conversion of the units of the Gaussian plume coupling\n            run_interpolation (bool): logical indicating whether interpolation of the meteorological data to\n                the sensor/source is required. Default passed from compute_coupling.\n\n        Returns:\n            plume_coupling (Union[list, np.ndarray]): List of arrays or single array containing the plume coupling\n                in 1e6*[hr/kg]. Entries of the list are per source in the case of a satellite sensor, if a single array\n                is returned the coupling for each observation (first dimension) to each source (second dimension) is\n                provided.\n\n        \"\"\"\n        if not isinstance(sensor_object, Sensor):\n            raise NotImplementedError(\"Please provide a valid sensor type\")\n\n        (\n            gas_density,\n            u_interpolated,\n            v_interpolated,\n            wind_turbulence_horizontal,\n            wind_turbulence_vertical,\n        ) = self.interpolate_all_meteorology(\n            meteorology=meteorology,\n            sensor_object=sensor_object,\n            gas_object=gas_object,\n            run_interpolation=run_interpolation,\n        )\n\n        wind_speed = np.sqrt(u_interpolated**2 + v_interpolated**2)\n        theta = np.arctan2(v_interpolated, u_interpolated)\n\n        if isinstance(sensor_object, Satellite):\n            plume_coupling = self.compute_coupling_satellite(\n                sensor_object=sensor_object,\n                wind_speed=wind_speed,\n                theta=theta,\n                wind_turbulence_horizontal=wind_turbulence_horizontal,\n                wind_turbulence_vertical=wind_turbulence_vertical,\n                gas_density=gas_density,\n            )\n\n        else:\n            plume_coupling = self.compute_coupling_ground(\n                sensor_object=sensor_object,\n                wind_speed=wind_speed,\n                theta=theta,\n                wind_turbulence_horizontal=wind_turbulence_horizontal,\n                wind_turbulence_vertical=wind_turbulence_vertical,\n                gas_density=gas_density,\n            )\n\n        if sensor_object.source_on is not None:\n            plume_coupling = plume_coupling * np.where(sensor_object.source_on != 0, 1, 0)[:, None]\n\n        return plume_coupling\n\n    def compute_coupling_array(\n        self,\n        sensor_x: np.ndarray,\n        sensor_y: np.ndarray,\n        sensor_z: np.ndarray,\n        source_z: np.ndarray,\n        wind_speed: np.ndarray,\n        theta: np.ndarray,\n        wind_turbulence_horizontal: np.ndarray,\n        wind_turbulence_vertical: np.ndarray,\n        gas_density: Union[float, np.ndarray],\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the Gaussian plume coupling.\n\n        Most low level function to calculate the Gaussian plume coupling. Assuming input shapes are consistent but no\n        checking is done on this.\n\n        Setting sigma_vert to 1e-16 when it is identically zero (distance_x == 0) so we don't get a divide by 0 error\n        all the time.\n\n        Args:\n            sensor_x (np.ndarray): sensor x location relative to source [m].\n            sensor_y (np.ndarray): sensor y location relative to source [m].\n            sensor_z (np.ndarray): sensor z location relative to ground height [m].\n            source_z (np.ndarray): source z location relative to ground height [m].\n            wind_speed (np.ndarray): wind speed at source locations in [m/s].\n            theta (np.ndarray): Mathematical wind direction at source locations [radians]:\n                calculated as np.arctan2(v_component_wind, u_component_wind).\n            wind_turbulence_horizontal (np.ndarray): Horizontal wind turbulence [deg].\n            wind_turbulence_vertical (np.ndarray): Vertical wind turbulence [deg].\n            gas_density (Union[float, np.ndarray]): Gas density to use in coupling calculation [kg/m^3].\n\n        Returns:\n            plume_coupling (np.ndarray): Gaussian plume coupling in (1e6)*[hr/kg]: gives concentrations\n                in [ppm] when multiplied by sources in [kg/hr].\n\n        \"\"\"\n        cos_theta = np.cos(theta)\n        sin_theta = np.sin(theta)\n\n        distance_x = cos_theta * sensor_x + sin_theta * sensor_y\n        if np.all(distance_x &lt; 0):\n            return np.zeros_like(distance_x)\n\n        distance_y = -sin_theta * sensor_x + cos_theta * sensor_y\n\n        sigma_hor = np.tan(wind_turbulence_horizontal * (np.pi / 180)) * np.abs(distance_x) + self.source_half_width\n        sigma_vert = np.tan(wind_turbulence_vertical * (np.pi / 180)) * np.abs(distance_x)\n\n        sigma_vert[sigma_vert == 0] = 1e-16\n\n        plume_coupling = (\n            (1 / (2 * np.pi * wind_speed * sigma_hor * sigma_vert))\n            * np.exp(-0.5 * (distance_y / sigma_hor) ** 2)\n            * (\n                np.exp(-0.5 * (((sensor_z + source_z) / sigma_vert) ** 2))\n                + np.exp(-0.5 * (((sensor_z - source_z) / sigma_vert) ** 2))\n            )\n        )\n\n        plume_coupling = np.divide(np.multiply(plume_coupling, 1e6), (gas_density * 3600))\n        plume_coupling[np.logical_or(distance_x &lt; 0, plume_coupling &lt; self.minimum_contribution)] = 0\n\n        return plume_coupling\n\n    def compute_coupling_satellite(\n        self,\n        sensor_object: Sensor,\n        wind_speed: np.ndarray,\n        theta: np.ndarray,\n        wind_turbulence_horizontal: np.ndarray,\n        wind_turbulence_vertical: np.ndarray,\n        gas_density: np.ndarray,\n    ) -&gt; list:\n        \"\"\"Compute Gaussian plume coupling for satellite sensor.\n\n        When the sensor is a Satellite object we calculate the plume coupling per source. Given the large number of\n        sources and the possibility of using the inclusion radius and inclusion indices here and validity of a local\n        ENU system over large distances we loop over each source and calculate the coupling on a per-source basis.\n\n        If source_map.inclusion_n_obs is None, we do not do any filtering on observations and we want to include all\n        observations in the plume coupling calculations.\n\n        All np.ndarray inputs should have a shape of [1 x nof_sources]\n\n        Args:\n            sensor_object (Sensor): Sensor object used in plume coupling calculation\n            wind_speed (np.ndarray): Wind speed [m/s]\n            theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n            wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n            wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n            gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n        Returns:\n            plume_coupling (list): List of Gaussian plume coupling 1e6*[hr/kg] arrays. The list has a length of\n                nof_sources, each array has the shape [nof_observations x 1] or [inclusion_n_obs x 1] when\n                inclusion_idx is used.\n\n        \"\"\"\n        plume_coupling = []\n\n        source_map_location_lla = self.source_map.location.to_lla()\n        for current_source in range(self.source_map.nof_sources):\n            if self.source_map.inclusion_n_obs is None:\n                enu_sensor_array = sensor_object.location.to_enu(\n                    ref_latitude=source_map_location_lla.latitude[current_source],\n                    ref_longitude=source_map_location_lla.longitude[current_source],\n                    ref_altitude=0,\n                ).to_array()\n\n            else:\n                if self.source_map.inclusion_n_obs[current_source] == 0:\n                    plume_coupling.append(np.array([]))\n                    continue\n\n                enu_sensor_array = _create_enu_sensor_array(\n                    inclusion_idx=self.source_map.inclusion_idx[current_source],\n                    sensor_object=sensor_object,\n                    source_map_location_lla=source_map_location_lla,\n                    current_source=current_source,\n                )\n\n            temp_coupling = self.compute_coupling_array(\n                enu_sensor_array[:, [0]],\n                enu_sensor_array[:, [1]],\n                enu_sensor_array[:, [2]],\n                source_map_location_lla.altitude[current_source],\n                wind_speed[:, current_source],\n                theta[:, current_source],\n                wind_turbulence_horizontal[:, current_source],\n                wind_turbulence_vertical[:, current_source],\n                gas_density[:, current_source],\n            )\n\n            plume_coupling.append(temp_coupling)\n\n        return plume_coupling\n\n    def compute_coupling_ground(\n        self,\n        sensor_object: Sensor,\n        wind_speed: np.ndarray,\n        theta: np.ndarray,\n        wind_turbulence_horizontal: np.ndarray,\n        wind_turbulence_vertical: np.ndarray,\n        gas_density: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute Gaussian plume coupling for a ground sensor.\n\n        If the source map is already defined as ENU the reference location is maintained but the sensor is checked\n        to make sure the same reference location is used. Otherwise, when converting to ENU object for the sensor\n        observations we use a single source and altitude 0 as the reference location. This way our ENU system is a\n        system w.r.t. ground level which is required for the current implementation of the actual coupling calculation.\n\n        When the sensor is a Beam object we calculate the plume coupling for all sources to all beam knot locations at\n        once in the same ENU coordinate system and finally averaged over the beam knots to get the final output.\n\n        In general, we calculate the coupling from all sources to all sensor observation locations. In order to achieve\n        this we input the sensor array as column and source array as row vector in calculating relative x etc.,\n        with the beam knot locations being the third dimension. When the sensor is a single point Sensor or a Drone\n        sensor we effectively have one beam knot, making the mean operation at the end effectively a reshape operation\n        which gets rid of the third dimension.\n\n        All np.ndarray inputs should have a shape of [nof_observations x 1]\n\n        Args:\n            sensor_object (Sensor): Sensor object used in plume coupling calculation\n            wind_speed (np.ndarray): Wind speed [m/s]\n            theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n            wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n            wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n            gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n        Returns:\n            plume_coupling (np.ndarray): Gaussian plume coupling 1e6*[hr/kg] array. The array has the\n                shape [nof_observations x nof_sources]\n\n        \"\"\"\n        if not isinstance(self.source_map.location, ENU):\n            source_map_lla = self.source_map.location.to_lla()\n            source_map_enu = source_map_lla.to_enu(\n                ref_latitude=source_map_lla.latitude[0], ref_longitude=source_map_lla.longitude[0], ref_altitude=0\n            )\n        else:\n            source_map_enu = self.source_map.location\n\n        enu_source_array = source_map_enu.to_array()\n\n        if isinstance(sensor_object, Beam):\n            enu_sensor_array = sensor_object.make_beam_knots(\n                ref_latitude=source_map_enu.ref_latitude,\n                ref_longitude=source_map_enu.ref_longitude,\n                ref_altitude=source_map_enu.ref_altitude,\n            )\n            relative_x = np.subtract(enu_sensor_array[:, 0][None, None, :], enu_source_array[:, 0][None, :, None])\n            relative_y = np.subtract(enu_sensor_array[:, 1][None, None, :], enu_source_array[:, 1][None, :, None])\n            z_sensor = enu_sensor_array[:, 2][None, None, :]\n        else:\n            enu_sensor_array = sensor_object.location.to_enu(\n                ref_latitude=source_map_enu.ref_latitude,\n                ref_longitude=source_map_enu.ref_longitude,\n                ref_altitude=source_map_enu.ref_altitude,\n            ).to_array()\n            relative_x = np.subtract(enu_sensor_array[:, 0][:, None, None], enu_source_array[:, 0][None, :, None])\n            relative_y = np.subtract(enu_sensor_array[:, 1][:, None, None], enu_source_array[:, 1][None, :, None])\n            z_sensor = enu_sensor_array[:, 2][:, None, None]\n\n        z_source = enu_source_array[:, 2][None, :, None]\n\n        plume_coupling = self.compute_coupling_array(\n            relative_x,\n            relative_y,\n            z_sensor,\n            z_source,\n            wind_speed[:, :, None],\n            theta[:, :, None],\n            wind_turbulence_horizontal[:, :, None],\n            wind_turbulence_vertical[:, :, None],\n            gas_density[:, :, None],\n        )\n\n        plume_coupling = plume_coupling.mean(axis=2)\n\n        return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling","title":"<code>compute_coupling(sensor_object, meteorology_object, gas_object=None, output_stacked=False, run_interpolation=True)</code>","text":"<p>Top level function to calculate the Gaussian plume coupling.</p> <p>Calculates the coupling for either a single sensor object or a dictionary of sensor objects.</p> <p>When both a SensorGroup and a MeteorologyGroup have been passed in, we assume they are consistent and contain exactly the same keys for each item in both groups. Also assuming interpolation has been performed and time axes are consistent, so we set run_interpolation to False</p> <p>When you input a SensorGroup and a single Meteorology object we convert this object into a dictionary, so we don't have to duplicate the same code.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Single sensor object or SensorGroup object which is used in the calculation of the plume coupling.</p> required <code>meteorology_object</code> <code>Union[MeteorologyGroup, Meteorology]</code> <p>Meteorology object or MeteorologyGroup object which is used in the calculation of the plume coupling.</p> required <code>gas_object</code> <code>GasSpecies</code> <p>Optional input, a gas species object to correctly calculate the gas density which is used in the conversion of the units of the Gaussian plume coupling</p> <code>None</code> <code>output_stacked</code> <code>bool</code> <p>if true outputs as stacked np.array across sensors if not outputs as dict</p> <code>False</code> <code>run_interpolation</code> <code>bool</code> <p>logical indicating whether interpolation of the meteorological data to the sensor/source is required. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>Union[list, ndarray, dict]</code> <p>List of arrays, single array or dictionary containing the plume coupling in hr/kg. When a single source object is passed in as input this function returns a list or an array depending on the sensor type. If a dictionary of sensor objects is passed in as input and output_stacked=False  this function returns a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling outputs for each sensor. If a dictionary of sensor objects is passed in as input and output_stacked=True  this function returns a np.array containing the stacked coupling matrices.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling(\n    self,\n    sensor_object: Union[SensorGroup, Sensor],\n    meteorology_object: Union[MeteorologyGroup, Meteorology],\n    gas_object: GasSpecies = None,\n    output_stacked: bool = False,\n    run_interpolation: bool = True,\n) -&gt; Union[list, np.ndarray, dict]:\n    \"\"\"Top level function to calculate the Gaussian plume coupling.\n\n    Calculates the coupling for either a single sensor object or a dictionary of sensor objects.\n\n    When both a SensorGroup and a MeteorologyGroup have been passed in, we assume they are consistent and contain\n    exactly the same keys for each item in both groups. Also assuming interpolation has been performed and time axes\n    are consistent, so we set run_interpolation to False\n\n    When you input a SensorGroup and a single Meteorology object we convert this object into a dictionary, so we\n    don't have to duplicate the same code.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Single sensor object or SensorGroup object which is used in the\n            calculation of the plume coupling.\n        meteorology_object (Union[MeteorologyGroup, Meteorology]): Meteorology object or MeteorologyGroup object\n            which is used in the calculation of the plume coupling.\n        gas_object (GasSpecies, optional): Optional input, a gas species object to correctly calculate the\n            gas density which is used in the conversion of the units of the Gaussian plume coupling\n        output_stacked (bool, optional): if true outputs as stacked np.array across sensors if not\n            outputs as dict\n        run_interpolation (bool, optional): logical indicating whether interpolation of the meteorological data to\n            the sensor/source is required. Defaults to True.\n\n    Returns:\n        plume_coupling (Union[list, np.ndarray, dict]): List of arrays, single array or dictionary containing the\n            plume coupling in hr/kg. When a single source object is passed in as input this function returns a list\n            or an array depending on the sensor type.\n            If a dictionary of sensor objects is passed in as input and output_stacked=False  this function returns\n            a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling\n            outputs for each sensor.\n            If a dictionary of sensor objects is passed in as input and output_stacked=True  this function returns\n            a np.array containing the stacked coupling matrices.\n\n    \"\"\"\n    if isinstance(sensor_object, SensorGroup):\n        output = {}\n        if isinstance(meteorology_object, Meteorology):\n            meteorology_object = dict.fromkeys(sensor_object.keys(), meteorology_object)\n        elif isinstance(meteorology_object, MeteorologyGroup):\n            run_interpolation = False\n\n        for sensor_key in sensor_object:\n            output[sensor_key] = self.compute_coupling_single_sensor(\n                sensor_object=sensor_object[sensor_key],\n                meteorology=meteorology_object[sensor_key],\n                gas_object=gas_object,\n                run_interpolation=run_interpolation,\n            )\n        if output_stacked:\n            output = np.concatenate(tuple(output.values()), axis=0)\n\n    elif isinstance(sensor_object, Sensor):\n        if isinstance(meteorology_object, MeteorologyGroup):\n            raise TypeError(\"Please provide a single Meteorology object when using a single Sensor object\")\n\n        output = self.compute_coupling_single_sensor(\n            sensor_object=sensor_object,\n            meteorology=meteorology_object,\n            gas_object=gas_object,\n            run_interpolation=run_interpolation,\n        )\n    else:\n        raise TypeError(\"Please provide either a Sensor or SensorGroup as input argument\")\n\n    return output\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_single_sensor","title":"<code>compute_coupling_single_sensor(sensor_object, meteorology, gas_object=None, run_interpolation=True)</code>","text":"<p>Wrapper function to compute the gaussian plume coupling for a single sensor.</p> <p>Wrapper is used to identify specific cases and calculate the Gaussian plume coupling accordingly.</p> <p>When the sensor object contains the source_on attribute we set all coupling values to 0 for observations for which source_on is False. Making sure the source_on is column array, aligning with the 1st dimension (nof_observations) of the plume coupling array.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Single sensor object which is used in the calculation of the plume coupling</p> required <code>meteorology</code> <code>Meteorology</code> <p>Meteorology object which is used in the calculation of the plume coupling</p> required <code>gas_object</code> <code>GasSpecies</code> <p>Optionally input a gas species object to correctly calculate the gas density which is used in the conversion of the units of the Gaussian plume coupling</p> <code>None</code> <code>run_interpolation</code> <code>bool</code> <p>logical indicating whether interpolation of the meteorological data to the sensor/source is required. Default passed from compute_coupling.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>Union[list, ndarray]</code> <p>List of arrays or single array containing the plume coupling in 1e6*[hr/kg]. Entries of the list are per source in the case of a satellite sensor, if a single array is returned the coupling for each observation (first dimension) to each source (second dimension) is provided.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_single_sensor(\n    self,\n    sensor_object: Sensor,\n    meteorology: Meteorology,\n    gas_object: Union[GasSpecies, None] = None,\n    run_interpolation: bool = True,\n) -&gt; Union[list, np.ndarray]:\n    \"\"\"Wrapper function to compute the gaussian plume coupling for a single sensor.\n\n    Wrapper is used to identify specific cases and calculate the Gaussian plume coupling accordingly.\n\n    When the sensor object contains the source_on attribute we set all coupling values to 0 for observations for\n    which source_on is False. Making sure the source_on is column array, aligning with the 1st dimension\n    (nof_observations) of the plume coupling array.\n\n    Args:\n        sensor_object (Sensor): Single sensor object which is used in the calculation of the plume coupling\n        meteorology (Meteorology): Meteorology object which is used in the calculation of the plume coupling\n        gas_object (GasSpecies, optional): Optionally input a gas species object to correctly calculate the\n            gas density which is used in the conversion of the units of the Gaussian plume coupling\n        run_interpolation (bool): logical indicating whether interpolation of the meteorological data to\n            the sensor/source is required. Default passed from compute_coupling.\n\n    Returns:\n        plume_coupling (Union[list, np.ndarray]): List of arrays or single array containing the plume coupling\n            in 1e6*[hr/kg]. Entries of the list are per source in the case of a satellite sensor, if a single array\n            is returned the coupling for each observation (first dimension) to each source (second dimension) is\n            provided.\n\n    \"\"\"\n    if not isinstance(sensor_object, Sensor):\n        raise NotImplementedError(\"Please provide a valid sensor type\")\n\n    (\n        gas_density,\n        u_interpolated,\n        v_interpolated,\n        wind_turbulence_horizontal,\n        wind_turbulence_vertical,\n    ) = self.interpolate_all_meteorology(\n        meteorology=meteorology,\n        sensor_object=sensor_object,\n        gas_object=gas_object,\n        run_interpolation=run_interpolation,\n    )\n\n    wind_speed = np.sqrt(u_interpolated**2 + v_interpolated**2)\n    theta = np.arctan2(v_interpolated, u_interpolated)\n\n    if isinstance(sensor_object, Satellite):\n        plume_coupling = self.compute_coupling_satellite(\n            sensor_object=sensor_object,\n            wind_speed=wind_speed,\n            theta=theta,\n            wind_turbulence_horizontal=wind_turbulence_horizontal,\n            wind_turbulence_vertical=wind_turbulence_vertical,\n            gas_density=gas_density,\n        )\n\n    else:\n        plume_coupling = self.compute_coupling_ground(\n            sensor_object=sensor_object,\n            wind_speed=wind_speed,\n            theta=theta,\n            wind_turbulence_horizontal=wind_turbulence_horizontal,\n            wind_turbulence_vertical=wind_turbulence_vertical,\n            gas_density=gas_density,\n        )\n\n    if sensor_object.source_on is not None:\n        plume_coupling = plume_coupling * np.where(sensor_object.source_on != 0, 1, 0)[:, None]\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_array","title":"<code>compute_coupling_array(sensor_x, sensor_y, sensor_z, source_z, wind_speed, theta, wind_turbulence_horizontal, wind_turbulence_vertical, gas_density)</code>","text":"<p>Compute the Gaussian plume coupling.</p> <p>Most low level function to calculate the Gaussian plume coupling. Assuming input shapes are consistent but no checking is done on this.</p> <p>Setting sigma_vert to 1e-16 when it is identically zero (distance_x == 0) so we don't get a divide by 0 error all the time.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_x</code> <code>ndarray</code> <p>sensor x location relative to source [m].</p> required <code>sensor_y</code> <code>ndarray</code> <p>sensor y location relative to source [m].</p> required <code>sensor_z</code> <code>ndarray</code> <p>sensor z location relative to ground height [m].</p> required <code>source_z</code> <code>ndarray</code> <p>source z location relative to ground height [m].</p> required <code>wind_speed</code> <code>ndarray</code> <p>wind speed at source locations in [m/s].</p> required <code>theta</code> <code>ndarray</code> <p>Mathematical wind direction at source locations [radians]: calculated as np.arctan2(v_component_wind, u_component_wind).</p> required <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Horizontal wind turbulence [deg].</p> required <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Vertical wind turbulence [deg].</p> required <code>gas_density</code> <code>Union[float, ndarray]</code> <p>Gas density to use in coupling calculation [kg/m^3].</p> required <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>ndarray</code> <p>Gaussian plume coupling in (1e6)*[hr/kg]: gives concentrations in [ppm] when multiplied by sources in [kg/hr].</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_array(\n    self,\n    sensor_x: np.ndarray,\n    sensor_y: np.ndarray,\n    sensor_z: np.ndarray,\n    source_z: np.ndarray,\n    wind_speed: np.ndarray,\n    theta: np.ndarray,\n    wind_turbulence_horizontal: np.ndarray,\n    wind_turbulence_vertical: np.ndarray,\n    gas_density: Union[float, np.ndarray],\n) -&gt; np.ndarray:\n    \"\"\"Compute the Gaussian plume coupling.\n\n    Most low level function to calculate the Gaussian plume coupling. Assuming input shapes are consistent but no\n    checking is done on this.\n\n    Setting sigma_vert to 1e-16 when it is identically zero (distance_x == 0) so we don't get a divide by 0 error\n    all the time.\n\n    Args:\n        sensor_x (np.ndarray): sensor x location relative to source [m].\n        sensor_y (np.ndarray): sensor y location relative to source [m].\n        sensor_z (np.ndarray): sensor z location relative to ground height [m].\n        source_z (np.ndarray): source z location relative to ground height [m].\n        wind_speed (np.ndarray): wind speed at source locations in [m/s].\n        theta (np.ndarray): Mathematical wind direction at source locations [radians]:\n            calculated as np.arctan2(v_component_wind, u_component_wind).\n        wind_turbulence_horizontal (np.ndarray): Horizontal wind turbulence [deg].\n        wind_turbulence_vertical (np.ndarray): Vertical wind turbulence [deg].\n        gas_density (Union[float, np.ndarray]): Gas density to use in coupling calculation [kg/m^3].\n\n    Returns:\n        plume_coupling (np.ndarray): Gaussian plume coupling in (1e6)*[hr/kg]: gives concentrations\n            in [ppm] when multiplied by sources in [kg/hr].\n\n    \"\"\"\n    cos_theta = np.cos(theta)\n    sin_theta = np.sin(theta)\n\n    distance_x = cos_theta * sensor_x + sin_theta * sensor_y\n    if np.all(distance_x &lt; 0):\n        return np.zeros_like(distance_x)\n\n    distance_y = -sin_theta * sensor_x + cos_theta * sensor_y\n\n    sigma_hor = np.tan(wind_turbulence_horizontal * (np.pi / 180)) * np.abs(distance_x) + self.source_half_width\n    sigma_vert = np.tan(wind_turbulence_vertical * (np.pi / 180)) * np.abs(distance_x)\n\n    sigma_vert[sigma_vert == 0] = 1e-16\n\n    plume_coupling = (\n        (1 / (2 * np.pi * wind_speed * sigma_hor * sigma_vert))\n        * np.exp(-0.5 * (distance_y / sigma_hor) ** 2)\n        * (\n            np.exp(-0.5 * (((sensor_z + source_z) / sigma_vert) ** 2))\n            + np.exp(-0.5 * (((sensor_z - source_z) / sigma_vert) ** 2))\n        )\n    )\n\n    plume_coupling = np.divide(np.multiply(plume_coupling, 1e6), (gas_density * 3600))\n    plume_coupling[np.logical_or(distance_x &lt; 0, plume_coupling &lt; self.minimum_contribution)] = 0\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_satellite","title":"<code>compute_coupling_satellite(sensor_object, wind_speed, theta, wind_turbulence_horizontal, wind_turbulence_vertical, gas_density)</code>","text":"<p>Compute Gaussian plume coupling for satellite sensor.</p> <p>When the sensor is a Satellite object we calculate the plume coupling per source. Given the large number of sources and the possibility of using the inclusion radius and inclusion indices here and validity of a local ENU system over large distances we loop over each source and calculate the coupling on a per-source basis.</p> <p>If source_map.inclusion_n_obs is None, we do not do any filtering on observations and we want to include all observations in the plume coupling calculations.</p> <p>All np.ndarray inputs should have a shape of [1 x nof_sources]</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Sensor object used in plume coupling calculation</p> required <code>wind_speed</code> <code>ndarray</code> <p>Wind speed [m/s]</p> required <code>theta</code> <code>ndarray</code> <p>Mathematical angle between the u- and v-components of wind [radians]</p> required <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Parameter of the wind stability in horizontal direction [deg]</p> required <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Parameter of the wind stability in vertical direction [deg]</p> required <code>gas_density</code> <code>ndarray</code> <p>(np.ndarray): Numpy array containing the gas density values to use [kg/m^3]</p> required <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>list</code> <p>List of Gaussian plume coupling 1e6*[hr/kg] arrays. The list has a length of nof_sources, each array has the shape [nof_observations x 1] or [inclusion_n_obs x 1] when inclusion_idx is used.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_satellite(\n    self,\n    sensor_object: Sensor,\n    wind_speed: np.ndarray,\n    theta: np.ndarray,\n    wind_turbulence_horizontal: np.ndarray,\n    wind_turbulence_vertical: np.ndarray,\n    gas_density: np.ndarray,\n) -&gt; list:\n    \"\"\"Compute Gaussian plume coupling for satellite sensor.\n\n    When the sensor is a Satellite object we calculate the plume coupling per source. Given the large number of\n    sources and the possibility of using the inclusion radius and inclusion indices here and validity of a local\n    ENU system over large distances we loop over each source and calculate the coupling on a per-source basis.\n\n    If source_map.inclusion_n_obs is None, we do not do any filtering on observations and we want to include all\n    observations in the plume coupling calculations.\n\n    All np.ndarray inputs should have a shape of [1 x nof_sources]\n\n    Args:\n        sensor_object (Sensor): Sensor object used in plume coupling calculation\n        wind_speed (np.ndarray): Wind speed [m/s]\n        theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n        wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n        wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n        gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n    Returns:\n        plume_coupling (list): List of Gaussian plume coupling 1e6*[hr/kg] arrays. The list has a length of\n            nof_sources, each array has the shape [nof_observations x 1] or [inclusion_n_obs x 1] when\n            inclusion_idx is used.\n\n    \"\"\"\n    plume_coupling = []\n\n    source_map_location_lla = self.source_map.location.to_lla()\n    for current_source in range(self.source_map.nof_sources):\n        if self.source_map.inclusion_n_obs is None:\n            enu_sensor_array = sensor_object.location.to_enu(\n                ref_latitude=source_map_location_lla.latitude[current_source],\n                ref_longitude=source_map_location_lla.longitude[current_source],\n                ref_altitude=0,\n            ).to_array()\n\n        else:\n            if self.source_map.inclusion_n_obs[current_source] == 0:\n                plume_coupling.append(np.array([]))\n                continue\n\n            enu_sensor_array = _create_enu_sensor_array(\n                inclusion_idx=self.source_map.inclusion_idx[current_source],\n                sensor_object=sensor_object,\n                source_map_location_lla=source_map_location_lla,\n                current_source=current_source,\n            )\n\n        temp_coupling = self.compute_coupling_array(\n            enu_sensor_array[:, [0]],\n            enu_sensor_array[:, [1]],\n            enu_sensor_array[:, [2]],\n            source_map_location_lla.altitude[current_source],\n            wind_speed[:, current_source],\n            theta[:, current_source],\n            wind_turbulence_horizontal[:, current_source],\n            wind_turbulence_vertical[:, current_source],\n            gas_density[:, current_source],\n        )\n\n        plume_coupling.append(temp_coupling)\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_ground","title":"<code>compute_coupling_ground(sensor_object, wind_speed, theta, wind_turbulence_horizontal, wind_turbulence_vertical, gas_density)</code>","text":"<p>Compute Gaussian plume coupling for a ground sensor.</p> <p>If the source map is already defined as ENU the reference location is maintained but the sensor is checked to make sure the same reference location is used. Otherwise, when converting to ENU object for the sensor observations we use a single source and altitude 0 as the reference location. This way our ENU system is a system w.r.t. ground level which is required for the current implementation of the actual coupling calculation.</p> <p>When the sensor is a Beam object we calculate the plume coupling for all sources to all beam knot locations at once in the same ENU coordinate system and finally averaged over the beam knots to get the final output.</p> <p>In general, we calculate the coupling from all sources to all sensor observation locations. In order to achieve this we input the sensor array as column and source array as row vector in calculating relative x etc., with the beam knot locations being the third dimension. When the sensor is a single point Sensor or a Drone sensor we effectively have one beam knot, making the mean operation at the end effectively a reshape operation which gets rid of the third dimension.</p> <p>All np.ndarray inputs should have a shape of [nof_observations x 1]</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Sensor object used in plume coupling calculation</p> required <code>wind_speed</code> <code>ndarray</code> <p>Wind speed [m/s]</p> required <code>theta</code> <code>ndarray</code> <p>Mathematical angle between the u- and v-components of wind [radians]</p> required <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Parameter of the wind stability in horizontal direction [deg]</p> required <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Parameter of the wind stability in vertical direction [deg]</p> required <code>gas_density</code> <code>ndarray</code> <p>(np.ndarray): Numpy array containing the gas density values to use [kg/m^3]</p> required <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>ndarray</code> <p>Gaussian plume coupling 1e6*[hr/kg] array. The array has the shape [nof_observations x nof_sources]</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_ground(\n    self,\n    sensor_object: Sensor,\n    wind_speed: np.ndarray,\n    theta: np.ndarray,\n    wind_turbulence_horizontal: np.ndarray,\n    wind_turbulence_vertical: np.ndarray,\n    gas_density: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Compute Gaussian plume coupling for a ground sensor.\n\n    If the source map is already defined as ENU the reference location is maintained but the sensor is checked\n    to make sure the same reference location is used. Otherwise, when converting to ENU object for the sensor\n    observations we use a single source and altitude 0 as the reference location. This way our ENU system is a\n    system w.r.t. ground level which is required for the current implementation of the actual coupling calculation.\n\n    When the sensor is a Beam object we calculate the plume coupling for all sources to all beam knot locations at\n    once in the same ENU coordinate system and finally averaged over the beam knots to get the final output.\n\n    In general, we calculate the coupling from all sources to all sensor observation locations. In order to achieve\n    this we input the sensor array as column and source array as row vector in calculating relative x etc.,\n    with the beam knot locations being the third dimension. When the sensor is a single point Sensor or a Drone\n    sensor we effectively have one beam knot, making the mean operation at the end effectively a reshape operation\n    which gets rid of the third dimension.\n\n    All np.ndarray inputs should have a shape of [nof_observations x 1]\n\n    Args:\n        sensor_object (Sensor): Sensor object used in plume coupling calculation\n        wind_speed (np.ndarray): Wind speed [m/s]\n        theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n        wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n        wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n        gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n    Returns:\n        plume_coupling (np.ndarray): Gaussian plume coupling 1e6*[hr/kg] array. The array has the\n            shape [nof_observations x nof_sources]\n\n    \"\"\"\n    if not isinstance(self.source_map.location, ENU):\n        source_map_lla = self.source_map.location.to_lla()\n        source_map_enu = source_map_lla.to_enu(\n            ref_latitude=source_map_lla.latitude[0], ref_longitude=source_map_lla.longitude[0], ref_altitude=0\n        )\n    else:\n        source_map_enu = self.source_map.location\n\n    enu_source_array = source_map_enu.to_array()\n\n    if isinstance(sensor_object, Beam):\n        enu_sensor_array = sensor_object.make_beam_knots(\n            ref_latitude=source_map_enu.ref_latitude,\n            ref_longitude=source_map_enu.ref_longitude,\n            ref_altitude=source_map_enu.ref_altitude,\n        )\n        relative_x = np.subtract(enu_sensor_array[:, 0][None, None, :], enu_source_array[:, 0][None, :, None])\n        relative_y = np.subtract(enu_sensor_array[:, 1][None, None, :], enu_source_array[:, 1][None, :, None])\n        z_sensor = enu_sensor_array[:, 2][None, None, :]\n    else:\n        enu_sensor_array = sensor_object.location.to_enu(\n            ref_latitude=source_map_enu.ref_latitude,\n            ref_longitude=source_map_enu.ref_longitude,\n            ref_altitude=source_map_enu.ref_altitude,\n        ).to_array()\n        relative_x = np.subtract(enu_sensor_array[:, 0][:, None, None], enu_source_array[:, 0][None, :, None])\n        relative_y = np.subtract(enu_sensor_array[:, 1][:, None, None], enu_source_array[:, 1][None, :, None])\n        z_sensor = enu_sensor_array[:, 2][:, None, None]\n\n    z_source = enu_source_array[:, 2][None, :, None]\n\n    plume_coupling = self.compute_coupling_array(\n        relative_x,\n        relative_y,\n        z_sensor,\n        z_source,\n        wind_speed[:, :, None],\n        theta[:, :, None],\n        wind_turbulence_horizontal[:, :, None],\n        wind_turbulence_vertical[:, :, None],\n        gas_density[:, :, None],\n    )\n\n    plume_coupling = plume_coupling.mean(axis=2)\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/site_layout/","title":"Site Layout","text":""},{"location":"pyelq/dispersion_model/site_layout/#site-layout","title":"Site Layout","text":"<p>Site layout module.</p> <p>This module defines the SiteLayout class, which represents the layout of a site, giving the option to include obstacles (e.g. buildings, tanks, equipment) as cylinders obstructing the flow field.</p>"},{"location":"pyelq/dispersion_model/site_layout/#pyelq.dispersion_model.site_layout.SiteLayout","title":"<code>SiteLayout</code>  <code>dataclass</code>","text":"<p>Class for site layout defining cylindrical obstacles in the environment.</p> <p>These are used with MeteorologyWindfield to calculate the wind field at each grid point with a potential flow around the cylindrical obstacles.</p> <p>Attributes:</p> Name Type Description <code>cylinder_coordinates</code> <code>Union[ENU, None]</code> <p>The coordinates of the cylindrical obstacles in the site layout. The east, north represent the the center of the cylinder and the up coordinate represents the cylinder height.</p> <code>cylinder_radius</code> <code>ndarray</code> <p>The radius of the cylindrical obstacles in the site layout.</p> <code>id_obstacles</code> <code>ndarray</code> <p>Boolean array indicating which grid points are within obstacle regions.</p> <code>id_obstacles_index</code> <code>ndarray</code> <p>The indices of the grid points that are within obstacle regions.</p> Source code in <code>src/pyelq/dispersion_model/site_layout.py</code> <pre><code>@dataclass\nclass SiteLayout:\n    \"\"\"Class for site layout defining cylindrical obstacles in the environment.\n\n    These are used with MeteorologyWindfield to calculate the wind field at each grid point with a potential flow\n    around the cylindrical obstacles.\n\n    Attributes:\n        cylinder_coordinates (Union[ENU, None]): The coordinates of the cylindrical obstacles in the site layout. The\n            east, north represent the the center of the cylinder and the up coordinate represents the cylinder height.\n        cylinder_radius (np.ndarray): The radius of the cylindrical obstacles in the site layout.\n        id_obstacles (np.ndarray): Boolean array indicating which grid points are within obstacle regions.\n        id_obstacles_index (np.ndarray): The indices of the grid points that are within obstacle regions.\n\n    \"\"\"\n\n    cylinder_coordinates: Union[ENU, None]\n    cylinder_radius: np.ndarray\n\n    id_obstacles: np.ndarray = field(init=False)\n    id_obstacles_index: np.ndarray = field(init=False)\n\n    @property\n    def nof_cylinders(self) -&gt; int:\n        \"\"\"Int: Returns the number of cylinders in the site layout.\"\"\"\n        if self.cylinder_coordinates is None:\n            return 0\n        return self.cylinder_coordinates.nof_observations\n\n    def find_index_obstacles(self, grid_coordinates: ENU):\n        \"\"\"Find the indices of the grid_coordinates that are within the radius of the obstacles.\n\n        This method uses a KDTree to efficiently find the indices of the grid points that are within the radius of the\n        cylindrical obstacles. It also checks the height of the grid points against the height of the obstacles.\n        If the height of the grid point is greater than the height of the obstacle, it is not considered an obstacle.\n\n        Args:\n            grid_coordinates (ENU): The coordinates of the grid points to check.\n\n        \"\"\"\n        if self.cylinder_coordinates is None or self.cylinder_coordinates.nof_observations == 0:\n            self.id_obstacles = np.zeros((grid_coordinates.nof_observations, 1), dtype=bool)\n            return\n\n        self.check_reference_coordinates(grid_coordinates)\n\n        grid_coordinates_array = grid_coordinates.to_array(dim=2)\n        tree = spatial.KDTree(grid_coordinates_array)\n        indices = tree.query_ball_point(x=self.cylinder_coordinates.to_array(dim=2), r=self.cylinder_radius.flatten())\n\n        if grid_coordinates.up is not None:\n\n            for i, height in enumerate(self.cylinder_coordinates.up):\n                indices[i] = np.array(indices[i])\n                if len(indices[i]) &gt; 0:\n                    indices[i] = indices[i][grid_coordinates.up[indices[i]].flatten() &lt;= height]\n        indices_conc = np.unique(np.concatenate(indices, axis=0)).astype(int)\n        self.id_obstacles_index = indices_conc\n        self.id_obstacles = np.zeros((grid_coordinates.nof_observations, 1), dtype=bool)\n        self.id_obstacles[[indices_conc]] = True\n\n    def check_reference_coordinates(self, grid_coordinates: ENU):\n        \"\"\"Check if the reference coordinates of the grid and cylinder coordinates match.\n\n        Args:\n            grid_coordinates (ENU): The coordinates of the grid points to check.\n\n        Raises:\n            ValueError: If the reference coordinates do not match.\n        \"\"\"\n        if grid_coordinates.ref_altitude != self.cylinder_coordinates.ref_altitude:\n            raise ValueError(\"Grid coordinates and cylinder coordinates must have the same reference altitude.\")\n        if grid_coordinates.ref_longitude != self.cylinder_coordinates.ref_longitude:\n            raise ValueError(\"Grid coordinates and cylinder coordinates must have the same reference longitude.\")\n        if grid_coordinates.ref_latitude != self.cylinder_coordinates.ref_latitude:\n            raise ValueError(\"Grid coordinates and cylinder coordinates must have the same reference latitude.\")\n</code></pre>"},{"location":"pyelq/dispersion_model/site_layout/#pyelq.dispersion_model.site_layout.SiteLayout.nof_cylinders","title":"<code>nof_cylinders</code>  <code>property</code>","text":""},{"location":"pyelq/dispersion_model/site_layout/#pyelq.dispersion_model.site_layout.SiteLayout.find_index_obstacles","title":"<code>find_index_obstacles(grid_coordinates)</code>","text":"<p>Find the indices of the grid_coordinates that are within the radius of the obstacles.</p> <p>This method uses a KDTree to efficiently find the indices of the grid points that are within the radius of the cylindrical obstacles. It also checks the height of the grid points against the height of the obstacles. If the height of the grid point is greater than the height of the obstacle, it is not considered an obstacle.</p> <p>Parameters:</p> Name Type Description Default <code>grid_coordinates</code> <code>ENU</code> <p>The coordinates of the grid points to check.</p> required Source code in <code>src/pyelq/dispersion_model/site_layout.py</code> <pre><code>def find_index_obstacles(self, grid_coordinates: ENU):\n    \"\"\"Find the indices of the grid_coordinates that are within the radius of the obstacles.\n\n    This method uses a KDTree to efficiently find the indices of the grid points that are within the radius of the\n    cylindrical obstacles. It also checks the height of the grid points against the height of the obstacles.\n    If the height of the grid point is greater than the height of the obstacle, it is not considered an obstacle.\n\n    Args:\n        grid_coordinates (ENU): The coordinates of the grid points to check.\n\n    \"\"\"\n    if self.cylinder_coordinates is None or self.cylinder_coordinates.nof_observations == 0:\n        self.id_obstacles = np.zeros((grid_coordinates.nof_observations, 1), dtype=bool)\n        return\n\n    self.check_reference_coordinates(grid_coordinates)\n\n    grid_coordinates_array = grid_coordinates.to_array(dim=2)\n    tree = spatial.KDTree(grid_coordinates_array)\n    indices = tree.query_ball_point(x=self.cylinder_coordinates.to_array(dim=2), r=self.cylinder_radius.flatten())\n\n    if grid_coordinates.up is not None:\n\n        for i, height in enumerate(self.cylinder_coordinates.up):\n            indices[i] = np.array(indices[i])\n            if len(indices[i]) &gt; 0:\n                indices[i] = indices[i][grid_coordinates.up[indices[i]].flatten() &lt;= height]\n    indices_conc = np.unique(np.concatenate(indices, axis=0)).astype(int)\n    self.id_obstacles_index = indices_conc\n    self.id_obstacles = np.zeros((grid_coordinates.nof_observations, 1), dtype=bool)\n    self.id_obstacles[[indices_conc]] = True\n</code></pre>"},{"location":"pyelq/dispersion_model/site_layout/#pyelq.dispersion_model.site_layout.SiteLayout.check_reference_coordinates","title":"<code>check_reference_coordinates(grid_coordinates)</code>","text":"<p>Check if the reference coordinates of the grid and cylinder coordinates match.</p> <p>Parameters:</p> Name Type Description Default <code>grid_coordinates</code> <code>ENU</code> <p>The coordinates of the grid points to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reference coordinates do not match.</p> Source code in <code>src/pyelq/dispersion_model/site_layout.py</code> <pre><code>def check_reference_coordinates(self, grid_coordinates: ENU):\n    \"\"\"Check if the reference coordinates of the grid and cylinder coordinates match.\n\n    Args:\n        grid_coordinates (ENU): The coordinates of the grid points to check.\n\n    Raises:\n        ValueError: If the reference coordinates do not match.\n    \"\"\"\n    if grid_coordinates.ref_altitude != self.cylinder_coordinates.ref_altitude:\n        raise ValueError(\"Grid coordinates and cylinder coordinates must have the same reference altitude.\")\n    if grid_coordinates.ref_longitude != self.cylinder_coordinates.ref_longitude:\n        raise ValueError(\"Grid coordinates and cylinder coordinates must have the same reference longitude.\")\n    if grid_coordinates.ref_latitude != self.cylinder_coordinates.ref_latitude:\n        raise ValueError(\"Grid coordinates and cylinder coordinates must have the same reference latitude.\")\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/","title":"Overview","text":""},{"location":"pyelq/meteorology/meteorology/#meteorology","title":"Meteorology","text":"<p>Meteorology module.</p> <p>The superclass for the meteorology classes</p>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology","title":"<code>Meteorology</code>  <code>dataclass</code>","text":"<p>Defines the properties and methods of the meteorology class.</p> <p>Sizes of all attributes should match.</p> <p>Attributes:</p> Name Type Description <code>wind_speed</code> <code>ndarray</code> <p>Wind speed [m/s]</p> <code>wind_direction</code> <code>ndarray</code> <p>Meteorological wind direction (from) [deg], see https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398</p> <code>u_component</code> <code>ndarray</code> <p>u component of wind [m/s] in the easterly direction</p> <code>v_component</code> <code>ndarray</code> <p>v component of wind [m/s] in the northerly direction</p> <code>w_component</code> <code>ndarray</code> <p>w component of wind [m/s] in the vertical direction</p> <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Parameter of the wind stability in horizontal direction [deg]</p> <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Parameter of the wind stability in vertical direction [deg]</p> <code>pressure</code> <code>ndarray</code> <p>Pressure [kPa]</p> <code>temperature</code> <code>ndarray</code> <p>Temperature [K]</p> <code>atmospheric_boundary_layer</code> <code>ndarray</code> <p>Atmospheric boundary layer [m]</p> <code>surface_albedo</code> <code>ndarray</code> <p>Surface reflectance parameter [unitless]</p> <code>time</code> <code>DatetimeArray</code> <p>Array containing time values associated with the meteorological observation</p> <code>location</code> <code>Coordinate</code> <p>(Coordinate, optional): Coordinate object specifying the meteorological observation locations</p> <code>label</code> <code>str</code> <p>String label for object</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>@dataclass\nclass Meteorology:\n    \"\"\"Defines the properties and methods of the meteorology class.\n\n    Sizes of all attributes should match.\n\n    Attributes:\n        wind_speed (np.ndarray, optional): Wind speed [m/s]\n        wind_direction (np.ndarray, optional): Meteorological wind direction (from) [deg], see\n            https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n        u_component (np.ndarray, optional): u component of wind [m/s] in the easterly direction\n        v_component (np.ndarray, optional): v component of wind [m/s] in the northerly direction\n        w_component (np.ndarray, optional): w component of wind [m/s] in the vertical direction\n        wind_turbulence_horizontal (np.ndarray, optional): Parameter of the wind stability in\n            horizontal direction [deg]\n        wind_turbulence_vertical (np.ndarray, optional): Parameter of the wind stability in\n            vertical direction [deg]\n        pressure (np.ndarray, optional): Pressure [kPa]\n        temperature (np.ndarray, optional): Temperature [K]\n        atmospheric_boundary_layer (np.ndarray, optional): Atmospheric boundary layer [m]\n        surface_albedo (np.ndarray, optional): Surface reflectance parameter [unitless]\n        time (pandas.arrays.DatetimeArray, optional): Array containing time values associated with the\n            meteorological observation\n        location: (Coordinate, optional): Coordinate object specifying the meteorological observation locations\n        label (str, optional): String label for object\n\n    \"\"\"\n\n    wind_speed: np.ndarray = field(init=False, default=None)\n    wind_direction: np.ndarray = field(init=False, default=None)\n    u_component: np.ndarray = field(init=False, default=None)\n    v_component: np.ndarray = field(init=False, default=None)\n    w_component: np.ndarray = field(init=False, default=None)\n    wind_turbulence_horizontal: np.ndarray = field(init=False, default=None)\n    wind_turbulence_vertical: np.ndarray = field(init=False, default=None)\n    pressure: np.ndarray = field(init=False, default=None)\n    temperature: np.ndarray = field(init=False, default=None)\n    atmospheric_boundary_layer: np.ndarray = field(init=False, default=None)\n    surface_albedo: np.ndarray = field(init=False, default=None)\n    time: DatetimeArray = field(init=False, default=None)\n    location: Coordinate = field(init=False, default=None)\n    label: str = field(init=False)\n\n    @property\n    def nof_observations(self) -&gt; int:\n        \"\"\"Number of observations.\"\"\"\n        if self.time is None:\n            return 0\n        return self.time.size\n\n    def calculate_wind_speed_from_uv(self) -&gt; None:\n        \"\"\"Calculate wind speed.\n\n        Calculate the wind speed from u and v components. Result gets stored in the wind_speed attribute\n\n        \"\"\"\n        self.wind_speed = np.sqrt(self.u_component**2 + self.v_component**2)\n\n    def calculate_wind_direction_from_uv(self) -&gt; None:\n        \"\"\"Calculate wind direction: meteorological convention 0 is wind from the North.\n\n        Calculate the wind direction from u and v components. Result gets stored in the wind_direction attribute\n        See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n        \"\"\"\n        self.wind_direction = (270 - 180 / np.pi * np.arctan2(self.v_component, self.u_component)) % 360\n\n    def calculate_uv_from_wind_speed_direction(self) -&gt; None:\n        \"\"\"Calculate u and v components from wind speed and direction.\n\n        Results get stored in the u_component and v_component attributes.\n        See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n        \"\"\"\n        self.u_component = -1 * self.wind_speed * np.sin(self.wind_direction * (np.pi / 180))\n        self.v_component = -1 * self.wind_speed * np.cos(self.wind_direction * (np.pi / 180))\n\n    def calculate_wind_turbulence_horizontal(self, window: str) -&gt; None:\n        \"\"\"Calculate the horizontal wind turbulence values from the wind direction attribute.\n\n        Wind turbulence values are calculated as the circular standard deviation of wind direction\n        (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circstd.html).\n        The implementation here is equivalent to using the circstd function from scipy.stats as an apply\n        function on a rolling window. However, using the rolling mean on sin and cos speeds up\n        the calculation by a factor of 100.\n\n        Outputted values are calculated at the center of the window and at least 3 observations are required in a\n        window for the calculation. If the window contains less values the result will be np.nan.\n        The result of the calculation will be stored as the wind_turbulence_horizontal attribute.\n\n        Args:\n            window (str): The size of the window in which values are aggregated specified as an offset alias:\n                https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n\n        \"\"\"\n        data_series = pd.Series(data=self.wind_direction, index=self.time)\n        sin_rolling = (np.sin(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n        cos_rolling = (np.cos(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n        aggregated_data = np.sqrt(-2 * np.log((sin_rolling**2 + cos_rolling**2) ** 0.5)) * 180 / np.pi\n        self.wind_turbulence_horizontal = aggregated_data.values\n\n    def plot_polar_hist(self, nof_sectors: int = 16, nof_divisions: int = 5, template: object = None) -&gt; go.Figure():\n        \"\"\"Plots a histogram of wind speed and wind direction in polar Coordinates.\n\n        Args:\n            nof_sectors (int, optional): The number of wind direction sectors into which the data is binned.\n            nof_divisions (int, optional): The number of wind speed divisions into which the data is binned.\n            template (object): A layout template which can be applied to the plot. Defaults to None.\n\n        Returns:\n            fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n        \"\"\"\n        sector_half_width = 0.5 * (360 / nof_sectors)\n        wind_direction_bin_edges = np.linspace(-sector_half_width, 360 - sector_half_width, nof_sectors + 1)\n        wind_speed_bin_edges = np.linspace(np.min(self.wind_speed), np.max(self.wind_speed), nof_divisions)\n\n        dataframe = pd.DataFrame()\n        dataframe[\"wind_direction\"] = [x - 360 if x &gt; (360 - sector_half_width) else x for x in self.wind_direction]\n        dataframe[\"wind_speed\"] = self.wind_speed\n\n        dataframe[\"sector\"] = pd.cut(dataframe[\"wind_direction\"], wind_direction_bin_edges, include_lowest=True)\n        if np.allclose(wind_speed_bin_edges[0], wind_speed_bin_edges):\n            dataframe[\"speed\"] = wind_speed_bin_edges[0]\n        else:\n            dataframe[\"speed\"] = pd.cut(dataframe[\"wind_speed\"], wind_speed_bin_edges, include_lowest=True)\n\n        dataframe = dataframe.groupby([\"sector\", \"speed\"], observed=False).count()\n        dataframe = dataframe.rename(columns={\"wind_speed\": \"count\"}).drop(columns=[\"wind_direction\"])\n        dataframe[\"%\"] = dataframe[\"count\"] / dataframe[\"count\"].sum()\n\n        dataframe = dataframe.reset_index()\n        dataframe[\"theta\"] = dataframe.apply(lambda x: x[\"sector\"].mid, axis=1)\n\n        fig = px.bar_polar(\n            dataframe,\n            r=\"%\",\n            theta=\"theta\",\n            color=\"speed\",\n            direction=\"clockwise\",\n            start_angle=90,\n            color_discrete_sequence=px.colors.sequential.Sunset_r,\n        )\n\n        ticktext = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n        polar_dict = {\n            \"radialaxis\": {\"tickangle\": 90},\n            \"radialaxis_angle\": 90,\n            \"angularaxis\": {\n                \"tickmode\": \"array\",\n                \"ticktext\": ticktext,\n                \"tickvals\": list(np.linspace(0, 360 - (360 / 8), 8)),\n            },\n        }\n        fig.add_annotation(\n            x=1,\n            y=1,\n            yref=\"paper\",\n            xref=\"paper\",\n            xanchor=\"right\",\n            yanchor=\"top\",\n            align=\"left\",\n            font={\"size\": 18, \"color\": \"#000000\"},\n            showarrow=False,\n            borderwidth=2,\n            borderpad=10,\n            bgcolor=\"#ffffff\",\n            bordercolor=\"#000000\",\n            opacity=0.8,\n            text=\"&lt;b&gt;Radial Axis:&lt;/b&gt; Proportion&lt;br&gt;of wind measurements&lt;br&gt;in a given direction.\",\n        )\n\n        fig.update_layout(polar=polar_dict)\n        fig.update_layout(template=template)\n        fig.update_layout(title=\"Distribution of Wind Speeds and Directions\")\n\n        return fig\n\n    def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n        \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar Coordinates.\n\n        This function implements the polar scatter functionality for a (single) Meteorology object. Assuming the all\n        Sensors in the SensorGroup are consistent with the Meteorology object.\n\n        Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n        in plot colors.\n\n        Args:\n            fig (go.Figure): A plotly figure onto which traces can be drawn.\n            sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n            template (object): A layout template which can be applied to the plot. Defaults to None.\n\n        Returns:\n            fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n        \"\"\"\n        max_concentration = 0\n\n        for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n            if sensor.concentration.shape != self.wind_direction.shape:\n                warnings.warn(\n                    f\"Concentration values for sensor {sensor_key} are of shape \"\n                    + f\"{sensor.concentration.shape}, but self.wind_direction has shape \"\n                    + f\"{self.wind_direction.shape}. It will not be plotted on the polar scatter plot.\"\n                )\n            else:\n                theta = self.wind_direction\n                color_idx = i % len(sensor_object.color_map)\n\n                fig.add_trace(\n                    go.Scatterpolar(\n                        r=sensor.concentration,\n                        theta=theta,\n                        mode=\"markers\",\n                        name=sensor_key,\n                        marker={\"color\": sensor_object.color_map[color_idx]},\n                    )\n                )\n                if sensor.concentration.size &gt; 0:\n                    max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n        fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n        return fig\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations.</p>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.calculate_wind_speed_from_uv","title":"<code>calculate_wind_speed_from_uv()</code>","text":"<p>Calculate wind speed.</p> <p>Calculate the wind speed from u and v components. Result gets stored in the wind_speed attribute</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_wind_speed_from_uv(self) -&gt; None:\n    \"\"\"Calculate wind speed.\n\n    Calculate the wind speed from u and v components. Result gets stored in the wind_speed attribute\n\n    \"\"\"\n    self.wind_speed = np.sqrt(self.u_component**2 + self.v_component**2)\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.calculate_wind_direction_from_uv","title":"<code>calculate_wind_direction_from_uv()</code>","text":"<p>Calculate wind direction: meteorological convention 0 is wind from the North.</p> <p>Calculate the wind direction from u and v components. Result gets stored in the wind_direction attribute See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_wind_direction_from_uv(self) -&gt; None:\n    \"\"\"Calculate wind direction: meteorological convention 0 is wind from the North.\n\n    Calculate the wind direction from u and v components. Result gets stored in the wind_direction attribute\n    See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n    \"\"\"\n    self.wind_direction = (270 - 180 / np.pi * np.arctan2(self.v_component, self.u_component)) % 360\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.calculate_uv_from_wind_speed_direction","title":"<code>calculate_uv_from_wind_speed_direction()</code>","text":"<p>Calculate u and v components from wind speed and direction.</p> <p>Results get stored in the u_component and v_component attributes. See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_uv_from_wind_speed_direction(self) -&gt; None:\n    \"\"\"Calculate u and v components from wind speed and direction.\n\n    Results get stored in the u_component and v_component attributes.\n    See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n    \"\"\"\n    self.u_component = -1 * self.wind_speed * np.sin(self.wind_direction * (np.pi / 180))\n    self.v_component = -1 * self.wind_speed * np.cos(self.wind_direction * (np.pi / 180))\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.calculate_wind_turbulence_horizontal","title":"<code>calculate_wind_turbulence_horizontal(window)</code>","text":"<p>Calculate the horizontal wind turbulence values from the wind direction attribute.</p> <p>Wind turbulence values are calculated as the circular standard deviation of wind direction (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circstd.html). The implementation here is equivalent to using the circstd function from scipy.stats as an apply function on a rolling window. However, using the rolling mean on sin and cos speeds up the calculation by a factor of 100.</p> <p>Outputted values are calculated at the center of the window and at least 3 observations are required in a window for the calculation. If the window contains less values the result will be np.nan. The result of the calculation will be stored as the wind_turbulence_horizontal attribute.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>str</code> <p>The size of the window in which values are aggregated specified as an offset alias: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases</p> required Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_wind_turbulence_horizontal(self, window: str) -&gt; None:\n    \"\"\"Calculate the horizontal wind turbulence values from the wind direction attribute.\n\n    Wind turbulence values are calculated as the circular standard deviation of wind direction\n    (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circstd.html).\n    The implementation here is equivalent to using the circstd function from scipy.stats as an apply\n    function on a rolling window. However, using the rolling mean on sin and cos speeds up\n    the calculation by a factor of 100.\n\n    Outputted values are calculated at the center of the window and at least 3 observations are required in a\n    window for the calculation. If the window contains less values the result will be np.nan.\n    The result of the calculation will be stored as the wind_turbulence_horizontal attribute.\n\n    Args:\n        window (str): The size of the window in which values are aggregated specified as an offset alias:\n            https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n\n    \"\"\"\n    data_series = pd.Series(data=self.wind_direction, index=self.time)\n    sin_rolling = (np.sin(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n    cos_rolling = (np.cos(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n    aggregated_data = np.sqrt(-2 * np.log((sin_rolling**2 + cos_rolling**2) ** 0.5)) * 180 / np.pi\n    self.wind_turbulence_horizontal = aggregated_data.values\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.plot_polar_hist","title":"<code>plot_polar_hist(nof_sectors=16, nof_divisions=5, template=None)</code>","text":"<p>Plots a histogram of wind speed and wind direction in polar Coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>nof_sectors</code> <code>int</code> <p>The number of wind direction sectors into which the data is binned.</p> <code>16</code> <code>nof_divisions</code> <code>int</code> <p>The number of wind speed divisions into which the data is binned.</p> <code>5</code> <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def plot_polar_hist(self, nof_sectors: int = 16, nof_divisions: int = 5, template: object = None) -&gt; go.Figure():\n    \"\"\"Plots a histogram of wind speed and wind direction in polar Coordinates.\n\n    Args:\n        nof_sectors (int, optional): The number of wind direction sectors into which the data is binned.\n        nof_divisions (int, optional): The number of wind speed divisions into which the data is binned.\n        template (object): A layout template which can be applied to the plot. Defaults to None.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    \"\"\"\n    sector_half_width = 0.5 * (360 / nof_sectors)\n    wind_direction_bin_edges = np.linspace(-sector_half_width, 360 - sector_half_width, nof_sectors + 1)\n    wind_speed_bin_edges = np.linspace(np.min(self.wind_speed), np.max(self.wind_speed), nof_divisions)\n\n    dataframe = pd.DataFrame()\n    dataframe[\"wind_direction\"] = [x - 360 if x &gt; (360 - sector_half_width) else x for x in self.wind_direction]\n    dataframe[\"wind_speed\"] = self.wind_speed\n\n    dataframe[\"sector\"] = pd.cut(dataframe[\"wind_direction\"], wind_direction_bin_edges, include_lowest=True)\n    if np.allclose(wind_speed_bin_edges[0], wind_speed_bin_edges):\n        dataframe[\"speed\"] = wind_speed_bin_edges[0]\n    else:\n        dataframe[\"speed\"] = pd.cut(dataframe[\"wind_speed\"], wind_speed_bin_edges, include_lowest=True)\n\n    dataframe = dataframe.groupby([\"sector\", \"speed\"], observed=False).count()\n    dataframe = dataframe.rename(columns={\"wind_speed\": \"count\"}).drop(columns=[\"wind_direction\"])\n    dataframe[\"%\"] = dataframe[\"count\"] / dataframe[\"count\"].sum()\n\n    dataframe = dataframe.reset_index()\n    dataframe[\"theta\"] = dataframe.apply(lambda x: x[\"sector\"].mid, axis=1)\n\n    fig = px.bar_polar(\n        dataframe,\n        r=\"%\",\n        theta=\"theta\",\n        color=\"speed\",\n        direction=\"clockwise\",\n        start_angle=90,\n        color_discrete_sequence=px.colors.sequential.Sunset_r,\n    )\n\n    ticktext = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n    polar_dict = {\n        \"radialaxis\": {\"tickangle\": 90},\n        \"radialaxis_angle\": 90,\n        \"angularaxis\": {\n            \"tickmode\": \"array\",\n            \"ticktext\": ticktext,\n            \"tickvals\": list(np.linspace(0, 360 - (360 / 8), 8)),\n        },\n    }\n    fig.add_annotation(\n        x=1,\n        y=1,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        align=\"left\",\n        font={\"size\": 18, \"color\": \"#000000\"},\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=\"&lt;b&gt;Radial Axis:&lt;/b&gt; Proportion&lt;br&gt;of wind measurements&lt;br&gt;in a given direction.\",\n    )\n\n    fig.update_layout(polar=polar_dict)\n    fig.update_layout(template=template)\n    fig.update_layout(title=\"Distribution of Wind Speeds and Directions\")\n\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.Meteorology.plot_polar_scatter","title":"<code>plot_polar_scatter(fig, sensor_object, template=None)</code>","text":"<p>Plots a scatter plot of concentration with respect to wind direction in polar Coordinates.</p> <p>This function implements the polar scatter functionality for a (single) Meteorology object. Assuming the all Sensors in the SensorGroup are consistent with the Meteorology object.</p> <p>Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency in plot colors.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>A plotly figure onto which traces can be drawn.</p> required <code>sensor_object</code> <code>SensorGroup</code> <p>SensorGroup object which contains the concentration information</p> required <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n    \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar Coordinates.\n\n    This function implements the polar scatter functionality for a (single) Meteorology object. Assuming the all\n    Sensors in the SensorGroup are consistent with the Meteorology object.\n\n    Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n    in plot colors.\n\n    Args:\n        fig (go.Figure): A plotly figure onto which traces can be drawn.\n        sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n        template (object): A layout template which can be applied to the plot. Defaults to None.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    \"\"\"\n    max_concentration = 0\n\n    for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n        if sensor.concentration.shape != self.wind_direction.shape:\n            warnings.warn(\n                f\"Concentration values for sensor {sensor_key} are of shape \"\n                + f\"{sensor.concentration.shape}, but self.wind_direction has shape \"\n                + f\"{self.wind_direction.shape}. It will not be plotted on the polar scatter plot.\"\n            )\n        else:\n            theta = self.wind_direction\n            color_idx = i % len(sensor_object.color_map)\n\n            fig.add_trace(\n                go.Scatterpolar(\n                    r=sensor.concentration,\n                    theta=theta,\n                    mode=\"markers\",\n                    name=sensor_key,\n                    marker={\"color\": sensor_object.color_map[color_idx]},\n                )\n            )\n            if sensor.concentration.size &gt; 0:\n                max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n    fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup","title":"<code>MeteorologyGroup</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict</code></p> <p>A dictionary containing multiple Meteorology objects.</p> <p>This class is used when we want to define/store a collection of meteorology objects consistent with an associated SensorGroup which can then be used in further processing, e.g. Gaussian plume coupling computation.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>@dataclass\nclass MeteorologyGroup(dict):\n    \"\"\"A dictionary containing multiple Meteorology objects.\n\n    This class is used when we want to define/store a collection of meteorology objects consistent with an associated\n    SensorGroup which can then be used in further processing, e.g. Gaussian plume coupling computation.\n\n    \"\"\"\n\n    @property\n    def nof_objects(self) -&gt; int:\n        \"\"\"Int: Number of meteorology objects contained in the MeteorologyGroup.\"\"\"\n        return len(self)\n\n    def add_object(self, met_object: Meteorology):\n        \"\"\"Add an object to the MeteorologyGroup.\"\"\"\n        self[met_object.label] = met_object\n\n    def calculate_uv_from_wind_speed_direction(self):\n        \"\"\"Calculate the u and v components for each member of the group.\"\"\"\n        for met in self.values():\n            met.calculate_uv_from_wind_speed_direction()\n\n    def calculate_wind_direction_from_uv(self):\n        \"\"\"Calculate wind direction from the u and v components for each member of the group.\"\"\"\n        for met in self.values():\n            met.calculate_wind_direction_from_uv()\n\n    def calculate_wind_speed_from_uv(self):\n        \"\"\"Calculate wind speed from the u and v components for each member of the group.\"\"\"\n        for met in self.values():\n            met.calculate_wind_speed_from_uv()\n\n    def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n        \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar coordinates.\n\n        This function implements the polar scatter functionality for a MeteorologyGroup object. It assumes each object\n        in the SensorGroup has an associated Meteorology object in the MeteorologyGroup.\n\n        Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n        in plot colors.\n\n        Args:\n            fig (go.Figure): A plotly figure onto which traces can be drawn.\n            sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n            template (object): A layout template which can be applied to the plot. Defaults to None.\n\n        Returns:\n            fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n        Raises\n            ValueError: When there is a sensor key which is not present in the MeteorologyGroup.\n\n        \"\"\"\n        max_concentration = 0\n\n        for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n            if sensor_key not in self.keys():\n                raise ValueError(f\"Key {sensor_key} not found in MeteorologyGroup.\")\n            temp_met_object = self[sensor_key]\n            if sensor.concentration.shape != temp_met_object.wind_direction.shape:\n                warnings.warn(\n                    f\"Concentration values for sensor {sensor_key} are of shape \"\n                    + f\"{sensor.concentration.shape}, but wind_direction values for meteorology object {sensor_key} \"\n                    f\"has shape {temp_met_object.wind_direction.shape}. It will not be plotted on the polar scatter \"\n                    f\"plot.\"\n                )\n            else:\n                theta = temp_met_object.wind_direction\n                color_idx = i % len(sensor_object.color_map)\n\n                fig.add_trace(\n                    go.Scatterpolar(\n                        r=sensor.concentration,\n                        theta=theta,\n                        mode=\"markers\",\n                        name=sensor_key,\n                        marker={\"color\": sensor_object.color_map[color_idx]},\n                    )\n                )\n\n                if sensor.concentration.size &gt; 0:\n                    max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n        fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n        return fig\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup.nof_objects","title":"<code>nof_objects</code>  <code>property</code>","text":""},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup.add_object","title":"<code>add_object(met_object)</code>","text":"<p>Add an object to the MeteorologyGroup.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def add_object(self, met_object: Meteorology):\n    \"\"\"Add an object to the MeteorologyGroup.\"\"\"\n    self[met_object.label] = met_object\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup.calculate_uv_from_wind_speed_direction","title":"<code>calculate_uv_from_wind_speed_direction()</code>","text":"<p>Calculate the u and v components for each member of the group.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_uv_from_wind_speed_direction(self):\n    \"\"\"Calculate the u and v components for each member of the group.\"\"\"\n    for met in self.values():\n        met.calculate_uv_from_wind_speed_direction()\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup.calculate_wind_direction_from_uv","title":"<code>calculate_wind_direction_from_uv()</code>","text":"<p>Calculate wind direction from the u and v components for each member of the group.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_wind_direction_from_uv(self):\n    \"\"\"Calculate wind direction from the u and v components for each member of the group.\"\"\"\n    for met in self.values():\n        met.calculate_wind_direction_from_uv()\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup.calculate_wind_speed_from_uv","title":"<code>calculate_wind_speed_from_uv()</code>","text":"<p>Calculate wind speed from the u and v components for each member of the group.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def calculate_wind_speed_from_uv(self):\n    \"\"\"Calculate wind speed from the u and v components for each member of the group.\"\"\"\n    for met in self.values():\n        met.calculate_wind_speed_from_uv()\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.MeteorologyGroup.plot_polar_scatter","title":"<code>plot_polar_scatter(fig, sensor_object, template=None)</code>","text":"<p>Plots a scatter plot of concentration with respect to wind direction in polar coordinates.</p> <p>This function implements the polar scatter functionality for a MeteorologyGroup object. It assumes each object in the SensorGroup has an associated Meteorology object in the MeteorologyGroup.</p> <p>Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency in plot colors.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>A plotly figure onto which traces can be drawn.</p> required <code>sensor_object</code> <code>SensorGroup</code> <p>SensorGroup object which contains the concentration information</p> required <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> <p>Raises     ValueError: When there is a sensor key which is not present in the MeteorologyGroup.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n    \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar coordinates.\n\n    This function implements the polar scatter functionality for a MeteorologyGroup object. It assumes each object\n    in the SensorGroup has an associated Meteorology object in the MeteorologyGroup.\n\n    Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n    in plot colors.\n\n    Args:\n        fig (go.Figure): A plotly figure onto which traces can be drawn.\n        sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n        template (object): A layout template which can be applied to the plot. Defaults to None.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    Raises\n        ValueError: When there is a sensor key which is not present in the MeteorologyGroup.\n\n    \"\"\"\n    max_concentration = 0\n\n    for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n        if sensor_key not in self.keys():\n            raise ValueError(f\"Key {sensor_key} not found in MeteorologyGroup.\")\n        temp_met_object = self[sensor_key]\n        if sensor.concentration.shape != temp_met_object.wind_direction.shape:\n            warnings.warn(\n                f\"Concentration values for sensor {sensor_key} are of shape \"\n                + f\"{sensor.concentration.shape}, but wind_direction values for meteorology object {sensor_key} \"\n                f\"has shape {temp_met_object.wind_direction.shape}. It will not be plotted on the polar scatter \"\n                f\"plot.\"\n            )\n        else:\n            theta = temp_met_object.wind_direction\n            color_idx = i % len(sensor_object.color_map)\n\n            fig.add_trace(\n                go.Scatterpolar(\n                    r=sensor.concentration,\n                    theta=theta,\n                    mode=\"markers\",\n                    name=sensor_key,\n                    marker={\"color\": sensor_object.color_map[color_idx]},\n                )\n            )\n\n            if sensor.concentration.size &gt; 0:\n                max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n    fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/meteorology/#pyelq.meteorology.meteorology.set_plot_polar_scatter_layout","title":"<code>set_plot_polar_scatter_layout(max_concentration, fig, template)</code>","text":"<p>Helper function to set the layout of the polar scatter plot.</p> <p>Helps avoid code duplication.</p> <p>Parameters:</p> Name Type Description Default <code>max_concentration</code> <code>float</code> <p>The maximum concentration value used to update radial axis range.</p> required <code>fig</code> <code>Figure</code> <p>A plotly figure onto which traces can be drawn.</p> required <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot.</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> Source code in <code>src/pyelq/meteorology/meteorology.py</code> <pre><code>def set_plot_polar_scatter_layout(max_concentration: float, fig: go.Figure(), template: object) -&gt; go.Figure:\n    \"\"\"Helper function to set the layout of the polar scatter plot.\n\n    Helps avoid code duplication.\n\n    Args:\n        max_concentration (float): The maximum concentration value used to update radial axis range.\n        fig (go.Figure): A plotly figure onto which traces can be drawn.\n        template (object): A layout template which can be applied to the plot.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    \"\"\"\n    ticktext = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n    polar_dict = {\n        \"radialaxis\": {\"tickangle\": 0, \"range\": [0.0, 1.01 * max_concentration]},\n        \"radialaxis_angle\": 0,\n        \"angularaxis\": {\n            \"tickmode\": \"array\",\n            \"ticktext\": ticktext,\n            \"direction\": \"clockwise\",\n            \"rotation\": 90,\n            \"tickvals\": list(np.linspace(0, 360 - (360 / 8), 8)),\n        },\n    }\n\n    fig.add_annotation(\n        x=1,\n        y=1,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        align=\"left\",\n        font={\"size\": 18, \"color\": \"#000000\"},\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=\"&lt;b&gt;Radial Axis:&lt;/b&gt; Wind&lt;br&gt;speed in m/s.\",\n    )\n\n    fig.update_layout(polar=polar_dict)\n    fig.update_layout(template=template)\n    fig.update_layout(title=\"Measured Concentration against Wind Direction.\")\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/meteorology_windfield/","title":"Windfield","text":""},{"location":"pyelq/meteorology/meteorology_windfield/#meteorology-windfield","title":"Meteorology Windfield","text":"<p>Meteorology windfield module.</p> <p>Version of the meteorology class that deals with spatial wind fields and can calculate the wind field around cylindrical obstacles.</p>"},{"location":"pyelq/meteorology/meteorology_windfield/#pyelq.meteorology.meteorology_windfield.MeteorologyWindfield","title":"<code>MeteorologyWindfield</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Meteorology</code></p> <p>Represents a spatially resolved wind field based on meteorological measurements and the presence of obstacles.</p> <p>This class extends the base <code>Meteorology</code> class by providing methods to compute the local wind vector (u and v components) at every grid point, factoring in obstacle perturbations using an analytical method. It accounts for spatial rotation to align with the instantaneous wind direction at each time step.</p> <p>Attributes:</p> Name Type Description <code>static_wind_field</code> <code>Meteorology</code> <p>The static wind field used for calculations.</p> <code>site_layout</code> <code>SiteLayout</code> <p>The layout of the site, including cylinder coordinates and radii.</p> Source code in <code>src/pyelq/meteorology/meteorology_windfield.py</code> <pre><code>@dataclass\nclass MeteorologyWindfield(Meteorology):\n    \"\"\"Represents a spatially resolved wind field based on meteorological measurements and the presence of obstacles.\n\n    This class extends the base `Meteorology` class by providing methods to compute the local wind vector (u and v\n    components) at every grid point, factoring in obstacle perturbations using an analytical method. It accounts for\n    spatial rotation to align with the instantaneous wind direction at each time step.\n\n    Attributes:\n        static_wind_field (Meteorology): The static wind field used for calculations.\n        site_layout (SiteLayout): The layout of the site, including cylinder coordinates and radii.\n\n    \"\"\"\n\n    static_wind_field: Meteorology\n    site_layout: Optional[Union[SiteLayout, None]] = None\n\n    def calculate_spatial_wind_field(self, grid_coordinates: ENU, time_index: int = None):\n        \"\"\"Calculates the spatial wind field over a grid considering obstacles.\n\n        Computes the full spatial wind field from a time series stored in self.static_wind_field at grid locations\n        provided in grid_coordinates considering distortion effects due to flow around cylindrical obstacles.\n\n        The method:\n        - Rotates grid coordinates into the wind-aligned frame based on mathematical wind direction.\n        - Calculates the distorted wind field due to the presence of cylindrical obstacles.\n        - Rotates the resulting local wind field back into the original frame.\n        - Updates the object's `u_component` and `v_component` accordingly.\n        - If w_component is present in the static wind field, it is broadcasted to match the grid points.\n        - If no site layout is provided, the wind field remains undisturbed and is simply broadcasted across the grid.\n\n        Output: The method updates the following properties in place:\n        - u_component np.ndarray: (n_grid x n_time) The x-component of the wind field at the grid points.\n        - v_component np.ndarray: (n_grid x n_time) The y-component of the wind field at the grid points.\n        - w_component np.ndarray: (n_grid x n_time) The z-component of the wind field at the grid points.\n\n        Args:\n            grid_coordinates (ENU): The coordinates of the grid points.\n            time_index (int): The time index for the meteorological data.\n\n        \"\"\"\n        if time_index is None:\n            time_index = np.ones_like(self.static_wind_field.u_component).astype(bool)\n\n        u = self.static_wind_field.u_component.reshape(-1, 1)[time_index]\n        v = self.static_wind_field.v_component.reshape(-1, 1)[time_index]\n        if self.static_wind_field.w_component is not None:\n            self.w_component = np.broadcast_to(\n                self.static_wind_field.w_component[time_index].T,\n                (grid_coordinates.nof_observations, u.shape[0]),\n            )\n\n        if self.site_layout is None:\n            self.u_component = np.broadcast_to(u.T, (grid_coordinates.nof_observations, u.shape[0]))\n            self.v_component = np.broadcast_to(v.T, (grid_coordinates.nof_observations, v.shape[0]))\n            return\n        mathematical_wind_direction = np.arctan2(v, u).flatten()\n        rotation_matrix, rotated_grid, rotated_cylinders = self._rotate_coordinates(\n            grid_coordinates, mathematical_wind_direction\n        )\n        u_rot, v_rot = self._calculate_wind_field_cardinal(\n            u=u,\n            v=v,\n            grid_coordinates=grid_coordinates,\n            rotated_grid=rotated_grid,\n            rotated_cylinders=rotated_cylinders,\n        )\n        u_stacked = np.stack((u_rot, v_rot), axis=2)\n        inverse_rot = np.transpose(rotation_matrix, axes=(1, 0, 2))\n        rotated_wind = np.einsum(\"ijt,ntj-&gt; nti\", inverse_rot, u_stacked)\n        self.u_component = rotated_wind[:, :, 0]\n        self.v_component = rotated_wind[:, :, 1]\n\n    def _rotate_coordinates(\n        self, grid_coordinates: ENU, wind_direction: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Rotates the x, y coordinates based on the wind direction.\n\n        Args:\n            grid_coordinates (ENU): The coordinates to be rotated.\n            wind_direction (np.array): The wind direction in radians.\n\n        Returns:\n            rotation_matrix (np.ndarray): rotation matrix used for inverse rotation\n            rotated_grid (np.ndarray): grid_coordinates in rotated coordinate system\n            rotated_cylinders (np.ndarray): cylinder locations in rotated coordinate system\n\n        \"\"\"\n        rotation_matrix = np.array(\n            [\n                [np.cos(wind_direction), np.sin(wind_direction)],\n                [-np.sin(wind_direction), np.cos(wind_direction)],\n            ]\n        )\n        rotated_grid = np.einsum(\"ijt,nj-&gt;nit\", rotation_matrix, grid_coordinates.to_array(dim=2))\n        rotated_cylinders = np.einsum(\n            \"ijt,nj-&gt;nit\", rotation_matrix, self.site_layout.cylinder_coordinates.to_array(dim=2)\n        )\n        return rotation_matrix, rotated_grid, rotated_cylinders\n\n    def _calculate_wind_field_cardinal(\n        self,\n        u: np.ndarray,\n        v: np.ndarray,\n        grid_coordinates: ENU,\n        rotated_grid: np.ndarray,\n        rotated_cylinders: np.ndarray,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Calculates the distorted wind field components (u, v) in the wind-aligned (cardinal) frame.\n\n        The method:\n        - Determines whether each grid point is influenced by nearby cylinders based on distance and cylinder radius.\n        - If no obstacles are relevant at the evaluation height, the wind field remains undisturbed.\n        - If obstacles are present, modifies the wind field using an analytical perturbation formula based on\n            potential flow theory.\n        - Wind inside obstacles is set to zero.\n\n        If no height information is provided (e.g. in the 2-dimensional solver case), the function assumes that\n        all cylinders and input points are at the same height, and applies the mask accordingly.\n\n        Args:\n            u (np.ndarray n_time x 1): The x-component of the wind vector.\n            v (np.ndarray  n_time x 1): The y-component of the wind vector.\n            grid_coordinates (ENU): location object containing information about the finite volume solve grid points.\n            rotated_grid (np.ndarray n_grid x 2 x n_time): The grid coordinates where the wind field is to be calculated\n            in the wind-aligned frame.\n            rotated_cylinders (np.ndarray n_cylinders x 2 x n_time): The coordinates of the cylinders in the\n            wind-aligned frame.\n\n        Returns:\n            u_rot (np.ndarray): The x-component of the wind field at the grid points.\n            v_rot (np.ndarray): The y-component of the wind field at the grid points.\n\n        \"\"\"\n        diff = rotated_grid[:, np.newaxis, :, :] - rotated_cylinders[np.newaxis, :, :, :]\n        radial_distance = np.linalg.norm(diff, axis=2)\n        x_diff = diff[:, :, 0, :]\n        y_diff = diff[:, :, 1, :]\n        radius_squared = self.site_layout.cylinder_radius.T**2\n        radial_distance_sq = radial_distance**2\n        radial_distance_quad = radial_distance_sq**2\n        radial_distance_quad[radial_distance_quad == 0] = np.nan\n        radius_sq_over_r4 = radius_squared[:, :, np.newaxis] / radial_distance_quad\n\n        if grid_coordinates.up is None:\n            sum_term_x = np.einsum(\"nct, nct-&gt;nt\", radius_sq_over_r4, (y_diff**2 - x_diff**2))\n            sum_term_y = np.einsum(\"nct, nct-&gt;nt\", radius_sq_over_r4, (y_diff * x_diff))\n        else:\n            height_mask = grid_coordinates.up &lt;= self.site_layout.cylinder_coordinates.up.T\n            height_mask = height_mask.reshape(grid_coordinates.nof_observations, self.site_layout.nof_cylinders)\n            sum_term_x = np.einsum(\"nc, nct, nct-&gt;nt\", height_mask, radius_sq_over_r4, (y_diff**2 - x_diff**2))\n            sum_term_y = np.einsum(\"nc, nct, nct-&gt;nt\", height_mask, radius_sq_over_r4, (y_diff * x_diff))\n        wind_speed = np.sqrt(u**2 + v**2).T\n        u_rot = wind_speed * (1 + sum_term_x)\n        v_rot = -2 * wind_speed * sum_term_y\n        u_rot[self.site_layout.id_obstacles.flatten(), :] = 0\n        v_rot[self.site_layout.id_obstacles.flatten(), :] = 0\n        return u_rot, v_rot\n</code></pre>"},{"location":"pyelq/meteorology/meteorology_windfield/#pyelq.meteorology.meteorology_windfield.MeteorologyWindfield.calculate_spatial_wind_field","title":"<code>calculate_spatial_wind_field(grid_coordinates, time_index=None)</code>","text":"<p>Calculates the spatial wind field over a grid considering obstacles.</p> <p>Computes the full spatial wind field from a time series stored in self.static_wind_field at grid locations provided in grid_coordinates considering distortion effects due to flow around cylindrical obstacles.</p> <p>The method: - Rotates grid coordinates into the wind-aligned frame based on mathematical wind direction. - Calculates the distorted wind field due to the presence of cylindrical obstacles. - Rotates the resulting local wind field back into the original frame. - Updates the object's <code>u_component</code> and <code>v_component</code> accordingly. - If w_component is present in the static wind field, it is broadcasted to match the grid points. - If no site layout is provided, the wind field remains undisturbed and is simply broadcasted across the grid.</p> <p>Output: The method updates the following properties in place: - u_component np.ndarray: (n_grid x n_time) The x-component of the wind field at the grid points. - v_component np.ndarray: (n_grid x n_time) The y-component of the wind field at the grid points. - w_component np.ndarray: (n_grid x n_time) The z-component of the wind field at the grid points.</p> <p>Parameters:</p> Name Type Description Default <code>grid_coordinates</code> <code>ENU</code> <p>The coordinates of the grid points.</p> required <code>time_index</code> <code>int</code> <p>The time index for the meteorological data.</p> <code>None</code> Source code in <code>src/pyelq/meteorology/meteorology_windfield.py</code> <pre><code>def calculate_spatial_wind_field(self, grid_coordinates: ENU, time_index: int = None):\n    \"\"\"Calculates the spatial wind field over a grid considering obstacles.\n\n    Computes the full spatial wind field from a time series stored in self.static_wind_field at grid locations\n    provided in grid_coordinates considering distortion effects due to flow around cylindrical obstacles.\n\n    The method:\n    - Rotates grid coordinates into the wind-aligned frame based on mathematical wind direction.\n    - Calculates the distorted wind field due to the presence of cylindrical obstacles.\n    - Rotates the resulting local wind field back into the original frame.\n    - Updates the object's `u_component` and `v_component` accordingly.\n    - If w_component is present in the static wind field, it is broadcasted to match the grid points.\n    - If no site layout is provided, the wind field remains undisturbed and is simply broadcasted across the grid.\n\n    Output: The method updates the following properties in place:\n    - u_component np.ndarray: (n_grid x n_time) The x-component of the wind field at the grid points.\n    - v_component np.ndarray: (n_grid x n_time) The y-component of the wind field at the grid points.\n    - w_component np.ndarray: (n_grid x n_time) The z-component of the wind field at the grid points.\n\n    Args:\n        grid_coordinates (ENU): The coordinates of the grid points.\n        time_index (int): The time index for the meteorological data.\n\n    \"\"\"\n    if time_index is None:\n        time_index = np.ones_like(self.static_wind_field.u_component).astype(bool)\n\n    u = self.static_wind_field.u_component.reshape(-1, 1)[time_index]\n    v = self.static_wind_field.v_component.reshape(-1, 1)[time_index]\n    if self.static_wind_field.w_component is not None:\n        self.w_component = np.broadcast_to(\n            self.static_wind_field.w_component[time_index].T,\n            (grid_coordinates.nof_observations, u.shape[0]),\n        )\n\n    if self.site_layout is None:\n        self.u_component = np.broadcast_to(u.T, (grid_coordinates.nof_observations, u.shape[0]))\n        self.v_component = np.broadcast_to(v.T, (grid_coordinates.nof_observations, v.shape[0]))\n        return\n    mathematical_wind_direction = np.arctan2(v, u).flatten()\n    rotation_matrix, rotated_grid, rotated_cylinders = self._rotate_coordinates(\n        grid_coordinates, mathematical_wind_direction\n    )\n    u_rot, v_rot = self._calculate_wind_field_cardinal(\n        u=u,\n        v=v,\n        grid_coordinates=grid_coordinates,\n        rotated_grid=rotated_grid,\n        rotated_cylinders=rotated_cylinders,\n    )\n    u_stacked = np.stack((u_rot, v_rot), axis=2)\n    inverse_rot = np.transpose(rotation_matrix, axes=(1, 0, 2))\n    rotated_wind = np.einsum(\"ijt,ntj-&gt; nti\", inverse_rot, u_stacked)\n    self.u_component = rotated_wind[:, :, 0]\n    self.v_component = rotated_wind[:, :, 1]\n</code></pre>"},{"location":"pyelq/plotting/plot/","title":"Plotting","text":""},{"location":"pyelq/plotting/plot/#plot","title":"Plot","text":"<p>Plot module.</p> <p>Large module containing all the plotting code used to create various plots. Contains helper functions and the Plot class definition.</p>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot","title":"<code>Plot</code>  <code>dataclass</code>","text":"<p>Defines the plot class.</p> <p>Can be used to generate various figures from model components while storing general settings to get consistent figure appearance.</p> <p>Attributes:</p> Name Type Description <code>figure_dict</code> <code>dict</code> <p>Figure dictionary, used as storage using keys to identify the different figures.</p> <code>layout</code> <code>dict</code> <p>Layout template for plotly figures, used in all figures generated using this class instance.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>@dataclass\nclass Plot:\n    \"\"\"Defines the plot class.\n\n    Can be used to generate various figures from model components while storing general settings to get consistent\n    figure appearance.\n\n    Attributes:\n        figure_dict (dict): Figure dictionary, used as storage using keys to identify the different figures.\n        layout (dict, optional): Layout template for plotly figures, used in all figures generated using this class\n            instance.\n\n    \"\"\"\n\n    figure_dict: dict = field(default_factory=dict)\n    layout: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Using post init to set the default layout, not able to do this in attribute definition/initialization.\"\"\"\n        self.layout = {\n            \"layout\": go.Layout(\n                font={\"family\": \"Futura\", \"size\": 20},\n                title={\"x\": 0.5},\n                title_font={\"size\": 30},\n                xaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n                yaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n                legend={\n                    \"orientation\": \"v\",\n                    \"yanchor\": \"middle\",\n                    \"y\": 0.5,\n                    \"xanchor\": \"right\",\n                    \"x\": 1.2,\n                    \"font\": {\"size\": 14, \"color\": \"black\"},\n                },\n            )\n        }\n\n    def show_all(self, renderer=\"browser\"):\n        \"\"\"Show all the figures which are in the figure dictionary.\n\n        Args:\n            renderer (str, optional): Default renderer to use when showing the figures.\n\n        \"\"\"\n        for fig in self.figure_dict.values():\n            fig.show(renderer=renderer)\n\n    def plot_single_trace(self, object_to_plot: Union[Type[SlabAndSpike], SourceModel, MCMC], **kwargs: Any):\n        \"\"\"Plotting a trace of a single variable.\n\n        Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n        First it grabs all the specifics needed for the plot and then plots the trace.\n\n        Args:\n            object_to_plot (Union[Type[SlabAndSpike], SourceModel, MCMC]): The object from which to plot a variable\n            **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n                some specific plots but not applicable to all.\n\n        \"\"\"\n        plot_specifics = create_trace_specifics(object_to_plot=object_to_plot, **kwargs)\n\n        burn_in = kwargs.pop(\"burn_in\", 0)\n\n        fig = go.Figure()\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=plot_specifics[\"x_values\"],\n            y_values=plot_specifics[\"y_values\"],\n            color=plot_specifics[\"color\"],\n            name=plot_specifics[\"name\"],\n            burn_in=burn_in,\n        )\n\n        if burn_in &gt; 0:\n            fig.add_vline(\n                x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n            )\n        if isinstance(object_to_plot, SlabAndSpike) and isinstance(object_to_plot, SourceModel):\n            prior_num_sources_on = round(object_to_plot.emission_rate.shape[0] * object_to_plot.slab_probability, 2)\n\n            fig.add_hline(\n                y=prior_num_sources_on,\n                line_width=3,\n                line_dash=\"dash\",\n                line_color=\"black\",\n                annotation_text=f\"Prior sources 'on': {prior_num_sources_on}\",\n            )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.update_layout(title=plot_specifics[\"title_text\"])\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n        self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n\n    def plot_trace_per_sensor(\n        self,\n        object_to_plot: Union[ErrorModel, PerSensor, MCMC],\n        sensor_object: Union[SensorGroup, Sensor],\n        plot_type: str,\n        **kwargs: Any,\n    ):\n        \"\"\"Plotting a trace of a single variable per sensor.\n\n        Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n        First it grabs all the specifics needed for the plot and then plots the trace per sensor.\n\n        Args:\n            object_to_plot (Union[ErrorModel, PerSensor, MCMC]): The object which to plot a variable from\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n            plot_type (str): String specifying a line or box plot.\n            **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n                some specific plots but not applicable to all.\n\n        \"\"\"\n        if isinstance(sensor_object, Sensor):\n            temp = SensorGroup()\n            temp.add_sensor(sensor_object)\n            sensor_object = deepcopy(temp)\n        plot_specifics = create_plot_specifics(\n            object_to_plot=object_to_plot, sensor_object=sensor_object, plot_type=plot_type, **kwargs\n        )\n        burn_in = kwargs.pop(\"burn_in\", 0)\n\n        fig = go.Figure()\n        for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n            color_idx = sensor_idx % len(sensor_object.color_map)\n            color = sensor_object.color_map[color_idx]\n\n            if plot_specifics[\"plot_type\"] == \"line\":\n                fig = plot_single_scatter(\n                    fig=fig,\n                    x_values=plot_specifics[\"x_values\"],\n                    y_values=plot_specifics[\"y_values\"][sensor_idx, :],\n                    color=color,\n                    name=sensor_key,\n                    burn_in=burn_in,\n                )\n            elif plot_specifics[\"plot_type\"] == \"box\":\n                fig = plot_single_box(\n                    fig=fig,\n                    y_values=plot_specifics[\"y_values\"][sensor_idx, burn_in:].flatten(),\n                    color=color,\n                    name=sensor_key,\n                )\n\n        if burn_in &gt; 0 and plot_specifics[\"plot_type\"] == \"line\":\n            fig.add_vline(\n                x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n            )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.update_layout(title=plot_specifics[\"title_text\"])\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n        self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n\n    def plot_fitted_values_per_sensor(\n        self,\n        mcmc_object: MCMC,\n        sensor_object: Union[SensorGroup, Sensor],\n        background_model: TemporalBackground = None,\n        burn_in: int = 0,\n    ):\n        \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when inputted.\n\n        Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n        measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n        The created figure is stored in the figure_dict attribute.\n\n        Args:\n            mcmc_object (MCMC): MCMC object which contains the fitted values in the store attribute of the object.\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n            background_model (TemporalBackground, optional): Background model containing the estimated background.\n            burn_in (int, optional): Number of burn-in iterations to discard before calculating the quantiles\n                and median. Defaults to 0.\n\n        \"\"\"\n        if \"y\" not in mcmc_object.store:\n            raise ValueError(\"Missing fitted values ('y') in mcmc_store_object\")\n\n        if isinstance(sensor_object, Sensor):\n            temp = SensorGroup()\n            temp.add_sensor(sensor_object)\n            sensor_object = deepcopy(temp)\n\n        y_values_overall = mcmc_object.store[\"y\"]\n        dict_key = \"fitted_values\"\n        title_text = \"Observations and Predicted Model Values Against Time\"\n        x_label = \"Time\"\n        y_label = \"Concentration (ppm)\"\n        fig = go.Figure()\n\n        for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n            plot_idx = np.array(sensor_object.sensor_index == sensor_idx)\n\n            x_values = sensor_object[sensor_key].time\n            y_values = y_values_overall[plot_idx, burn_in:]\n\n            color_idx = sensor_idx % len(sensor_object.color_map)\n            color = sensor_object.color_map[color_idx]\n\n            fig = plot_quantiles_from_array(\n                fig=fig, x_values=x_values, y_values=y_values, quantiles=[10, 90], color=color, name=sensor_key\n            )\n\n        if isinstance(background_model, TemporalBackground):\n            fig = plot_quantiles_from_array(\n                fig=fig,\n                x_values=background_model.time,\n                y_values=background_model.bg,\n                quantiles=[10, 90],\n                color=\"rgb(186, 186, 186)\",\n                name=\"Background\",\n            )\n\n            fig.for_each_trace(\n                lambda trace: (\n                    trace.update(showlegend=True, name=\"Background\") if trace.name == \"Median for Background\" else ()\n                ),\n            )\n\n        fig = sensor_object.plot_timeseries(fig=fig, color_map=sensor_object.color_map, mode=\"markers\")\n\n        fig.add_annotation(\n            x=1,\n            y=1.1,\n            yref=\"paper\",\n            xref=\"paper\",\n            xanchor=\"left\",\n            yanchor=\"top\",\n            font={\"size\": 12, \"color\": \"#000000\"},\n            align=\"left\",\n            showarrow=False,\n            borderwidth=2,\n            borderpad=10,\n            bgcolor=\"#ffffff\",\n            bordercolor=\"#000000\",\n            opacity=0.8,\n            text=(\n                \"&lt;b&gt;Point&lt;/b&gt;: Real observation&lt;br&gt;&lt;b&gt;Line&lt;/b&gt;: Predicted Value&lt;br&gt;&lt;b&gt;Shading&lt;/b&gt;: \" + \"Quantiles 10-90\"\n            ),\n        )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.update_layout(title=title_text)\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n\n        self.figure_dict[dict_key] = fig\n\n    def plot_emission_rate_estimates(self, source_model_object, y_axis_type=\"linear\", **kwargs: Any):\n        \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n        Based on the inputs it plots the results of the mcmc analysis, being the estimated emission rate values for\n        each source location together with the total emissions estimate, which is the sum over all source locations.\n\n        The created figure is stored in the figure_dict attribute.\n\n        After the loop over all sources we add an empty trace to have the legend entry and desired legend group\n        behaviour.\n\n        Args:\n            source_model_object (SourceModel): Source model object which contains the estimated emission rate estimates.\n            y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n            **kwargs (Any): Additional key word arguments, e.g. burn_in, dict_key, used in some specific plots but not\n                applicable to all.\n\n        \"\"\"\n        total_emissions = np.nansum(source_model_object.emission_rate, axis=0)\n        x_values = np.array(range(total_emissions.size))\n\n        burn_in = kwargs.pop(\"burn_in\", 0)\n\n        dict_key = \"estimated_values_plot\"\n        title_text = \"Estimated Values of Sources With Respect to MCMC Iterations\"\n        x_label = MCMC_ITERATION_NUMBER_LITERAL\n        y_label = \"Estimated Emission&lt;br&gt;Values (kg/hr)\"\n\n        fig = go.Figure()\n\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=x_values,\n            y_values=total_emissions,\n            color=\"rgb(239, 85, 59)\",\n            name=\"Total Site Emissions\",\n            burn_in=burn_in,\n            show_legend=True,\n        )\n\n        for source_idx in range(source_model_object.emission_rate.shape[0]):\n            y_values = source_model_object.emission_rate[source_idx, :]\n            if source_model_object.individual_source_labels[source_idx] is not None:\n                source_label = source_model_object.individual_source_labels[source_idx]\n            else:\n                source_label = f\"Source {source_idx}\"\n\n            fig = plot_single_scatter(\n                fig=fig,\n                x_values=x_values,\n                y_values=y_values,\n                color=RGB_LIGHT_BLUE,\n                name=source_label,\n                burn_in=burn_in,\n                show_legend=False,\n                legend_group=\"Source traces\",\n            )\n\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=np.array([None]),\n            y_values=np.array([None]),\n            color=RGB_LIGHT_BLUE,\n            name=\"Source traces\",\n            burn_in=0,\n            show_legend=True,\n        )\n\n        if burn_in &gt; 0:\n            fig.add_vline(\n                x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n            )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.add_annotation(\n            x=1.05,\n            y=1.05,\n            yref=\"paper\",\n            xref=\"paper\",\n            xanchor=\"left\",\n            yanchor=\"top\",\n            align=\"left\",\n            font={\"size\": 12, \"color\": \"#000000\"},\n            showarrow=False,\n            borderwidth=2,\n            borderpad=10,\n            bgcolor=\"#ffffff\",\n            bordercolor=\"#000000\",\n            opacity=0.8,\n            text=(\n                \"&lt;b&gt;Total Site Emissions&lt;/b&gt; are&lt;br&gt;the sum of all estimated&lt;br&gt;\"\n                \"emission rates at a given&lt;br&gt;iteration number.\"\n            ),\n        )\n\n        fig.update_layout(title=title_text)\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n        if y_axis_type == \"log\":\n            fig.update_yaxes(type=\"log\")\n            dict_key = \"log_estimated_values_plot\"\n        elif y_axis_type != \"linear\":\n            raise ValueError(f\"Only linear or log y axis type is allowed, {y_axis_type} was currently specified.\")\n\n        self.figure_dict[dict_key] = fig\n\n    def create_empty_map_figure(self, dict_key: str = \"map_plot\") -&gt; None:\n        \"\"\"Creating an empty map figure to use when you want to add additional traces on a map.\n\n        Args:\n            dict_key (str, optional): String key for figure dictionary\n\n        \"\"\"\n        self.figure_dict[dict_key] = go.Figure(\n            data=go.Scattermap(),\n            layout={\n                \"map_style\": \"carto-positron\",\n                \"map_center_lat\": 0,\n                \"map_center_lon\": 0,\n                \"map_zoom\": 0,\n            },\n        )\n\n    def plot_values_on_map(\n        self, dict_key: str, coordinates: LLA, values: np.ndarray, aggregate_function: Callable = np.sum, **kwargs: Any\n    ):\n        \"\"\"Plot values on a map based on coordinates.\n\n        Args:\n            dict_key (str): Sting key to use in the figure dictionary\n            coordinates (LLA): LLA coordinates to use in plotting the values on the map\n            values (np.ndarray): Numpy array of values consistent with coordinates to plot on the map\n            aggregate_function (Callable, optional): Function which to apply on the data in each hexagonal bin to\n                aggregate the data and visualise the result.\n            **kwargs (Any): Additional keyword arguments for plotting behaviour (opacity, map_color_scale, num_hexagons,\n                show_positions)\n\n        \"\"\"\n        map_color_scale = kwargs.pop(\"map_color_scale\", \"YlOrRd\")\n        num_hexagons = kwargs.pop(\"num_hexagons\", None)\n        opacity = kwargs.pop(\"opacity\", 0.8)\n        show_positions = kwargs.pop(\"show_positions\", False)\n\n        latitude_check, _ = is_regularly_spaced(coordinates.latitude)\n        longitude_check, _ = is_regularly_spaced(coordinates.longitude)\n        if latitude_check and longitude_check:\n            self.create_empty_map_figure(dict_key=dict_key)\n            trace = plot_regular_grid(\n                coordinates=coordinates,\n                values=values,\n                opacity=opacity,\n                map_color_scale=map_color_scale,\n                tolerance=1e-7,\n                unit=\"\",\n            )\n            self.figure_dict[dict_key].add_trace(trace)\n        else:\n            fig = plot_hexagonal_grid(\n                coordinates=coordinates,\n                values=values,\n                opacity=opacity,\n                map_color_scale=map_color_scale,\n                num_hexagons=num_hexagons,\n                show_positions=show_positions,\n                aggregate_function=aggregate_function,\n            )\n            fig.update_layout(map_style=\"carto-positron\")\n            self.figure_dict[dict_key] = fig\n\n        center_longitude = np.mean(coordinates.longitude)\n        center_latitude = np.mean(coordinates.latitude)\n        self.figure_dict[dict_key].update_layout(\n            map={\"zoom\": 10, \"center\": {\"lon\": center_longitude, \"lat\": center_latitude}}\n        )\n\n        if self.layout is not None:\n            self.figure_dict[dict_key].update_layout(template=self.layout)\n\n    def plot_quantification_results_on_map(\n        self,\n        model_object: \"ELQModel\",\n        source_model_to_plot_key: str = None,\n        bin_size_x: float = 1,\n        bin_size_y: float = 1,\n        normalized_count_limit: float = 0.005,\n        burn_in: int = 0,\n        show_summary_results: bool = True,\n        show_fixed_source_locations: bool = True,\n    ):\n        \"\"\"Function to create a map with the quantification results of the model object.\n\n        This function takes the \"SourceModel\" object and calculates the statistics for the quantification results.\n        It then populates the figure dictionary with three different maps showing the normalized count,\n        median emission rate and the inter-quartile range of the emission rate estimates.\n\n        Args:\n            model_object (ELQModel): ELQModel object containing the quantification results\n            source_model_to_plot_key (str, optional): Key to use in the model_object.components dictionary to access\n              the SourceModel object. If None, defaults to \"sources_combined\".\n            bin_size_x (float, optional): Size of the bins in the x-direction. Defaults to 1.\n            bin_size_y (float, optional): Size of the bins in the y-direction. Defaults to 1.\n            normalized_count_limit (float, optional): Limit for the normalized count to show on the map.\n                Defaults to 0.005.\n            burn_in (int, optional): Number of burn-in iterations to discard before calculating the statistics.\n                Defaults to 0.\n            show_summary_results (bool, optional): Flag to show the summary results on the map. Defaults to True.\n            show_fixed_source_locations (bool, optional): Flag to show the fixed sources location when present in one\n                of the sourcemaps. Defaults to True.\n\n        \"\"\"\n        if source_model_to_plot_key is None:\n            source_model_to_plot_key = \"sources_combined\"\n\n        source_model = model_object.components[source_model_to_plot_key]\n        sensor_object = model_object.sensor_object\n\n        source_locations = source_model.all_source_locations\n        emission_rates = source_model.emission_rate\n\n        ref_latitude = source_locations.ref_latitude\n        ref_longitude = source_locations.ref_longitude\n        ref_altitude = source_locations.ref_altitude\n\n        datetime_min_string = sensor_object.time.min().strftime(\"%d-%b-%Y, %H:%M:%S\")\n        datetime_max_string = sensor_object.time.max().strftime(\"%d-%b-%Y, %H:%M:%S\")\n\n        result_weighted, _, normalized_count, count_boolean, enu_points, summary_result = (\n            calculate_rectangular_statistics(\n                emission_rates=emission_rates,\n                source_locations=source_locations,\n                bin_size_x=bin_size_x,\n                bin_size_y=bin_size_y,\n                burn_in=burn_in,\n                normalized_count_limit=normalized_count_limit,\n            )\n        )\n\n        polygons = create_lla_polygons_from_xy_points(\n            points_array=enu_points,\n            ref_latitude=ref_latitude,\n            ref_longitude=ref_longitude,\n            ref_altitude=ref_altitude,\n            boolean_mask=count_boolean,\n        )\n\n        if show_summary_results:\n            summary_trace = self.create_summary_trace(summary_result=summary_result)\n\n        self.create_empty_map_figure(dict_key=\"count_map\")\n        trace = plot_polygons_on_map(\n            polygons=polygons,\n            values=normalized_count[count_boolean].flatten(),\n            opacity=0.8,\n            name=\"normalized_count\",\n            colorbar={\"title\": \"Normalized Count\", \"orientation\": \"h\"},\n            map_color_scale=\"Bluered\",\n        )\n        self.figure_dict[\"count_map\"].add_trace(trace)\n        self.figure_dict[\"count_map\"].update_layout(\n            map_style=\"carto-positron\",\n            map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n            title=f\"Source location probability \"\n            f\"(&gt;={normalized_count_limit}) for \"\n            f\"{datetime_min_string} to {datetime_max_string}\",\n            font_family=\"Futura\",\n            font_size=15,\n        )\n        sensor_object.plot_sensor_location(self.figure_dict[\"count_map\"])\n        self.figure_dict[\"count_map\"].update_traces(showlegend=False)\n\n        adjusted_result_weights = result_weighted.copy()\n        adjusted_result_weights[adjusted_result_weights == 0] = np.nan\n\n        median_of_all_emissions = np.nanmedian(adjusted_result_weights, axis=2)\n\n        self.create_empty_map_figure(dict_key=\"median_map\")\n\n        trace = plot_polygons_on_map(\n            polygons=polygons,\n            values=median_of_all_emissions[count_boolean].flatten(),\n            opacity=0.8,\n            name=\"median_emission\",\n            colorbar={\"title\": \"Median Emission\", \"orientation\": \"h\"},\n            map_color_scale=\"Bluered\",\n        )\n        self.figure_dict[\"median_map\"].add_trace(trace)\n        self.figure_dict[\"median_map\"].update_layout(\n            map_style=\"carto-positron\",\n            map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n            title=f\"Median emission rate estimate for {datetime_min_string} to {datetime_max_string}\",\n            font_family=\"Futura\",\n            font_size=15,\n        )\n        sensor_object.plot_sensor_location(self.figure_dict[\"median_map\"])\n        self.figure_dict[\"median_map\"].update_traces(showlegend=False)\n\n        iqr_of_all_emissions = np.nanquantile(a=adjusted_result_weights, q=0.75, axis=2) - np.nanquantile(\n            a=adjusted_result_weights, q=0.25, axis=2\n        )\n        self.create_empty_map_figure(dict_key=\"iqr_map\")\n\n        trace = plot_polygons_on_map(\n            polygons=polygons,\n            values=iqr_of_all_emissions[count_boolean].flatten(),\n            opacity=0.8,\n            name=\"iqr_emission\",\n            colorbar={\"title\": \"IQR\", \"orientation\": \"h\"},\n            map_color_scale=\"Bluered\",\n        )\n        self.figure_dict[\"iqr_map\"].add_trace(trace)\n        self.figure_dict[\"iqr_map\"].update_layout(\n            map_style=\"carto-positron\",\n            map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n            title=f\"Inter Quartile range (25%-75%) of emission rate \"\n            f\"estimate for {datetime_min_string} to {datetime_max_string}\",\n            font_family=\"Futura\",\n            font_size=15,\n        )\n        sensor_object.plot_sensor_location(self.figure_dict[\"iqr_map\"])\n        self.figure_dict[\"iqr_map\"].update_traces(showlegend=False)\n\n        if show_fixed_source_locations:\n            for key, _ in model_object.components.items():\n                if bool(re.search(\"fixed\", key)):\n                    source_model_fixed = model_object.components[key]\n                    source_locations_fixed = source_model_fixed.all_source_locations\n                    source_location_fixed_lla = source_locations_fixed.to_lla()\n                    sources_lat = source_location_fixed_lla.latitude[:, 0]\n                    sources_lon = source_location_fixed_lla.longitude[:, 0]\n                    fixed_source_location_trace = go.Scattermap(\n                        mode=\"markers\",\n                        lon=sources_lon,\n                        lat=sources_lat,\n                        name=f\"Fixed source locations, {key}\",\n                        marker={\"size\": 10, \"opacity\": 0.8},\n                    )\n                    self.figure_dict[\"count_map\"].add_trace(fixed_source_location_trace)\n                    self.figure_dict[\"median_map\"].add_trace(fixed_source_location_trace)\n                    self.figure_dict[\"iqr_map\"].add_trace(fixed_source_location_trace)\n\n        if show_summary_results:\n            self.figure_dict[\"count_map\"].add_trace(summary_trace)\n            self.figure_dict[\"count_map\"].update_traces(showlegend=True)\n            self.figure_dict[\"median_map\"].add_trace(summary_trace)\n            self.figure_dict[\"median_map\"].update_traces(showlegend=True)\n            self.figure_dict[\"iqr_map\"].add_trace(summary_trace)\n            self.figure_dict[\"iqr_map\"].update_traces(showlegend=True)\n\n    def plot_coverage(\n        self,\n        coordinates: LLA,\n        couplings: np.ndarray,\n        threshold_function: Callable = np.max,\n        coverage_threshold: float = 6,\n        opacity: float = 0.8,\n        map_color_scale=\"jet\",\n    ):\n        \"\"\"Creates a coverage plot using the coverage function from Gaussian Plume.\n\n        Args:\n            coordinates (LLA object): A LLA coordinate object containing a set of locations.\n            couplings (np.array): The calculated values of coupling (The 'A matrix') for a set of wind data.\n            threshold_function (Callable, optional): Callable function which returns some single value that defines the\n                                         maximum or 'threshold' coupling. Examples: np.quantile(q=0.9),\n                                         np.max, np.mean. Defaults to np.max.\n            coverage_threshold (float, optional): The threshold value of the estimated emission rate which is\n                                                  considered to be within the coverage. Defaults to 6 kg/hr.\n            opacity (float): The opacity of the grid cells when they are plotted.\n            map_color_scale (str): The string which defines which plotly colour scale should be used when plotting\n                                   the values.\n\n        \"\"\"\n        coverage_values = GaussianPlume(source_map=None).compute_coverage(\n            couplings=couplings, threshold_function=threshold_function, coverage_threshold=coverage_threshold\n        )\n        self.plot_values_on_map(\n            dict_key=\"coverage_map\",\n            coordinates=coordinates,\n            values=coverage_values,\n            aggregate_function=np.max,\n            opacity=opacity,\n            map_color_scale=map_color_scale,\n        )\n\n    @staticmethod\n    def create_summary_trace(\n        summary_result: pd.DataFrame,\n    ) -&gt; go.Scattermap:\n        \"\"\"Helper function to create the summary information to plot on top of map type plots.\n\n        We use the summary result calculated through the support functions module to create a trace which contains\n        the summary information for each source location.\n\n        Args:\n            summary_result (pd.DataFrame): DataFrame containing the summary information for each source location.\n\n        Returns:\n            summary_trace (go.Scattermap): Trace with summary information to plot on top of map type plots.\n\n        \"\"\"\n        summary_text_values = [\n            f\"&lt;b&gt;Source ID&lt;/b&gt;: {value}&lt;br&gt;\"\n            f\"&lt;b&gt;(Lon, Lat, Alt)&lt;/b&gt; ([deg], [deg], [m]):&lt;br&gt;\"\n            f\"({summary_result.longitude[value]:.7f}, \"\n            f\"{summary_result.latitude[value]:.7f}, {summary_result.altitude[value]:.3f})&lt;br&gt;\"\n            f\"&lt;b&gt;Height&lt;/b&gt;: {summary_result.height[value]:.3f} [m]&lt;br&gt;\"\n            f\"&lt;b&gt;Median emission rate&lt;/b&gt;: {summary_result.median_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;2.5% quantile&lt;/b&gt;: {summary_result.quantile_025[value]:.3f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;97.5% quantile&lt;/b&gt;: {summary_result.quantile_975[value]:.3f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;IQR&lt;/b&gt;: {summary_result.iqr_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;Blob present during&lt;/b&gt;: \"\n            f\"{summary_result.absolute_count_iterations[value]:.0f} iterations&lt;br&gt;\"\n            f\"&lt;b&gt;Blob likelihood&lt;/b&gt;: {summary_result.blob_likelihood[value]:.5f}&lt;br&gt;\"\n            for value in summary_result.index\n        ]\n\n        summary_trace = go.Scattermap(\n            lat=summary_result.latitude,\n            lon=summary_result.longitude,\n            mode=\"markers\",\n            marker=go.scattermap.Marker(size=14, color=\"black\"),\n            text=summary_text_values,\n            name=\"Summary\",\n            hoverinfo=\"text\",\n        )\n\n        return summary_trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Using post init to set the default layout, not able to do this in attribute definition/initialization.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Using post init to set the default layout, not able to do this in attribute definition/initialization.\"\"\"\n    self.layout = {\n        \"layout\": go.Layout(\n            font={\"family\": \"Futura\", \"size\": 20},\n            title={\"x\": 0.5},\n            title_font={\"size\": 30},\n            xaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n            yaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n            legend={\n                \"orientation\": \"v\",\n                \"yanchor\": \"middle\",\n                \"y\": 0.5,\n                \"xanchor\": \"right\",\n                \"x\": 1.2,\n                \"font\": {\"size\": 14, \"color\": \"black\"},\n            },\n        )\n    }\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.show_all","title":"<code>show_all(renderer='browser')</code>","text":"<p>Show all the figures which are in the figure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>renderer</code> <code>str</code> <p>Default renderer to use when showing the figures.</p> <code>'browser'</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def show_all(self, renderer=\"browser\"):\n    \"\"\"Show all the figures which are in the figure dictionary.\n\n    Args:\n        renderer (str, optional): Default renderer to use when showing the figures.\n\n    \"\"\"\n    for fig in self.figure_dict.values():\n        fig.show(renderer=renderer)\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_single_trace","title":"<code>plot_single_trace(object_to_plot, **kwargs)</code>","text":"<p>Plotting a trace of a single variable.</p> <p>Depending on the object to plot it creates a figure which is stored in the figure_dict attribute. First it grabs all the specifics needed for the plot and then plots the trace.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[Type[SlabAndSpike], SourceModel, MCMC]</code> <p>The object from which to plot a variable</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_single_trace(self, object_to_plot: Union[Type[SlabAndSpike], SourceModel, MCMC], **kwargs: Any):\n    \"\"\"Plotting a trace of a single variable.\n\n    Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n    First it grabs all the specifics needed for the plot and then plots the trace.\n\n    Args:\n        object_to_plot (Union[Type[SlabAndSpike], SourceModel, MCMC]): The object from which to plot a variable\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n            some specific plots but not applicable to all.\n\n    \"\"\"\n    plot_specifics = create_trace_specifics(object_to_plot=object_to_plot, **kwargs)\n\n    burn_in = kwargs.pop(\"burn_in\", 0)\n\n    fig = go.Figure()\n    fig = plot_single_scatter(\n        fig=fig,\n        x_values=plot_specifics[\"x_values\"],\n        y_values=plot_specifics[\"y_values\"],\n        color=plot_specifics[\"color\"],\n        name=plot_specifics[\"name\"],\n        burn_in=burn_in,\n    )\n\n    if burn_in &gt; 0:\n        fig.add_vline(\n            x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n        )\n    if isinstance(object_to_plot, SlabAndSpike) and isinstance(object_to_plot, SourceModel):\n        prior_num_sources_on = round(object_to_plot.emission_rate.shape[0] * object_to_plot.slab_probability, 2)\n\n        fig.add_hline(\n            y=prior_num_sources_on,\n            line_width=3,\n            line_dash=\"dash\",\n            line_color=\"black\",\n            annotation_text=f\"Prior sources 'on': {prior_num_sources_on}\",\n        )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.update_layout(title=plot_specifics[\"title_text\"])\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n    self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_trace_per_sensor","title":"<code>plot_trace_per_sensor(object_to_plot, sensor_object, plot_type, **kwargs)</code>","text":"<p>Plotting a trace of a single variable per sensor.</p> <p>Depending on the object to plot it creates a figure which is stored in the figure_dict attribute. First it grabs all the specifics needed for the plot and then plots the trace per sensor.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[ErrorModel, PerSensor, MCMC]</code> <p>The object which to plot a variable from</p> required <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the object_to_plot</p> required <code>plot_type</code> <code>str</code> <p>String specifying a line or box plot.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_trace_per_sensor(\n    self,\n    object_to_plot: Union[ErrorModel, PerSensor, MCMC],\n    sensor_object: Union[SensorGroup, Sensor],\n    plot_type: str,\n    **kwargs: Any,\n):\n    \"\"\"Plotting a trace of a single variable per sensor.\n\n    Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n    First it grabs all the specifics needed for the plot and then plots the trace per sensor.\n\n    Args:\n        object_to_plot (Union[ErrorModel, PerSensor, MCMC]): The object which to plot a variable from\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n        plot_type (str): String specifying a line or box plot.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n            some specific plots but not applicable to all.\n\n    \"\"\"\n    if isinstance(sensor_object, Sensor):\n        temp = SensorGroup()\n        temp.add_sensor(sensor_object)\n        sensor_object = deepcopy(temp)\n    plot_specifics = create_plot_specifics(\n        object_to_plot=object_to_plot, sensor_object=sensor_object, plot_type=plot_type, **kwargs\n    )\n    burn_in = kwargs.pop(\"burn_in\", 0)\n\n    fig = go.Figure()\n    for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n        color_idx = sensor_idx % len(sensor_object.color_map)\n        color = sensor_object.color_map[color_idx]\n\n        if plot_specifics[\"plot_type\"] == \"line\":\n            fig = plot_single_scatter(\n                fig=fig,\n                x_values=plot_specifics[\"x_values\"],\n                y_values=plot_specifics[\"y_values\"][sensor_idx, :],\n                color=color,\n                name=sensor_key,\n                burn_in=burn_in,\n            )\n        elif plot_specifics[\"plot_type\"] == \"box\":\n            fig = plot_single_box(\n                fig=fig,\n                y_values=plot_specifics[\"y_values\"][sensor_idx, burn_in:].flatten(),\n                color=color,\n                name=sensor_key,\n            )\n\n    if burn_in &gt; 0 and plot_specifics[\"plot_type\"] == \"line\":\n        fig.add_vline(\n            x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n        )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.update_layout(title=plot_specifics[\"title_text\"])\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n    self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_fitted_values_per_sensor","title":"<code>plot_fitted_values_per_sensor(mcmc_object, sensor_object, background_model=None, burn_in=0)</code>","text":"<p>Plot the fitted values from the mcmc object against time, also shows the estimated background when inputted.</p> <p>Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.</p> <p>The created figure is stored in the figure_dict attribute.</p> <p>Parameters:</p> Name Type Description Default <code>mcmc_object</code> <code>MCMC</code> <p>MCMC object which contains the fitted values in the store attribute of the object.</p> required <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the object_to_plot</p> required <code>background_model</code> <code>TemporalBackground</code> <p>Background model containing the estimated background.</p> <code>None</code> <code>burn_in</code> <code>int</code> <p>Number of burn-in iterations to discard before calculating the quantiles and median. Defaults to 0.</p> <code>0</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_fitted_values_per_sensor(\n    self,\n    mcmc_object: MCMC,\n    sensor_object: Union[SensorGroup, Sensor],\n    background_model: TemporalBackground = None,\n    burn_in: int = 0,\n):\n    \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when inputted.\n\n    Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n    measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n    The created figure is stored in the figure_dict attribute.\n\n    Args:\n        mcmc_object (MCMC): MCMC object which contains the fitted values in the store attribute of the object.\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n        background_model (TemporalBackground, optional): Background model containing the estimated background.\n        burn_in (int, optional): Number of burn-in iterations to discard before calculating the quantiles\n            and median. Defaults to 0.\n\n    \"\"\"\n    if \"y\" not in mcmc_object.store:\n        raise ValueError(\"Missing fitted values ('y') in mcmc_store_object\")\n\n    if isinstance(sensor_object, Sensor):\n        temp = SensorGroup()\n        temp.add_sensor(sensor_object)\n        sensor_object = deepcopy(temp)\n\n    y_values_overall = mcmc_object.store[\"y\"]\n    dict_key = \"fitted_values\"\n    title_text = \"Observations and Predicted Model Values Against Time\"\n    x_label = \"Time\"\n    y_label = \"Concentration (ppm)\"\n    fig = go.Figure()\n\n    for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n        plot_idx = np.array(sensor_object.sensor_index == sensor_idx)\n\n        x_values = sensor_object[sensor_key].time\n        y_values = y_values_overall[plot_idx, burn_in:]\n\n        color_idx = sensor_idx % len(sensor_object.color_map)\n        color = sensor_object.color_map[color_idx]\n\n        fig = plot_quantiles_from_array(\n            fig=fig, x_values=x_values, y_values=y_values, quantiles=[10, 90], color=color, name=sensor_key\n        )\n\n    if isinstance(background_model, TemporalBackground):\n        fig = plot_quantiles_from_array(\n            fig=fig,\n            x_values=background_model.time,\n            y_values=background_model.bg,\n            quantiles=[10, 90],\n            color=\"rgb(186, 186, 186)\",\n            name=\"Background\",\n        )\n\n        fig.for_each_trace(\n            lambda trace: (\n                trace.update(showlegend=True, name=\"Background\") if trace.name == \"Median for Background\" else ()\n            ),\n        )\n\n    fig = sensor_object.plot_timeseries(fig=fig, color_map=sensor_object.color_map, mode=\"markers\")\n\n    fig.add_annotation(\n        x=1,\n        y=1.1,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"left\",\n        yanchor=\"top\",\n        font={\"size\": 12, \"color\": \"#000000\"},\n        align=\"left\",\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=(\n            \"&lt;b&gt;Point&lt;/b&gt;: Real observation&lt;br&gt;&lt;b&gt;Line&lt;/b&gt;: Predicted Value&lt;br&gt;&lt;b&gt;Shading&lt;/b&gt;: \" + \"Quantiles 10-90\"\n        ),\n    )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.update_layout(title=title_text)\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n\n    self.figure_dict[dict_key] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_emission_rate_estimates","title":"<code>plot_emission_rate_estimates(source_model_object, y_axis_type='linear', **kwargs)</code>","text":"<p>Plot the emission rate estimates source model object against MCMC iteration.</p> <p>Based on the inputs it plots the results of the mcmc analysis, being the estimated emission rate values for each source location together with the total emissions estimate, which is the sum over all source locations.</p> <p>The created figure is stored in the figure_dict attribute.</p> <p>After the loop over all sources we add an empty trace to have the legend entry and desired legend group behaviour.</p> <p>Parameters:</p> Name Type Description Default <code>source_model_object</code> <code>SourceModel</code> <p>Source model object which contains the estimated emission rate estimates.</p> required <code>y_axis_type</code> <code>str</code> <p>String to indicate whether the y-axis should be linear of log scale.</p> <code>'linear'</code> <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_emission_rate_estimates(self, source_model_object, y_axis_type=\"linear\", **kwargs: Any):\n    \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n    Based on the inputs it plots the results of the mcmc analysis, being the estimated emission rate values for\n    each source location together with the total emissions estimate, which is the sum over all source locations.\n\n    The created figure is stored in the figure_dict attribute.\n\n    After the loop over all sources we add an empty trace to have the legend entry and desired legend group\n    behaviour.\n\n    Args:\n        source_model_object (SourceModel): Source model object which contains the estimated emission rate estimates.\n        y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, dict_key, used in some specific plots but not\n            applicable to all.\n\n    \"\"\"\n    total_emissions = np.nansum(source_model_object.emission_rate, axis=0)\n    x_values = np.array(range(total_emissions.size))\n\n    burn_in = kwargs.pop(\"burn_in\", 0)\n\n    dict_key = \"estimated_values_plot\"\n    title_text = \"Estimated Values of Sources With Respect to MCMC Iterations\"\n    x_label = MCMC_ITERATION_NUMBER_LITERAL\n    y_label = \"Estimated Emission&lt;br&gt;Values (kg/hr)\"\n\n    fig = go.Figure()\n\n    fig = plot_single_scatter(\n        fig=fig,\n        x_values=x_values,\n        y_values=total_emissions,\n        color=\"rgb(239, 85, 59)\",\n        name=\"Total Site Emissions\",\n        burn_in=burn_in,\n        show_legend=True,\n    )\n\n    for source_idx in range(source_model_object.emission_rate.shape[0]):\n        y_values = source_model_object.emission_rate[source_idx, :]\n        if source_model_object.individual_source_labels[source_idx] is not None:\n            source_label = source_model_object.individual_source_labels[source_idx]\n        else:\n            source_label = f\"Source {source_idx}\"\n\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=x_values,\n            y_values=y_values,\n            color=RGB_LIGHT_BLUE,\n            name=source_label,\n            burn_in=burn_in,\n            show_legend=False,\n            legend_group=\"Source traces\",\n        )\n\n    fig = plot_single_scatter(\n        fig=fig,\n        x_values=np.array([None]),\n        y_values=np.array([None]),\n        color=RGB_LIGHT_BLUE,\n        name=\"Source traces\",\n        burn_in=0,\n        show_legend=True,\n    )\n\n    if burn_in &gt; 0:\n        fig.add_vline(\n            x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n        )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.add_annotation(\n        x=1.05,\n        y=1.05,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"left\",\n        yanchor=\"top\",\n        align=\"left\",\n        font={\"size\": 12, \"color\": \"#000000\"},\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=(\n            \"&lt;b&gt;Total Site Emissions&lt;/b&gt; are&lt;br&gt;the sum of all estimated&lt;br&gt;\"\n            \"emission rates at a given&lt;br&gt;iteration number.\"\n        ),\n    )\n\n    fig.update_layout(title=title_text)\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n    if y_axis_type == \"log\":\n        fig.update_yaxes(type=\"log\")\n        dict_key = \"log_estimated_values_plot\"\n    elif y_axis_type != \"linear\":\n        raise ValueError(f\"Only linear or log y axis type is allowed, {y_axis_type} was currently specified.\")\n\n    self.figure_dict[dict_key] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.create_empty_map_figure","title":"<code>create_empty_map_figure(dict_key='map_plot')</code>","text":"<p>Creating an empty map figure to use when you want to add additional traces on a map.</p> <p>Parameters:</p> Name Type Description Default <code>dict_key</code> <code>str</code> <p>String key for figure dictionary</p> <code>'map_plot'</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def create_empty_map_figure(self, dict_key: str = \"map_plot\") -&gt; None:\n    \"\"\"Creating an empty map figure to use when you want to add additional traces on a map.\n\n    Args:\n        dict_key (str, optional): String key for figure dictionary\n\n    \"\"\"\n    self.figure_dict[dict_key] = go.Figure(\n        data=go.Scattermap(),\n        layout={\n            \"map_style\": \"carto-positron\",\n            \"map_center_lat\": 0,\n            \"map_center_lon\": 0,\n            \"map_zoom\": 0,\n        },\n    )\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_values_on_map","title":"<code>plot_values_on_map(dict_key, coordinates, values, aggregate_function=np.sum, **kwargs)</code>","text":"<p>Plot values on a map based on coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>dict_key</code> <code>str</code> <p>Sting key to use in the figure dictionary</p> required <code>coordinates</code> <code>LLA</code> <p>LLA coordinates to use in plotting the values on the map</p> required <code>values</code> <code>ndarray</code> <p>Numpy array of values consistent with coordinates to plot on the map</p> required <code>aggregate_function</code> <code>Callable</code> <p>Function which to apply on the data in each hexagonal bin to aggregate the data and visualise the result.</p> <code>sum</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plotting behaviour (opacity, map_color_scale, num_hexagons, show_positions)</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_values_on_map(\n    self, dict_key: str, coordinates: LLA, values: np.ndarray, aggregate_function: Callable = np.sum, **kwargs: Any\n):\n    \"\"\"Plot values on a map based on coordinates.\n\n    Args:\n        dict_key (str): Sting key to use in the figure dictionary\n        coordinates (LLA): LLA coordinates to use in plotting the values on the map\n        values (np.ndarray): Numpy array of values consistent with coordinates to plot on the map\n        aggregate_function (Callable, optional): Function which to apply on the data in each hexagonal bin to\n            aggregate the data and visualise the result.\n        **kwargs (Any): Additional keyword arguments for plotting behaviour (opacity, map_color_scale, num_hexagons,\n            show_positions)\n\n    \"\"\"\n    map_color_scale = kwargs.pop(\"map_color_scale\", \"YlOrRd\")\n    num_hexagons = kwargs.pop(\"num_hexagons\", None)\n    opacity = kwargs.pop(\"opacity\", 0.8)\n    show_positions = kwargs.pop(\"show_positions\", False)\n\n    latitude_check, _ = is_regularly_spaced(coordinates.latitude)\n    longitude_check, _ = is_regularly_spaced(coordinates.longitude)\n    if latitude_check and longitude_check:\n        self.create_empty_map_figure(dict_key=dict_key)\n        trace = plot_regular_grid(\n            coordinates=coordinates,\n            values=values,\n            opacity=opacity,\n            map_color_scale=map_color_scale,\n            tolerance=1e-7,\n            unit=\"\",\n        )\n        self.figure_dict[dict_key].add_trace(trace)\n    else:\n        fig = plot_hexagonal_grid(\n            coordinates=coordinates,\n            values=values,\n            opacity=opacity,\n            map_color_scale=map_color_scale,\n            num_hexagons=num_hexagons,\n            show_positions=show_positions,\n            aggregate_function=aggregate_function,\n        )\n        fig.update_layout(map_style=\"carto-positron\")\n        self.figure_dict[dict_key] = fig\n\n    center_longitude = np.mean(coordinates.longitude)\n    center_latitude = np.mean(coordinates.latitude)\n    self.figure_dict[dict_key].update_layout(\n        map={\"zoom\": 10, \"center\": {\"lon\": center_longitude, \"lat\": center_latitude}}\n    )\n\n    if self.layout is not None:\n        self.figure_dict[dict_key].update_layout(template=self.layout)\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_quantification_results_on_map","title":"<code>plot_quantification_results_on_map(model_object, source_model_to_plot_key=None, bin_size_x=1, bin_size_y=1, normalized_count_limit=0.005, burn_in=0, show_summary_results=True, show_fixed_source_locations=True)</code>","text":"<p>Function to create a map with the quantification results of the model object.</p> <p>This function takes the \"SourceModel\" object and calculates the statistics for the quantification results. It then populates the figure dictionary with three different maps showing the normalized count, median emission rate and the inter-quartile range of the emission rate estimates.</p> <p>Parameters:</p> Name Type Description Default <code>model_object</code> <code>ELQModel</code> <p>ELQModel object containing the quantification results</p> required <code>source_model_to_plot_key</code> <code>str</code> <p>Key to use in the model_object.components dictionary to access the SourceModel object. If None, defaults to \"sources_combined\".</p> <code>None</code> <code>bin_size_x</code> <code>float</code> <p>Size of the bins in the x-direction. Defaults to 1.</p> <code>1</code> <code>bin_size_y</code> <code>float</code> <p>Size of the bins in the y-direction. Defaults to 1.</p> <code>1</code> <code>normalized_count_limit</code> <code>float</code> <p>Limit for the normalized count to show on the map. Defaults to 0.005.</p> <code>0.005</code> <code>burn_in</code> <code>int</code> <p>Number of burn-in iterations to discard before calculating the statistics. Defaults to 0.</p> <code>0</code> <code>show_summary_results</code> <code>bool</code> <p>Flag to show the summary results on the map. Defaults to True.</p> <code>True</code> <code>show_fixed_source_locations</code> <code>bool</code> <p>Flag to show the fixed sources location when present in one of the sourcemaps. Defaults to True.</p> <code>True</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_quantification_results_on_map(\n    self,\n    model_object: \"ELQModel\",\n    source_model_to_plot_key: str = None,\n    bin_size_x: float = 1,\n    bin_size_y: float = 1,\n    normalized_count_limit: float = 0.005,\n    burn_in: int = 0,\n    show_summary_results: bool = True,\n    show_fixed_source_locations: bool = True,\n):\n    \"\"\"Function to create a map with the quantification results of the model object.\n\n    This function takes the \"SourceModel\" object and calculates the statistics for the quantification results.\n    It then populates the figure dictionary with three different maps showing the normalized count,\n    median emission rate and the inter-quartile range of the emission rate estimates.\n\n    Args:\n        model_object (ELQModel): ELQModel object containing the quantification results\n        source_model_to_plot_key (str, optional): Key to use in the model_object.components dictionary to access\n          the SourceModel object. If None, defaults to \"sources_combined\".\n        bin_size_x (float, optional): Size of the bins in the x-direction. Defaults to 1.\n        bin_size_y (float, optional): Size of the bins in the y-direction. Defaults to 1.\n        normalized_count_limit (float, optional): Limit for the normalized count to show on the map.\n            Defaults to 0.005.\n        burn_in (int, optional): Number of burn-in iterations to discard before calculating the statistics.\n            Defaults to 0.\n        show_summary_results (bool, optional): Flag to show the summary results on the map. Defaults to True.\n        show_fixed_source_locations (bool, optional): Flag to show the fixed sources location when present in one\n            of the sourcemaps. Defaults to True.\n\n    \"\"\"\n    if source_model_to_plot_key is None:\n        source_model_to_plot_key = \"sources_combined\"\n\n    source_model = model_object.components[source_model_to_plot_key]\n    sensor_object = model_object.sensor_object\n\n    source_locations = source_model.all_source_locations\n    emission_rates = source_model.emission_rate\n\n    ref_latitude = source_locations.ref_latitude\n    ref_longitude = source_locations.ref_longitude\n    ref_altitude = source_locations.ref_altitude\n\n    datetime_min_string = sensor_object.time.min().strftime(\"%d-%b-%Y, %H:%M:%S\")\n    datetime_max_string = sensor_object.time.max().strftime(\"%d-%b-%Y, %H:%M:%S\")\n\n    result_weighted, _, normalized_count, count_boolean, enu_points, summary_result = (\n        calculate_rectangular_statistics(\n            emission_rates=emission_rates,\n            source_locations=source_locations,\n            bin_size_x=bin_size_x,\n            bin_size_y=bin_size_y,\n            burn_in=burn_in,\n            normalized_count_limit=normalized_count_limit,\n        )\n    )\n\n    polygons = create_lla_polygons_from_xy_points(\n        points_array=enu_points,\n        ref_latitude=ref_latitude,\n        ref_longitude=ref_longitude,\n        ref_altitude=ref_altitude,\n        boolean_mask=count_boolean,\n    )\n\n    if show_summary_results:\n        summary_trace = self.create_summary_trace(summary_result=summary_result)\n\n    self.create_empty_map_figure(dict_key=\"count_map\")\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=normalized_count[count_boolean].flatten(),\n        opacity=0.8,\n        name=\"normalized_count\",\n        colorbar={\"title\": \"Normalized Count\", \"orientation\": \"h\"},\n        map_color_scale=\"Bluered\",\n    )\n    self.figure_dict[\"count_map\"].add_trace(trace)\n    self.figure_dict[\"count_map\"].update_layout(\n        map_style=\"carto-positron\",\n        map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n        title=f\"Source location probability \"\n        f\"(&gt;={normalized_count_limit}) for \"\n        f\"{datetime_min_string} to {datetime_max_string}\",\n        font_family=\"Futura\",\n        font_size=15,\n    )\n    sensor_object.plot_sensor_location(self.figure_dict[\"count_map\"])\n    self.figure_dict[\"count_map\"].update_traces(showlegend=False)\n\n    adjusted_result_weights = result_weighted.copy()\n    adjusted_result_weights[adjusted_result_weights == 0] = np.nan\n\n    median_of_all_emissions = np.nanmedian(adjusted_result_weights, axis=2)\n\n    self.create_empty_map_figure(dict_key=\"median_map\")\n\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=median_of_all_emissions[count_boolean].flatten(),\n        opacity=0.8,\n        name=\"median_emission\",\n        colorbar={\"title\": \"Median Emission\", \"orientation\": \"h\"},\n        map_color_scale=\"Bluered\",\n    )\n    self.figure_dict[\"median_map\"].add_trace(trace)\n    self.figure_dict[\"median_map\"].update_layout(\n        map_style=\"carto-positron\",\n        map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n        title=f\"Median emission rate estimate for {datetime_min_string} to {datetime_max_string}\",\n        font_family=\"Futura\",\n        font_size=15,\n    )\n    sensor_object.plot_sensor_location(self.figure_dict[\"median_map\"])\n    self.figure_dict[\"median_map\"].update_traces(showlegend=False)\n\n    iqr_of_all_emissions = np.nanquantile(a=adjusted_result_weights, q=0.75, axis=2) - np.nanquantile(\n        a=adjusted_result_weights, q=0.25, axis=2\n    )\n    self.create_empty_map_figure(dict_key=\"iqr_map\")\n\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=iqr_of_all_emissions[count_boolean].flatten(),\n        opacity=0.8,\n        name=\"iqr_emission\",\n        colorbar={\"title\": \"IQR\", \"orientation\": \"h\"},\n        map_color_scale=\"Bluered\",\n    )\n    self.figure_dict[\"iqr_map\"].add_trace(trace)\n    self.figure_dict[\"iqr_map\"].update_layout(\n        map_style=\"carto-positron\",\n        map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n        title=f\"Inter Quartile range (25%-75%) of emission rate \"\n        f\"estimate for {datetime_min_string} to {datetime_max_string}\",\n        font_family=\"Futura\",\n        font_size=15,\n    )\n    sensor_object.plot_sensor_location(self.figure_dict[\"iqr_map\"])\n    self.figure_dict[\"iqr_map\"].update_traces(showlegend=False)\n\n    if show_fixed_source_locations:\n        for key, _ in model_object.components.items():\n            if bool(re.search(\"fixed\", key)):\n                source_model_fixed = model_object.components[key]\n                source_locations_fixed = source_model_fixed.all_source_locations\n                source_location_fixed_lla = source_locations_fixed.to_lla()\n                sources_lat = source_location_fixed_lla.latitude[:, 0]\n                sources_lon = source_location_fixed_lla.longitude[:, 0]\n                fixed_source_location_trace = go.Scattermap(\n                    mode=\"markers\",\n                    lon=sources_lon,\n                    lat=sources_lat,\n                    name=f\"Fixed source locations, {key}\",\n                    marker={\"size\": 10, \"opacity\": 0.8},\n                )\n                self.figure_dict[\"count_map\"].add_trace(fixed_source_location_trace)\n                self.figure_dict[\"median_map\"].add_trace(fixed_source_location_trace)\n                self.figure_dict[\"iqr_map\"].add_trace(fixed_source_location_trace)\n\n    if show_summary_results:\n        self.figure_dict[\"count_map\"].add_trace(summary_trace)\n        self.figure_dict[\"count_map\"].update_traces(showlegend=True)\n        self.figure_dict[\"median_map\"].add_trace(summary_trace)\n        self.figure_dict[\"median_map\"].update_traces(showlegend=True)\n        self.figure_dict[\"iqr_map\"].add_trace(summary_trace)\n        self.figure_dict[\"iqr_map\"].update_traces(showlegend=True)\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_coverage","title":"<code>plot_coverage(coordinates, couplings, threshold_function=np.max, coverage_threshold=6, opacity=0.8, map_color_scale='jet')</code>","text":"<p>Creates a coverage plot using the coverage function from Gaussian Plume.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>LLA object</code> <p>A LLA coordinate object containing a set of locations.</p> required <code>couplings</code> <code>array</code> <p>The calculated values of coupling (The 'A matrix') for a set of wind data.</p> required <code>threshold_function</code> <code>Callable</code> <p>Callable function which returns some single value that defines the                          maximum or 'threshold' coupling. Examples: np.quantile(q=0.9),                          np.max, np.mean. Defaults to np.max.</p> <code>max</code> <code>coverage_threshold</code> <code>float</code> <p>The threshold value of the estimated emission rate which is                                   considered to be within the coverage. Defaults to 6 kg/hr.</p> <code>6</code> <code>opacity</code> <code>float</code> <p>The opacity of the grid cells when they are plotted.</p> <code>0.8</code> <code>map_color_scale</code> <code>str</code> <p>The string which defines which plotly colour scale should be used when plotting                    the values.</p> <code>'jet'</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_coverage(\n    self,\n    coordinates: LLA,\n    couplings: np.ndarray,\n    threshold_function: Callable = np.max,\n    coverage_threshold: float = 6,\n    opacity: float = 0.8,\n    map_color_scale=\"jet\",\n):\n    \"\"\"Creates a coverage plot using the coverage function from Gaussian Plume.\n\n    Args:\n        coordinates (LLA object): A LLA coordinate object containing a set of locations.\n        couplings (np.array): The calculated values of coupling (The 'A matrix') for a set of wind data.\n        threshold_function (Callable, optional): Callable function which returns some single value that defines the\n                                     maximum or 'threshold' coupling. Examples: np.quantile(q=0.9),\n                                     np.max, np.mean. Defaults to np.max.\n        coverage_threshold (float, optional): The threshold value of the estimated emission rate which is\n                                              considered to be within the coverage. Defaults to 6 kg/hr.\n        opacity (float): The opacity of the grid cells when they are plotted.\n        map_color_scale (str): The string which defines which plotly colour scale should be used when plotting\n                               the values.\n\n    \"\"\"\n    coverage_values = GaussianPlume(source_map=None).compute_coverage(\n        couplings=couplings, threshold_function=threshold_function, coverage_threshold=coverage_threshold\n    )\n    self.plot_values_on_map(\n        dict_key=\"coverage_map\",\n        coordinates=coordinates,\n        values=coverage_values,\n        aggregate_function=np.max,\n        opacity=opacity,\n        map_color_scale=map_color_scale,\n    )\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.create_summary_trace","title":"<code>create_summary_trace(summary_result)</code>  <code>staticmethod</code>","text":"<p>Helper function to create the summary information to plot on top of map type plots.</p> <p>We use the summary result calculated through the support functions module to create a trace which contains the summary information for each source location.</p> <p>Parameters:</p> Name Type Description Default <code>summary_result</code> <code>DataFrame</code> <p>DataFrame containing the summary information for each source location.</p> required <p>Returns:</p> Name Type Description <code>summary_trace</code> <code>Scattermap</code> <p>Trace with summary information to plot on top of map type plots.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>@staticmethod\ndef create_summary_trace(\n    summary_result: pd.DataFrame,\n) -&gt; go.Scattermap:\n    \"\"\"Helper function to create the summary information to plot on top of map type plots.\n\n    We use the summary result calculated through the support functions module to create a trace which contains\n    the summary information for each source location.\n\n    Args:\n        summary_result (pd.DataFrame): DataFrame containing the summary information for each source location.\n\n    Returns:\n        summary_trace (go.Scattermap): Trace with summary information to plot on top of map type plots.\n\n    \"\"\"\n    summary_text_values = [\n        f\"&lt;b&gt;Source ID&lt;/b&gt;: {value}&lt;br&gt;\"\n        f\"&lt;b&gt;(Lon, Lat, Alt)&lt;/b&gt; ([deg], [deg], [m]):&lt;br&gt;\"\n        f\"({summary_result.longitude[value]:.7f}, \"\n        f\"{summary_result.latitude[value]:.7f}, {summary_result.altitude[value]:.3f})&lt;br&gt;\"\n        f\"&lt;b&gt;Height&lt;/b&gt;: {summary_result.height[value]:.3f} [m]&lt;br&gt;\"\n        f\"&lt;b&gt;Median emission rate&lt;/b&gt;: {summary_result.median_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;2.5% quantile&lt;/b&gt;: {summary_result.quantile_025[value]:.3f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;97.5% quantile&lt;/b&gt;: {summary_result.quantile_975[value]:.3f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;IQR&lt;/b&gt;: {summary_result.iqr_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;Blob present during&lt;/b&gt;: \"\n        f\"{summary_result.absolute_count_iterations[value]:.0f} iterations&lt;br&gt;\"\n        f\"&lt;b&gt;Blob likelihood&lt;/b&gt;: {summary_result.blob_likelihood[value]:.5f}&lt;br&gt;\"\n        for value in summary_result.index\n    ]\n\n    summary_trace = go.Scattermap(\n        lat=summary_result.latitude,\n        lon=summary_result.longitude,\n        mode=\"markers\",\n        marker=go.scattermap.Marker(size=14, color=\"black\"),\n        text=summary_text_values,\n        name=\"Summary\",\n        hoverinfo=\"text\",\n    )\n\n    return summary_trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.lighter_rgb","title":"<code>lighter_rgb(rbg_string)</code>","text":"<p>Takes in an RGB string and returns a lighter version of this colour.</p> <p>The colour is made lighter by increasing the magnitude of the RGB values by half of the difference between the original value and the number 255.</p> <p>Parameters:</p> Name Type Description Default <code>rbg_string</code> <code>str</code> <p>An RGB string.</p> required Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def lighter_rgb(rbg_string: str) -&gt; str:\n    \"\"\"Takes in an RGB string and returns a lighter version of this colour.\n\n    The colour is made lighter by increasing the magnitude of the RGB values by half of the difference between the\n    original value and the number 255.\n\n    Arguments:\n        rbg_string (str): An RGB string.\n\n    \"\"\"\n    rbg_string = rbg_string[4:-1]\n    rbg_string = rbg_string.replace(\" \", \"\")\n    colors = rbg_string.split(\",\")\n    colors_out = [np.nan, np.nan, np.nan]\n\n    for i, color in enumerate(colors):\n        color = int(color)\n        color = min(int(round(color + ((255 - color) * 0.5))), 255)\n        colors_out[i] = color\n\n    return f\"rgb({colors_out[0]}, {colors_out[1]}, {colors_out[2]})\"\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_quantiles_from_array","title":"<code>plot_quantiles_from_array(fig, x_values, y_values, quantiles, color, name=None)</code>","text":"<p>Plot quantiles over y-values against x-values.</p> <p>Assuming x-values have size N and y-values have size [N x M] where the second dimension is the dimension to calculate the quantiles over.</p> <p>Will plot the median of the y-values as a solid line and a filled area between the lower and upper specified quantile.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure to add the traces on.</p> required <code>x_values</code> <code>Union[ndarray, DatetimeArray]</code> <p>Numpy array containing the x-values to plot.</p> required <code>y_values</code> <code>ndarray</code> <p>Numpy array containing the y-values to calculate the quantiles for.</p> required <code>quantiles</code> <code>Union[tuple, list, ndarray]</code> <p>Values of upper and lower quantile to plot in range (0-100)</p> required <code>color</code> <code>str</code> <p>RGB string specifying color for quantile fill plot.</p> required <code>name</code> <code>str</code> <p>Optional string name to show in the legend.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure with the quantile filled traces and median trace added on it.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_quantiles_from_array(\n    fig: go.Figure,\n    x_values: Union[np.ndarray, pd.arrays.DatetimeArray],\n    y_values: np.ndarray,\n    quantiles: Union[tuple, list, np.ndarray],\n    color: str,\n    name: str = None,\n) -&gt; go.Figure:\n    \"\"\"Plot quantiles over y-values against x-values.\n\n    Assuming x-values have size N and y-values have size [N x M] where the second dimension is the dimension to\n    calculate the quantiles over.\n\n    Will plot the median of the y-values as a solid line and a filled area between the lower and upper specified\n    quantile.\n\n    Args:\n        fig (go.Figure): Plotly figure to add the traces on.\n        x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Numpy array containing the x-values to plot.\n        y_values (np.ndarray): Numpy array containing the y-values to calculate the quantiles for.\n        quantiles (Union[tuple, list, np.ndarray]): Values of upper and lower quantile to plot in range (0-100)\n        color (str): RGB string specifying color for quantile fill plot.\n        name (str, optional): Optional string name to show in the legend.\n\n    Returns:\n         fig (go.Figure): Plotly figure with the quantile filled traces and median trace added on it.\n\n    \"\"\"\n    color_fill = f\"rgba{color[3:-1]}, 0.3)\"\n\n    median_trace = go.Scatter(\n        x=x_values,\n        y=np.median(y_values, axis=1),\n        mode=\"lines\",\n        line={\"width\": 3, \"color\": color},\n        name=f\"Median for {name}\",\n        legendgroup=name,\n        showlegend=False,\n    )\n\n    lower_quantile_trace = go.Scatter(\n        x=x_values,\n        y=np.quantile(y_values, axis=1, q=quantiles[0] / 100),\n        mode=\"lines\",\n        line={\"width\": 0, \"color\": color_fill},\n        name=f\"{quantiles[0]}% quantile\",\n        legendgroup=name,\n        showlegend=False,\n    )\n\n    upper_quantile_trace = go.Scatter(\n        x=x_values,\n        y=np.quantile(y_values, axis=1, q=quantiles[1] / 100),\n        fill=\"tonexty\",\n        fillcolor=color_fill,\n        mode=\"lines\",\n        line={\"width\": 0, \"color\": color_fill},\n        name=f\"{quantiles[1]}% quantile\",\n        legendgroup=name,\n        showlegend=False,\n    )\n\n    fig.add_trace(median_trace)\n    fig.add_trace(lower_quantile_trace)\n    fig.add_trace(upper_quantile_trace)\n\n    return fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.create_trace_specifics","title":"<code>create_trace_specifics(object_to_plot, **kwargs)</code>","text":"<p>Specification of different traces of single variables.</p> <p>Provides all details for plots where we want to plot a single variable as a line plot. Based on the object_to_plot we select the correct plot to show.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[Type[SlabAndSpike], SourceModel, MCMC]</code> <p>Object which we want to plot a single variable from</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the following key/values: x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot. y_values (np.ndarray): Numpy array containing the y-values to use in plotting. dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the Plot     class. title_text (str): String title of the plot. x_label (str): String label of x-axis. y_label (str) : String label of y-axis. name (str): String name to show in the legend. color (str): RGB string specifying color for plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When no specifics are defined for the inputted object to plot.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def create_trace_specifics(object_to_plot: Union[Type[SlabAndSpike], SourceModel, MCMC], **kwargs: Any) -&gt; dict:\n    \"\"\"Specification of different traces of single variables.\n\n    Provides all details for plots where we want to plot a single variable as a line plot. Based on the object_to_plot\n    we select the correct plot to show.\n\n    Args:\n        object_to_plot (Union[Type[SlabAndSpike], SourceModel, MCMC]): Object which we want to plot a single\n            variable from\n        **kwargs (Any): Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not\n            applicable to all.\n\n    Returns:\n        dict: A dictionary with the following key/values:\n            x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot.\n            y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n            dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the Plot\n                class.\n            title_text (str): String title of the plot.\n            x_label (str): String label of x-axis.\n            y_label (str) : String label of y-axis.\n            name (str): String name to show in the legend.\n            color (str): RGB string specifying color for plot.\n\n    Raises:\n        ValueError: When no specifics are defined for the inputted object to plot.\n\n    \"\"\"\n    if isinstance(object_to_plot, SourceModel):\n        dict_key = kwargs.pop(\"dict_key\", \"number_of_sources_plot\")\n        title_text = \"Number of Sources 'on' against MCMC iterations\"\n        x_label = MCMC_ITERATION_NUMBER_LITERAL\n        y_label = \"Number of Sources 'on'\"\n        y_values = object_to_plot.number_on_sources\n        x_values = np.array(range(y_values.size))\n        color = \"rgb(248, 156, 116)\"\n        name = \"Number of Sources 'on'\"\n\n    elif isinstance(object_to_plot, MCMC):\n        dict_key = kwargs.pop(\"dict_key\", \"log_posterior_plot\")\n        title_text = \"Log posterior values against MCMC iterations\"\n        x_label = MCMC_ITERATION_NUMBER_LITERAL\n        y_label = \"Log Posterior&lt;br&gt;Value\"\n        y_values = object_to_plot.store[\"log_post\"].flatten()\n        x_values = np.array(range(y_values.size))\n        color = RGB_LIGHT_BLUE\n        name = \"Log Posterior\"\n\n        if \"burn_in\" not in kwargs:\n            warnings.warn(\"Burn in is not specified for the Log Posterior plot, are you sure this is correct?\")\n\n    else:\n        raise ValueError(\"No values to plot\")\n\n    return {\n        \"x_values\": x_values,\n        \"y_values\": y_values,\n        \"dict_key\": dict_key,\n        \"title_text\": title_text,\n        \"x_label\": x_label,\n        \"y_label\": y_label,\n        \"name\": name,\n        \"color\": color,\n    }\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.create_plot_specifics","title":"<code>create_plot_specifics(object_to_plot, sensor_object, plot_type='', **kwargs)</code>","text":"<p>Specification of different traces where we want to plot a trace for each sensor.</p> <p>Provides all details for plots where we want to plot a single variable for each sensor as a line or box plot. Based on the object_to_plot we select the correct plot to show.</p> <p>When plotting the MCMC Observations and Predicted Model Values Against Time plot we are assuming time axis is the same for all sensors w.r.t. the fitted values from the MCMC store attribute, so we are only using the time axis from the first sensor.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[ErrorModel, PerSensor, MCMC]</code> <p>Object which we want to plot a single variable from</p> required <code>sensor_object</code> <code>SensorGroup</code> <p>SensorGroup object associated with the object_to_plot</p> required <code>plot_type</code> <code>str</code> <p>String specifying either a line or a box plot.</p> <code>''</code> <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the following key/values: x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot. y_values (np.ndarray): Numpy array containing the y-values to use in plotting. dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the     Plot class. title_text (str): String title of the plot. x_label (str): String label of x-axis. y_label (str): String label of y-axis. plot_type (str): Type of plot which needs to be generated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When no specifics are defined for the inputted object to plot.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def create_plot_specifics(\n    object_to_plot: Union[ErrorModel, PerSensor, MCMC], sensor_object: SensorGroup, plot_type: str = \"\", **kwargs: Any\n) -&gt; dict:\n    \"\"\"Specification of different traces where we want to plot a trace for each sensor.\n\n    Provides all details for plots where we want to plot a single variable for each sensor as a line or box plot.\n    Based on the object_to_plot we select the correct plot to show.\n\n    When plotting the MCMC Observations and Predicted Model Values Against Time plot we are assuming time axis is the\n    same for all sensors w.r.t. the fitted values from the MCMC store attribute, so we are only using the time axis\n    from the first sensor.\n\n    Args:\n        object_to_plot (Union[ErrorModel, PerSensor, MCMC]): Object which we want to plot a single variable from\n        sensor_object (SensorGroup): SensorGroup object associated with the object_to_plot\n        plot_type (str, optional): String specifying either a line or a box plot.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not\n            applicable to all.\n\n    Returns:\n        dict: A dictionary with the following key/values:\n            x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot.\n            y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n            dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the\n                Plot class.\n            title_text (str): String title of the plot.\n            x_label (str): String label of x-axis.\n            y_label (str): String label of y-axis.\n            plot_type (str): Type of plot which needs to be generated.\n\n    Raises:\n        ValueError: When no specifics are defined for the inputted object to plot.\n\n    \"\"\"\n    if isinstance(object_to_plot, ErrorModel):\n        y_values = np.sqrt(1 / object_to_plot.precision)\n        x_values = np.array(range(y_values.shape[1]))\n\n        if plot_type == \"line\":\n            dict_key = kwargs.pop(\"dict_key\", \"error_model_iterations\")\n            title_text = \"Estimated Error Model Values\"\n            x_label = MCMC_ITERATION_NUMBER_LITERAL\n            y_label = \"Estimated Error Model&lt;br&gt;Standard Deviation (ppm)\"\n\n        elif plot_type == \"box\":\n            dict_key = kwargs.pop(\"dict_key\", \"error_model_distributions\")\n            title_text = \"Distributions of Estimated Error Model Values After Burn-In\"\n            x_label = \"Sensor\"\n            y_label = \"Estimated Error Model&lt;br&gt;Standard Deviation (ppm)\"\n\n        else:\n            raise ValueError(\"Only line and box are allowed for the plot_type argument for ErrorModel\")\n\n        if \"burn_in\" not in kwargs:\n            warnings.warn(\"Burn in is not specified for the ErrorModel plot, are you sure this is correct?\")\n\n    elif isinstance(object_to_plot, PerSensor):\n        offset_sensor_name = list(sensor_object.values())[0].label\n        y_values = object_to_plot.offset\n        nan_row = np.tile(np.nan, (1, y_values.shape[1]))\n        y_values = np.concatenate((nan_row, y_values), axis=0)\n        x_values = np.array(range(y_values.shape[1]))\n\n        if plot_type == \"line\":\n            dict_key = kwargs.pop(\"dict_key\", \"offset_iterations\")\n            title_text = f\"Estimated Value of Offset w.r.t. {offset_sensor_name}\"\n            x_label = MCMC_ITERATION_NUMBER_LITERAL\n            y_label = \"Estimated Offset&lt;br&gt;Value (ppm)\"\n\n        elif plot_type == \"box\":\n            dict_key = kwargs.pop(\"dict_key\", \"offset_distributions\")\n            title_text = f\"Distributions of Estimated Offset Values w.r.t. {offset_sensor_name} After Burn-In\"\n            x_label = \"Sensor\"\n            y_label = \"Estimated Offset&lt;br&gt;Value (ppm)\"\n\n        else:\n            raise ValueError(\"Only line and box are allowed for the plot_type argument for PerSensor OffsetModel\")\n\n        if \"burn_in\" not in kwargs:\n            warnings.warn(\"Burn in is not specified for the PerSensor OffsetModel plot, are you sure this is correct?\")\n\n    elif isinstance(object_to_plot, MCMC):\n        y_values = object_to_plot.store[\"y\"]\n        x_values = list(sensor_object.values())[0].time\n        dict_key = kwargs.pop(\"dict_key\", \"fitted_values\")\n        title_text = \"Observations and Predicted Model Values Against Time\"\n        x_label = \"Time\"\n        y_label = \"Concentration (ppm)\"\n        plot_type = \"line\"\n\n    else:\n        raise ValueError(\"No values to plot\")\n\n    return {\n        \"x_values\": x_values,\n        \"y_values\": y_values,\n        \"dict_key\": dict_key,\n        \"title_text\": title_text,\n        \"x_label\": x_label,\n        \"y_label\": y_label,\n        \"plot_type\": plot_type,\n    }\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_single_scatter","title":"<code>plot_single_scatter(fig, x_values, y_values, color, name, **kwargs)</code>","text":"<p>Plots a single scatter trace on the supplied figure object.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure to add the trace to.</p> required <code>x_values</code> <code>Union[ndarray, DatetimeArray]</code> <p>X values to plot</p> required <code>y_values</code> <code>ndarray</code> <p>Numpy array containing the y-values to use in plotting.</p> required <code>color</code> <code>str</code> <p>RGB color string to use for this trace.</p> required <code>name</code> <code>str</code> <p>String name to show in the legend.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, legend_group, show_legend, used in some specific plots but not applicable to all.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure with the trace added to it.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_single_scatter(\n    fig: go.Figure,\n    x_values: Union[np.ndarray, pd.arrays.DatetimeArray],\n    y_values: np.ndarray,\n    color: str,\n    name: str,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Plots a single scatter trace on the supplied figure object.\n\n    Args:\n        fig (go.Figure): Plotly figure to add the trace to.\n        x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): X values to plot\n        y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n        color (str): RGB color string to use for this trace.\n        name (str): String name to show in the legend.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, used in some specific\n            plots but not applicable to all.\n\n    Returns:\n        fig (go.Figure): Plotly figure with the trace added to it.\n\n    \"\"\"\n    burn_in = kwargs.pop(\"burn_in\", 0)\n    legend_group = kwargs.pop(\"legend_group\", name)\n    show_legend = kwargs.pop(\"show_legend\", True)\n    if burn_in &gt; 0:\n        fig.add_trace(\n            go.Scatter(\n                x=x_values[: burn_in + 1],\n                y=y_values[: burn_in + 1],\n                name=name,\n                mode=\"lines\",\n                line={\"width\": 3, \"color\": lighter_rgb(color)},\n                legendgroup=legend_group,\n                showlegend=False,\n            )\n        )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x_values[burn_in:],\n            y=y_values[burn_in:],\n            name=name,\n            mode=\"lines\",\n            line={\"width\": 3, \"color\": color},\n            legendgroup=legend_group,\n            showlegend=show_legend,\n        )\n    )\n\n    return fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_single_box","title":"<code>plot_single_box(fig, y_values, color, name)</code>","text":"<p>Plot a single box plot trace on the plot figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure to add the trace to.</p> required <code>y_values</code> <code>ndarray</code> <p>Numpy array containing the y-values to use in plotting.</p> required <code>color</code> <code>str</code> <p>RGB color string to use for this trace.</p> required <code>name</code> <code>str</code> <p>String name to show in the legend.</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure with the trace added to it.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_single_box(fig: go.Figure, y_values: np.ndarray, color: str, name: str) -&gt; go.Figure:\n    \"\"\"Plot a single box plot trace on the plot figure.\n\n    Args:\n        fig (go.Figure): Plotly figure to add the trace to.\n        y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n        color (str): RGB color string to use for this trace.\n        name (str): String name to show in the legend.\n\n    Returns:\n        fig (go.Figure): Plotly figure with the trace added to it.\n\n    \"\"\"\n    fig.add_trace(go.Box(y=y_values, name=name, legendgroup=name, marker={\"color\": color}))\n\n    return fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_polygons_on_map","title":"<code>plot_polygons_on_map(polygons, values, opacity, map_color_scale, **kwargs)</code>","text":"<p>Plot a set of polygons on a map.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[ndarray, list]</code> <p>Numpy array or list containing the polygons to plot.</p> required <code>values</code> <code>ndarray</code> <p>Numpy array consistent with polygons containing the value which is                  used in coloring the polygons on the map.</p> required <code>opacity</code> <code>float</code> <p>Float between 0 and 1 specifying the opacity of the polygon fill color.</p> required <code>map_color_scale</code> <code>str</code> <p>The string which defines which plotly color scale.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments which can be passed on the go.Choroplethmap object (will override the default values as specified in this function)</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>trace</code> <code>Choroplethmap</code> <p>go.Choroplethmap trace with the colored polygons which can be added to a go.Figure object.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_polygons_on_map(\n    polygons: Union[np.ndarray, list], values: np.ndarray, opacity: float, map_color_scale: str, **kwargs: Any\n) -&gt; go.Choroplethmap:\n    \"\"\"Plot a set of polygons on a map.\n\n    Args:\n        polygons (Union[np.ndarray, list]): Numpy array or list containing the polygons to plot.\n        values (np.ndarray): Numpy array consistent with polygons containing the value which is\n                             used in coloring the polygons on the map.\n        opacity (float): Float between 0 and 1 specifying the opacity of the polygon fill color.\n        map_color_scale (str): The string which defines which plotly color scale.\n        **kwargs (Any): Additional key word arguments which can be passed on the go.Choroplethmap object\n            (will override the default values as specified in this function)\n\n    Returns:\n        trace: go.Choroplethmap trace with the colored polygons which can be added to a go.Figure object.\n\n    \"\"\"\n    polygon_id = list(range(values.shape[0]))\n    feature_collection = FeatureCollection([Feature(geometry=polygons[idx], id_value=idx) for idx in polygon_id])\n    text_box = [\n        f\"&lt;b&gt;Polygon ID&lt;/b&gt;: {counter:d}&lt;br&gt;&lt;b&gt;Center (lon, lat)&lt;/b&gt;: \"\n        f\"({polygons[counter].centroid.coords[0][0]:.4f}, {polygons[counter].centroid.coords[0][1]:.4f})&lt;br&gt;\"\n        f\"&lt;b&gt;Value&lt;/b&gt;: {values[counter]:f}&lt;br&gt;\"\n        for counter in polygon_id\n    ]\n\n    trace_options = {\n        \"geojson\": feature_collection,\n        \"featureidkey\": \"id_value\",\n        \"locations\": polygon_id,\n        \"z\": values,\n        \"marker\": {\"line\": {\"width\": 0}, \"opacity\": opacity},\n        \"hoverinfo\": \"text\",\n        \"text\": text_box,\n        \"name\": \"Values\",\n        \"colorscale\": map_color_scale,\n        \"colorbar\": {\"title\": \"Values\"},\n        \"showlegend\": True,\n    }\n\n    for key, value in kwargs.items():\n        trace_options[key] = value\n\n    trace = go.Choroplethmap(**trace_options)\n\n    return trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_regular_grid","title":"<code>plot_regular_grid(coordinates, values, opacity, map_color_scale, tolerance=1e-07, unit='kg/hr', name='Values')</code>","text":"<p>Plots a regular grid of LLA data onto a map.</p> <p>So long as the input array is regularly spaced, the value of the spacing is found. A set of rectangles are defined where the centre of the rectangle is the LLA coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>LLA object</code> <p>A LLA coordinate object containing a set of locations.</p> required <code>values</code> <code>array</code> <p>A set of values that correspond to locations specified in the coordinates.</p> required <code>opacity</code> <code>float</code> <p>The opacity of the grid cells when they are plotted.</p> required <code>map_color_scale</code> <code>str</code> <p>The string which defines which plotly color scale should be used when plotting the values.</p> required <code>tolerance</code> <code>float</code> <p>Absolute value above which the difference between values is considered significant.                          Used to calculate the regular grid of coordinate values. Defaults to 1e-7.</p> <code>1e-07</code> <code>unit</code> <code>str</code> <p>The unit to be added to the colorscale. Defaults to kg/hr.</p> <code>'kg/hr'</code> <code>name</code> <code>str</code> <p>Name for the trace to be used in the color bar as well</p> <code>'Values'</code> <p>Returns:</p> Name Type Description <code>trace</code> <code>Choroplethmap</code> <p>Trace with the colored polygons which can be added to a go.Figure object.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_regular_grid(\n    coordinates: LLA,\n    values: np.ndarray,\n    opacity: float,\n    map_color_scale: str,\n    tolerance: float = 1e-7,\n    unit: str = \"kg/hr\",\n    name=\"Values\",\n) -&gt; go.Choroplethmap:\n    \"\"\"Plots a regular grid of LLA data onto a map.\n\n    So long as the input array is regularly spaced, the value of the spacing is found. A set of rectangles are defined\n    where the centre of the rectangle is the LLA coordinate.\n\n    Args:\n        coordinates (LLA object): A LLA coordinate object containing a set of locations.\n        values (np.array): A set of values that correspond to locations specified in the coordinates.\n        opacity (float): The opacity of the grid cells when they are plotted.\n        map_color_scale (str): The string which defines which plotly color scale should be used when plotting\n            the values.\n        tolerance (float, optional): Absolute value above which the difference between values is considered significant.\n                                     Used to calculate the regular grid of coordinate values. Defaults to 1e-7.\n        unit (str, optional): The unit to be added to the colorscale. Defaults to kg/hr.\n        name (str, optional): Name for the trace to be used in the color bar as well\n\n    Returns:\n        trace (go.Choroplethmap): Trace with the colored polygons which can be added to a go.Figure object.\n\n    \"\"\"\n    _, gridsize_lat = is_regularly_spaced(coordinates.latitude, tolerance=tolerance)\n    _, gridsize_lon = is_regularly_spaced(coordinates.longitude, tolerance=tolerance)\n\n    polygons = [\n        geometry.box(\n            coordinates.longitude[idx] - gridsize_lon / 2,\n            coordinates.latitude[idx] - gridsize_lat / 2,\n            coordinates.longitude[idx] + gridsize_lon / 2,\n            coordinates.latitude[idx] + gridsize_lat / 2,\n        )\n        for idx in range(coordinates.nof_observations)\n    ]\n\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=values,\n        opacity=opacity,\n        name=name,\n        colorbar={\"title\": name + \"&lt;br&gt;\" + unit},\n        map_color_scale=map_color_scale,\n    )\n\n    return trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_hexagonal_grid","title":"<code>plot_hexagonal_grid(coordinates, values, opacity, map_color_scale, num_hexagons, show_positions, aggregate_function=np.sum)</code>","text":"<p>Plots a set of values into hexagonal bins with respect to the location of the values.</p> <p>Any data points that fall within the area of a hexagon are used to perform aggregation and bin the data. See: https://plotly.com/python-api-reference/generated/plotly.figure_factory.create_hexbin_mapbox.html</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>LLA object</code> <p>A LLA coordinate object containing a set of locations.</p> required <code>values</code> <code>array</code> <p>A set of values that correspond to locations specified in the coordinates.</p> required <code>opacity</code> <code>float</code> <p>The opacity of the hexagons when they are plotted.</p> required <code>map_color_scale</code> <code>str</code> <p>Colour scale for plotting values.</p> required <code>num_hexagons</code> <code>Union[int, None]</code> <p>The number of hexagons which define the horizontal axis of the plot.</p> required <code>show_positions</code> <code>bool</code> <p>A flag to determine whether the original data should be shown alongside the binning hexagons.</p> required <code>aggregate_function</code> <code>Callable</code> <p>Function which to apply on the data in each hexagonal bin to aggregate the data and visualise the result.</p> <code>sum</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A plotly go figure representing the data which was submitted to this function.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_hexagonal_grid(\n    coordinates: LLA,\n    values: np.ndarray,\n    opacity: float,\n    map_color_scale: str,\n    num_hexagons: Union[int, None],\n    show_positions: bool,\n    aggregate_function: Callable = np.sum,\n):\n    \"\"\"Plots a set of values into hexagonal bins with respect to the location of the values.\n\n    Any data points that fall within the area of a hexagon are used to perform aggregation and bin the data.\n    See: https://plotly.com/python-api-reference/generated/plotly.figure_factory.create_hexbin_mapbox.html\n\n    Args:\n        coordinates (LLA object): A LLA coordinate object containing a set of locations.\n        values (np.array): A set of values that correspond to locations specified in the coordinates.\n        opacity (float): The opacity of the hexagons when they are plotted.\n        map_color_scale (str): Colour scale for plotting values.\n        num_hexagons (Union[int, None]): The number of hexagons which define the *horizontal* axis of the plot.\n        show_positions (bool): A flag to determine whether the original data should be shown alongside\n            the binning hexagons.\n        aggregate_function (Callable, optional): Function which to apply on the data in each hexagonal bin to aggregate\n            the data and visualise the result.\n\n    Returns:\n        (go.Figure): A plotly go figure representing the data which was submitted to this function.\n\n    \"\"\"\n    if num_hexagons is None:\n        num_hexagons = max(1, np.ceil((np.max(coordinates.longitude) - np.min(coordinates.longitude)) / 0.25))\n\n    coordinates = coordinates.to_lla()\n\n    hex_plot = ff.create_hexbin_mapbox(\n        lat=coordinates.latitude,\n        lon=coordinates.longitude,\n        color=values,\n        nx_hexagon=num_hexagons,\n        opacity=opacity,\n        agg_func=aggregate_function,\n        color_continuous_scale=map_color_scale,\n        show_original_data=show_positions,\n        original_data_marker={\"color\": \"black\"},\n    )\n\n    return hex_plot\n</code></pre>"},{"location":"pyelq/sensor/beam/","title":"Beam","text":""},{"location":"pyelq/sensor/beam/#beam","title":"Beam","text":"<p>Beam module.</p> <p>Subclass of Sensor. Used for beam sensors</p>"},{"location":"pyelq/sensor/beam/#pyelq.sensor.beam.Beam","title":"<code>Beam</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Sensor</code></p> <p>Defines Beam sensor class.</p> <p>Location attribute from superclass is assumed to be a Coordinate class object containing 2 locations, the first of the sensor and the second of the retro.</p> <p>Attributes:</p> Name Type Description <code>n_beam_knots</code> <code>int</code> <p>Number of beam knots to evaluate along a single beam</p> Source code in <code>src/pyelq/sensor/beam.py</code> <pre><code>@dataclass\nclass Beam(Sensor):\n    \"\"\"Defines Beam sensor class.\n\n    Location attribute from superclass is assumed to be a Coordinate class object containing 2 locations, the first of\n    the sensor and the second of the retro.\n\n    Attributes:\n        n_beam_knots (int, optional): Number of beam knots to evaluate along a single beam\n\n    \"\"\"\n\n    n_beam_knots: int = 50\n\n    @property\n    def midpoint(self) -&gt; np.ndarray:\n        \"\"\"np.ndarray: Midpoint of the beam.\"\"\"\n        return np.mean(self.location.to_array(), axis=0)\n\n    def make_beam_knots(self, ref_latitude, ref_longitude, ref_altitude=0) -&gt; np.ndarray:\n        \"\"\"Create beam knot locations.\n\n        Creates beam knot locations based on location attribute and n_beam_knot attribute.\n        Results in an array of beam knot locations of shape [n_beam_knots x 3]. Have to provide a reference point in\n        order to create the beam knots in a local frame, spaced in meters\n\n        Args:\n            ref_latitude (float): Reference latitude in degrees\n            ref_longitude (float): Reference longitude in degrees\n            ref_altitude (float, optional): Reference altitude in meters\n\n        \"\"\"\n        temp_location = self.location.to_enu(\n            ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude\n        ).to_array()\n        beam_knot_array = np.linspace(temp_location[0, :], temp_location[1, :], num=self.n_beam_knots, endpoint=True)\n        return beam_knot_array\n</code></pre>"},{"location":"pyelq/sensor/beam/#pyelq.sensor.beam.Beam.midpoint","title":"<code>midpoint</code>  <code>property</code>","text":"<p>np.ndarray: Midpoint of the beam.</p>"},{"location":"pyelq/sensor/beam/#pyelq.sensor.beam.Beam.make_beam_knots","title":"<code>make_beam_knots(ref_latitude, ref_longitude, ref_altitude=0)</code>","text":"<p>Create beam knot locations.</p> <p>Creates beam knot locations based on location attribute and n_beam_knot attribute. Results in an array of beam knot locations of shape [n_beam_knots x 3]. Have to provide a reference point in order to create the beam knots in a local frame, spaced in meters</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>Reference latitude in degrees</p> required <code>ref_longitude</code> <code>float</code> <p>Reference longitude in degrees</p> required <code>ref_altitude</code> <code>float</code> <p>Reference altitude in meters</p> <code>0</code> Source code in <code>src/pyelq/sensor/beam.py</code> <pre><code>def make_beam_knots(self, ref_latitude, ref_longitude, ref_altitude=0) -&gt; np.ndarray:\n    \"\"\"Create beam knot locations.\n\n    Creates beam knot locations based on location attribute and n_beam_knot attribute.\n    Results in an array of beam knot locations of shape [n_beam_knots x 3]. Have to provide a reference point in\n    order to create the beam knots in a local frame, spaced in meters\n\n    Args:\n        ref_latitude (float): Reference latitude in degrees\n        ref_longitude (float): Reference longitude in degrees\n        ref_altitude (float, optional): Reference altitude in meters\n\n    \"\"\"\n    temp_location = self.location.to_enu(\n        ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude\n    ).to_array()\n    beam_knot_array = np.linspace(temp_location[0, :], temp_location[1, :], num=self.n_beam_knots, endpoint=True)\n    return beam_knot_array\n</code></pre>"},{"location":"pyelq/sensor/satellite/","title":"Satellite","text":""},{"location":"pyelq/sensor/satellite/#satellite","title":"Satellite","text":"<p>Satellite module.</p> <p>Subclass of Sensor. Mainly used to accommodate satellite sensor TROPOMI. See: http://www.tropomi.eu/data-products/methane : http: //www.tropomi.eu/data-products/methane and http://www.tropomi.eu/data-products/nitrogen-dioxide</p>"},{"location":"pyelq/sensor/satellite/#pyelq.sensor.satellite.Satellite","title":"<code>Satellite</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Sensor</code></p> <p>Defines Satellite sensor class.</p> <p>Attributes:</p> Name Type Description <code>qa_value</code> <code>ndarray</code> <p>Array containing quality values associated with the observations.</p> <code>precision</code> <code>ndarray</code> <p>Array containing precision values associated with the observations.</p> <code>precision_kernel</code> <code>ndarray</code> <p>Array containing precision kernel values associated with the observations.</p> <code>ground_pixel</code> <code>ndarray</code> <p>Array containing ground pixels values associated with the observations. Ground pixels are indicating the dimension perpendicular to the flight direction.</p> <code>scanline</code> <code>ndarray</code> <p>Array containing scanline values associated with the observations. Scanlines are indicating the dimension in the direction of flight.</p> <code>orbit</code> <code>ndarray</code> <p>Array containing orbit values associated with the observations.</p> <code>pixel_bounds</code> <code>ndarray</code> <p>Array containing Polygon features which define the pixel bounds.</p> Source code in <code>src/pyelq/sensor/satellite.py</code> <pre><code>@dataclass\nclass Satellite(Sensor):\n    \"\"\"Defines Satellite sensor class.\n\n    Attributes:\n        qa_value (np.ndarray, optional): Array containing quality values associated with the observations.\n        precision (np.ndarray, optional): Array containing precision values associated with the observations.\n        precision_kernel (np.ndarray, optional): Array containing precision kernel values associated with the\n            observations.\n        ground_pixel (np.ndarray, optional): Array containing ground pixels values associated with the observations.\n            Ground pixels are indicating the dimension perpendicular to the flight direction.\n        scanline (np.ndarray, optional): Array containing scanline values associated with the observations.\n            Scanlines are indicating the dimension in the direction of flight.\n        orbit (np.ndarray, optional): Array containing orbit values associated with the observations.\n        pixel_bounds (np.ndarray, optional): Array containing Polygon features which define the pixel bounds.\n\n    \"\"\"\n\n    qa_value: np.ndarray = field(init=False)\n    precision: np.ndarray = field(init=False)\n    precision_kernel: np.ndarray = field(init=False)\n    ground_pixel: np.ndarray = field(init=False)\n    scanline: np.ndarray = field(init=False)\n    orbit: np.ndarray = field(init=False, default=None)\n    pixel_bounds: np.ndarray = field(init=False)\n\n    def get_orbits(self) -&gt; np.ndarray:\n        \"\"\"Gets the unique orbits which are present in the data.\n\n        Raises:\n            ValueError: When orbits attribute is None\n\n        Returns:\n            np.ndarray: Unique orbits present in the data.\n\n        \"\"\"\n        if self.orbit is None:\n            raise ValueError(\"Orbits attribute is None\")\n        return np.unique(self.orbit)\n</code></pre>"},{"location":"pyelq/sensor/satellite/#pyelq.sensor.satellite.Satellite.get_orbits","title":"<code>get_orbits()</code>","text":"<p>Gets the unique orbits which are present in the data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When orbits attribute is None</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Unique orbits present in the data.</p> Source code in <code>src/pyelq/sensor/satellite.py</code> <pre><code>def get_orbits(self) -&gt; np.ndarray:\n    \"\"\"Gets the unique orbits which are present in the data.\n\n    Raises:\n        ValueError: When orbits attribute is None\n\n    Returns:\n        np.ndarray: Unique orbits present in the data.\n\n    \"\"\"\n    if self.orbit is None:\n        raise ValueError(\"Orbits attribute is None\")\n    return np.unique(self.orbit)\n</code></pre>"},{"location":"pyelq/sensor/sensor/","title":"Overview","text":""},{"location":"pyelq/sensor/sensor/#sensor-classes","title":"Sensor classes","text":"<p>An overview of the sensor classes:</p> <ul> <li> <p>Beam</p> </li> <li> <p>Satellite</p> </li> </ul>"},{"location":"pyelq/sensor/sensor/#sensor-superclass","title":"Sensor superclass","text":"<p>Sensor module.</p> <p>The superclass for the sensor classes. This module provides the higher level Sensor and SensorGroup classes. The Sensor class is a single sensor, the SensorGroup is a dictionary of Sensors. The SensorGroup class is created to deal with the properties over all sensors together.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor","title":"<code>Sensor</code>  <code>dataclass</code>","text":"<p>Defines the properties and methods of the sensor class.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str</code> <p>String label for sensor</p> <code>time</code> <code>DatetimeArray</code> <p>Array containing time values associated with concentration reading</p> <code>location</code> <code>Coordinate</code> <p>Coordinate object specifying the observation locations</p> <code>concentration</code> <code>ndarray</code> <p>Array containing concentration values associated with time reading</p> <code>source_on</code> <code>ndarray</code> <p>Array of size nof_observations containing boolean values indicating whether a source is on or off for each observation, i.e. we are assuming the sensor can/can't see a source</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>@dataclass\nclass Sensor:\n    \"\"\"Defines the properties and methods of the sensor class.\n\n    Attributes:\n        label (str, optional): String label for sensor\n        time (pandas.arrays.DatetimeArray, optional): Array containing time values associated with concentration\n            reading\n        location (Coordinate, optional): Coordinate object specifying the observation locations\n        concentration (np.ndarray, optional): Array containing concentration values associated with time reading\n        source_on (np.ndarray, optional): Array of size nof_observations containing boolean values indicating\n            whether a source is on or off for each observation, i.e. we are assuming the sensor can/can't see a source\n\n    \"\"\"\n\n    label: str = field(init=False)\n    time: DatetimeArray = field(init=False, default=None)\n    location: Coordinate = field(init=False)\n    concentration: np.ndarray = field(default_factory=lambda: np.array([]))\n    source_on: np.ndarray = field(init=False, default=None)\n\n    @property\n    def nof_observations(self) -&gt; int:\n        \"\"\"Int: Number of observations contained in concentration array.\"\"\"\n        return self.concentration.size\n\n    def plot_sensor_location(self, fig: go.Figure, color=None) -&gt; go.Figure:\n        \"\"\"Plotting the sensor location.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color (`optional`): When specified, the color to be used\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor location trace added to it\n\n        \"\"\"\n        lla_object = self.location.to_lla()\n\n        marker_dict = {\"size\": 10, \"opacity\": 0.8}\n        if color is not None:\n            marker_dict[\"color\"] = color\n\n        fig.add_trace(\n            go.Scattermap(\n                mode=\"markers+lines\",\n                lat=np.array(lla_object.latitude),\n                lon=np.array(lla_object.longitude),\n                marker=marker_dict,\n                line={\"width\": 3},\n                name=self.label,\n            )\n        )\n        return fig\n\n    def plot_timeseries(self, fig: go.Figure, color=None, mode: str = \"markers\") -&gt; go.Figure:\n        \"\"\"Timeseries plot of the sensor concentration observations.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color (`optional`): When specified, the color to be used\n            mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor concentration timeseries trace added to it\n\n        \"\"\"\n        marker_dict = {\"size\": 5, \"opacity\": 1}\n        if color is not None:\n            marker_dict[\"color\"] = color\n\n        fig.add_trace(\n            go.Scatter(\n                x=self.time,\n                y=self.concentration.flatten(),\n                mode=mode,\n                marker=marker_dict,\n                name=self.label,\n                legendgroup=self.label,\n            )\n        )\n\n        return fig\n\n    def subset_sensor(self, section_index: int) -&gt; \"Sensor\":\n        \"\"\"Subset the sensor based on the provided section index.\n\n        The method is designed to return a new `Sensor` object containing only the observations corresponding to a\n        specified section index. Sections are defined by unique values in the `source_on` attribute. For a case where\n        the source is turned on and off multiple times, (0 values in `source_on` indicate off periods and positive\n        integers indicate different on periods). For example, if section_index=1, a new Sensor will be returned\n        containing only observations where self.source_on == 1.\n\n        If sensor.location.shape[0] and sensor.time.shape[0] align we assume the location values are dependent on time and\n        therefore need to be filtered accordingly and otherwise we keep the original location.\n\n        This functionality is useful for situations where data is collected in multiple sections, e.g. repeated on/off\n        releases where we want to work with one section at a time or later stitch multiple per-section segments\n        together.\n\n        Args:\n            section_index (int): Integer indicating which observations to keep.\n\n        Returns:\n            new_sensor (Sensor): A new Sensor object containing only the specified observations.\n\n        \"\"\"\n        if self.source_on is None:\n            return deepcopy(self)\n\n        section_indices = (self.source_on == section_index).flatten()\n        new_sensor = deepcopy(self)\n        new_sensor.time = self.time[section_indices]\n        new_sensor.concentration = self.concentration[section_indices]\n        location_object = new_sensor.location.to_array()\n        if location_object.shape[0] == self.time.shape[0]:\n            location_object = location_object[section_indices, :]\n            new_sensor.location = new_sensor.location.from_array(location_object)\n\n        new_sensor.source_on = self.source_on[section_indices]\n        return new_sensor\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.plot_sensor_location","title":"<code>plot_sensor_location(fig, color=None)</code>","text":"<p>Plotting the sensor location.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color</code> <code>`optional`</code> <p>When specified, the color to be used</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor location trace added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_sensor_location(self, fig: go.Figure, color=None) -&gt; go.Figure:\n    \"\"\"Plotting the sensor location.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color (`optional`): When specified, the color to be used\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor location trace added to it\n\n    \"\"\"\n    lla_object = self.location.to_lla()\n\n    marker_dict = {\"size\": 10, \"opacity\": 0.8}\n    if color is not None:\n        marker_dict[\"color\"] = color\n\n    fig.add_trace(\n        go.Scattermap(\n            mode=\"markers+lines\",\n            lat=np.array(lla_object.latitude),\n            lon=np.array(lla_object.longitude),\n            marker=marker_dict,\n            line={\"width\": 3},\n            name=self.label,\n        )\n    )\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.plot_timeseries","title":"<code>plot_timeseries(fig, color=None, mode='markers')</code>","text":"<p>Timeseries plot of the sensor concentration observations.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color</code> <code>`optional`</code> <p>When specified, the color to be used</p> <code>None</code> <code>mode</code> <code>str</code> <p>Mode used for plotting, i.e. markers, lines or markers+lines</p> <code>'markers'</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor concentration timeseries trace added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_timeseries(self, fig: go.Figure, color=None, mode: str = \"markers\") -&gt; go.Figure:\n    \"\"\"Timeseries plot of the sensor concentration observations.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color (`optional`): When specified, the color to be used\n        mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor concentration timeseries trace added to it\n\n    \"\"\"\n    marker_dict = {\"size\": 5, \"opacity\": 1}\n    if color is not None:\n        marker_dict[\"color\"] = color\n\n    fig.add_trace(\n        go.Scatter(\n            x=self.time,\n            y=self.concentration.flatten(),\n            mode=mode,\n            marker=marker_dict,\n            name=self.label,\n            legendgroup=self.label,\n        )\n    )\n\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.subset_sensor","title":"<code>subset_sensor(section_index)</code>","text":"<p>Subset the sensor based on the provided section index.</p> <p>The method is designed to return a new <code>Sensor</code> object containing only the observations corresponding to a specified section index. Sections are defined by unique values in the <code>source_on</code> attribute. For a case where the source is turned on and off multiple times, (0 values in <code>source_on</code> indicate off periods and positive integers indicate different on periods). For example, if section_index=1, a new Sensor will be returned containing only observations where self.source_on == 1.</p> <p>If sensor.location.shape[0] and sensor.time.shape[0] align we assume the location values are dependent on time and therefore need to be filtered accordingly and otherwise we keep the original location.</p> <p>This functionality is useful for situations where data is collected in multiple sections, e.g. repeated on/off releases where we want to work with one section at a time or later stitch multiple per-section segments together.</p> <p>Parameters:</p> Name Type Description Default <code>section_index</code> <code>int</code> <p>Integer indicating which observations to keep.</p> required <p>Returns:</p> Name Type Description <code>new_sensor</code> <code>Sensor</code> <p>A new Sensor object containing only the specified observations.</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def subset_sensor(self, section_index: int) -&gt; \"Sensor\":\n    \"\"\"Subset the sensor based on the provided section index.\n\n    The method is designed to return a new `Sensor` object containing only the observations corresponding to a\n    specified section index. Sections are defined by unique values in the `source_on` attribute. For a case where\n    the source is turned on and off multiple times, (0 values in `source_on` indicate off periods and positive\n    integers indicate different on periods). For example, if section_index=1, a new Sensor will be returned\n    containing only observations where self.source_on == 1.\n\n    If sensor.location.shape[0] and sensor.time.shape[0] align we assume the location values are dependent on time and\n    therefore need to be filtered accordingly and otherwise we keep the original location.\n\n    This functionality is useful for situations where data is collected in multiple sections, e.g. repeated on/off\n    releases where we want to work with one section at a time or later stitch multiple per-section segments\n    together.\n\n    Args:\n        section_index (int): Integer indicating which observations to keep.\n\n    Returns:\n        new_sensor (Sensor): A new Sensor object containing only the specified observations.\n\n    \"\"\"\n    if self.source_on is None:\n        return deepcopy(self)\n\n    section_indices = (self.source_on == section_index).flatten()\n    new_sensor = deepcopy(self)\n    new_sensor.time = self.time[section_indices]\n    new_sensor.concentration = self.concentration[section_indices]\n    location_object = new_sensor.location.to_array()\n    if location_object.shape[0] == self.time.shape[0]:\n        location_object = location_object[section_indices, :]\n        new_sensor.location = new_sensor.location.from_array(location_object)\n\n    new_sensor.source_on = self.source_on[section_indices]\n    return new_sensor\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup","title":"<code>SensorGroup</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict</code></p> <p>A dictionary containing multiple Sensors.</p> <p>This class is used when we want to combine a collection of sensors and be able to store/access overall properties.</p> <p>Attributes:</p> Name Type Description <code>color_map</code> <code>list</code> <p>Default colormap to use for plotting</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>@dataclass\nclass SensorGroup(dict):\n    \"\"\"A dictionary containing multiple Sensors.\n\n    This class is used when we want to combine a collection of sensors and be able to store/access overall properties.\n\n    Attributes:\n        color_map (list, optional): Default colormap to use for plotting\n\n    \"\"\"\n\n    color_map: list = field(default_factory=list, init=False)\n\n    def __post_init__(self):\n        self.color_map = px.colors.qualitative.Pastel\n\n    @property\n    def nof_observations(self) -&gt; int:\n        \"\"\"Int: The total number of observations across all the sensors.\"\"\"\n        return int(np.sum([sensor.nof_observations for sensor in self.values()], axis=None))\n\n    @property\n    def concentration(self) -&gt; np.ndarray:\n        \"\"\"np.ndarray: Column vector of concentration values across all sensors, unwrapped per sensor.\"\"\"\n        return np.concatenate([sensor.concentration.flatten() for sensor in self.values()], axis=0)\n\n    @property\n    def time(self) -&gt; pd.arrays.DatetimeArray:\n        \"\"\"DatetimeArray: Column vector of time values across all sensors.\"\"\"\n        return pd.array(np.concatenate([sensor.time for sensor in self.values()]), dtype=\"datetime64[ns]\")\n\n    @property\n    def location(self) -&gt; Coordinate:\n        \"\"\"Coordinate: Coordinate object containing observation locations from all sensors in the group.\"\"\"\n        location_object = deepcopy(list(self.values())[0].location)\n        if isinstance(location_object, ENU):\n            attr_list = [\"east\", \"north\", \"up\"]\n        elif isinstance(location_object, LLA):\n            attr_list = [\"latitude\", \"longitude\", \"altitude\"]\n        elif isinstance(location_object, ECEF):\n            attr_list = [\"x\", \"y\", \"z\"]\n        else:\n            raise TypeError(\n                f\"Location object should be either ENU, LLA or ECEF, while currently it is{type(location_object)}\"\n            )\n        for attr in attr_list:\n            setattr(\n                location_object,\n                attr,\n                np.concatenate([np.array(getattr(sensor.location, attr), ndmin=1) for sensor in self.values()], axis=0),\n            )\n        return location_object\n\n    @property\n    def sensor_index(self) -&gt; np.ndarray:\n        \"\"\"np.ndarray: Column vector of integer indices linking concentration observation to a particular sensor.\"\"\"\n        return np.concatenate(\n            [np.ones(sensor.nof_observations, dtype=int) * i for i, sensor in enumerate(self.values())]\n        )\n\n    @property\n    def source_on(self) -&gt; np.ndarray:\n        \"\"\"Column vector of integers indicating whether sources are expected to be on, unwrapped over sensors.\n\n        Different integer values represent different sections where the source is on.\n\n        Assumes source is on when None is specified for a specific sensor.\n\n        Returns:\n            np.ndarray: Source on attribute, unwrapped over sensors.\n\n        \"\"\"\n        overall_idx = np.array([])\n        for curr_key in self.keys():\n            if self[curr_key].source_on is None:\n                temp_idx = np.ones(self[curr_key].nof_observations).astype(int)\n            else:\n                temp_idx = self[curr_key].source_on.flatten()\n\n            overall_idx = np.concatenate([overall_idx, temp_idx])\n        return overall_idx.astype(int)\n\n    @property\n    def nof_sensors(self) -&gt; int:\n        \"\"\"Int: Number of sensors contained in the SensorGroup.\"\"\"\n        return len(self)\n\n    def add_sensor(self, sensor: Sensor):\n        \"\"\"Add a sensor to the SensorGroup.\"\"\"\n        self[sensor.label] = sensor\n\n    def plot_sensor_location(self, fig: go.Figure, color_map: list = None) -&gt; go.Figure:\n        \"\"\"Plotting of the locations of all sensors in the SensorGroup.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n                the colors\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor location traces added to it\n\n        \"\"\"\n        if color_map is None:\n            color_map = self.color_map\n\n        for i, sensor in enumerate(self.values()):\n            color_idx = i % len(color_map)\n            fig = sensor.plot_sensor_location(fig, color=color_map[color_idx])\n\n        return fig\n\n    def plot_timeseries(self, fig: go.Figure, color_map: list = None, mode: str = \"markers\") -&gt; go.Figure:\n        \"\"\"Plotting of the concentration timeseries of all sensors in the SensorGroup.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n                the colors\n            mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor concentration time series traces added to it\n\n        \"\"\"\n        if color_map is None:\n            color_map = self.color_map\n\n        for i, sensor in enumerate(self.values()):\n            color_idx = i % len(color_map)\n            fig = sensor.plot_timeseries(fig, color=color_map[color_idx], mode=mode)\n\n        return fig\n\n    def subset_sensor(self, section_index: int) -&gt; \"SensorGroup\":\n        \"\"\"Subset the sensor based on the provided section index.\n\n        The method is designed to return a new `SensorGroup` object containing only the observations corresponding to a\n        specified section index. Sections are defined by unique values in the `sensor.source_on` attribute. For a case\n        where the source is turned on and off multiple times, (0 values in `sensor.source_on` indicate off periods and\n        positive integers indicate different on periods). For example, if section_index=1, a new SensorGroup will be\n        returned containing only observations where sensor.source_on == 1.\n        This functionality is useful for situations where data is collected in multiple sections, e.g. repeated on/off\n        releases where we want to work with one section at a time or later stitch multiple per-section segments\n        together.\n\n        Args:\n            section_index (int): An integer indicating which observations to keep.\n\n        Returns:\n            SensorGroup: A new SensorGroup object containing only the specified observations\n\n        \"\"\"\n        subset_sensor = SensorGroup()\n        for _, sensor in enumerate(self.values()):\n            subset_sensor_i = sensor.subset_sensor(section_index)\n            subset_sensor.add_sensor(subset_sensor_i)\n        return subset_sensor\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.concentration","title":"<code>concentration</code>  <code>property</code>","text":"<p>np.ndarray: Column vector of concentration values across all sensors, unwrapped per sensor.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.time","title":"<code>time</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.location","title":"<code>location</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.sensor_index","title":"<code>sensor_index</code>  <code>property</code>","text":"<p>np.ndarray: Column vector of integer indices linking concentration observation to a particular sensor.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.source_on","title":"<code>source_on</code>  <code>property</code>","text":"<p>Column vector of integers indicating whether sources are expected to be on, unwrapped over sensors.</p> <p>Different integer values represent different sections where the source is on.</p> <p>Assumes source is on when None is specified for a specific sensor.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Source on attribute, unwrapped over sensors.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.nof_sensors","title":"<code>nof_sensors</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.add_sensor","title":"<code>add_sensor(sensor)</code>","text":"<p>Add a sensor to the SensorGroup.</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def add_sensor(self, sensor: Sensor):\n    \"\"\"Add a sensor to the SensorGroup.\"\"\"\n    self[sensor.label] = sensor\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.plot_sensor_location","title":"<code>plot_sensor_location(fig, color_map=None)</code>","text":"<p>Plotting of the locations of all sensors in the SensorGroup.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color_map</code> <code>list</code> <p>When specified, the colormap to be used, plotting will cycle through the colors</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor location traces added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_sensor_location(self, fig: go.Figure, color_map: list = None) -&gt; go.Figure:\n    \"\"\"Plotting of the locations of all sensors in the SensorGroup.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n            the colors\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor location traces added to it\n\n    \"\"\"\n    if color_map is None:\n        color_map = self.color_map\n\n    for i, sensor in enumerate(self.values()):\n        color_idx = i % len(color_map)\n        fig = sensor.plot_sensor_location(fig, color=color_map[color_idx])\n\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.plot_timeseries","title":"<code>plot_timeseries(fig, color_map=None, mode='markers')</code>","text":"<p>Plotting of the concentration timeseries of all sensors in the SensorGroup.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color_map</code> <code>list</code> <p>When specified, the colormap to be used, plotting will cycle through the colors</p> <code>None</code> <code>mode</code> <code>str</code> <p>Mode used for plotting, i.e. markers, lines or markers+lines</p> <code>'markers'</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor concentration time series traces added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_timeseries(self, fig: go.Figure, color_map: list = None, mode: str = \"markers\") -&gt; go.Figure:\n    \"\"\"Plotting of the concentration timeseries of all sensors in the SensorGroup.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n            the colors\n        mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor concentration time series traces added to it\n\n    \"\"\"\n    if color_map is None:\n        color_map = self.color_map\n\n    for i, sensor in enumerate(self.values()):\n        color_idx = i % len(color_map)\n        fig = sensor.plot_timeseries(fig, color=color_map[color_idx], mode=mode)\n\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.subset_sensor","title":"<code>subset_sensor(section_index)</code>","text":"<p>Subset the sensor based on the provided section index.</p> <p>The method is designed to return a new <code>SensorGroup</code> object containing only the observations corresponding to a specified section index. Sections are defined by unique values in the <code>sensor.source_on</code> attribute. For a case where the source is turned on and off multiple times, (0 values in <code>sensor.source_on</code> indicate off periods and positive integers indicate different on periods). For example, if section_index=1, a new SensorGroup will be returned containing only observations where sensor.source_on == 1. This functionality is useful for situations where data is collected in multiple sections, e.g. repeated on/off releases where we want to work with one section at a time or later stitch multiple per-section segments together.</p> <p>Parameters:</p> Name Type Description Default <code>section_index</code> <code>int</code> <p>An integer indicating which observations to keep.</p> required <p>Returns:</p> Name Type Description <code>SensorGroup</code> <code>SensorGroup</code> <p>A new SensorGroup object containing only the specified observations</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def subset_sensor(self, section_index: int) -&gt; \"SensorGroup\":\n    \"\"\"Subset the sensor based on the provided section index.\n\n    The method is designed to return a new `SensorGroup` object containing only the observations corresponding to a\n    specified section index. Sections are defined by unique values in the `sensor.source_on` attribute. For a case\n    where the source is turned on and off multiple times, (0 values in `sensor.source_on` indicate off periods and\n    positive integers indicate different on periods). For example, if section_index=1, a new SensorGroup will be\n    returned containing only observations where sensor.source_on == 1.\n    This functionality is useful for situations where data is collected in multiple sections, e.g. repeated on/off\n    releases where we want to work with one section at a time or later stitch multiple per-section segments\n    together.\n\n    Args:\n        section_index (int): An integer indicating which observations to keep.\n\n    Returns:\n        SensorGroup: A new SensorGroup object containing only the specified observations\n\n    \"\"\"\n    subset_sensor = SensorGroup()\n    for _, sensor in enumerate(self.values()):\n        subset_sensor_i = sensor.subset_sensor(section_index)\n        subset_sensor.add_sensor(subset_sensor_i)\n    return subset_sensor\n</code></pre>"},{"location":"pyelq/support_functions/post_processing/","title":"Post Processing","text":""},{"location":"pyelq/support_functions/post_processing/#post-processing","title":"Post Processing","text":"<p>Post-processing module.</p> <p>Module containing some functions used in post-processing of the results.</p>"},{"location":"pyelq/support_functions/post_processing/#pyelq.support_functions.post_processing.is_regularly_spaced","title":"<code>is_regularly_spaced(array, tolerance=0.01, return_delta=True)</code>","text":"<p>Determines whether an input array is regularly spaced, within some (absolute) tolerance.</p> <p>Gets the large differences (defined by tolerance) in the array, and sees whether all of them are within 5% of one another.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to be analysed.</p> required <code>tolerance</code> <code>float</code> <p>Absolute value above which the difference between values is considered significant. Defaults to 0.01.</p> <code>0.01</code> <code>return_delta</code> <code>bool</code> <p>Whether to return the value of the regular grid spacing. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the grid is regularly spaced.</p> <code>float</code> <p>The value of the regular grid spacing.</p> Source code in <code>src/pyelq/support_functions/post_processing.py</code> <pre><code>def is_regularly_spaced(array: np.ndarray, tolerance: float = 0.01, return_delta: bool = True):\n    \"\"\"Determines whether an input array is regularly spaced, within some (absolute) tolerance.\n\n    Gets the large differences (defined by tolerance) in the array, and sees whether all of them are within 5% of one\n    another.\n\n    Args:\n        array (np.ndarray): Input array to be analysed.\n        tolerance (float, optional): Absolute value above which the difference between values is considered significant.\n            Defaults to 0.01.\n        return_delta (bool, optional): Whether to return the value of the regular grid spacing. Defaults to True.\n\n    Returns:\n        (bool): Whether the grid is regularly spaced.\n        (float): The value of the regular grid spacing.\n\n    \"\"\"\n    unique_vals = np.unique(array)\n    diff_unique_vals = np.diff(unique_vals)\n    diff_big = diff_unique_vals[diff_unique_vals &gt; tolerance]\n\n    boolean = np.all([np.isclose(diff_big[i], diff_big[i + 1], rtol=0.05) for i in range(len(diff_big) - 1)])\n\n    if return_delta:\n        return boolean, np.mean(diff_big)\n\n    return boolean, None\n</code></pre>"},{"location":"pyelq/support_functions/post_processing/#pyelq.support_functions.post_processing.calculate_rectangular_statistics","title":"<code>calculate_rectangular_statistics(emission_rates, source_locations, bin_size_x=1, bin_size_y=1, burn_in=0, normalized_count_limit=0.005)</code>","text":"<p>Function which aggregates the pyELQ results into rectangular bins and outputs the related summary statistics.</p> <p>The function creates a pixel grid (binning) in East-North coordinates based on the bin_size_x and bin_size_y parameters. For each bin both a count as well as a weighted sum of the emission rate estimates is calculated. The count is normalized by the number of iterations used in the MCMC and a boolean array is created which indicates if the count is above a certain threshold. Connected pixels where the count is above this threshold are considered to be a single blob/source and emission estimates per blob are summed over all pixels in the blob. The function then calculates the summary statistics for each blob of estimates which are connected pixels. The summary statistics include the median and IQR of the emission rate estimates, the mean location of the blob and the likelihood of the blob.</p> <p>Parameters:</p> Name Type Description Default <code>emission_rates</code> <code>ndarray</code> <p>and array of shape (number_of_sources, number_of_iterations)</p> required <code>source_locations</code> <code>ENU</code> <p>An object containing the east, north, and up coordinates of source locations,</p> required <code>bin_size_x</code> <code>float</code> <p>Size of the bins in the x-direction. Defaults to 1.</p> <code>1</code> <code>bin_size_y</code> <code>float</code> <p>Size of the bins in the y-direction. Defaults to 1.</p> <code>1</code> <code>burn_in</code> <code>int</code> <p>Number of burn-in iterations used in the MCMC. Defaults to 0.</p> <code>0</code> <code>normalized_count_limit</code> <code>float</code> <p>Threshold for the normalized count to be considered a blob.</p> <code>0.005</code> <p>Returns:</p> Name Type Description <code>result_weighted</code> <code>ndarray</code> <p>Weighted sum of the emission rate estimates in each bin.</p> <code>overall_count</code> <code>ndarray</code> <p>Count of the number of estimates in each bin.</p> <code>normalized_count</code> <code>ndarray</code> <p>Normalized count of the number of estimates in each bin.</p> <code>count_boolean</code> <code>ndarray</code> <p>Boolean array which indicates if likelihood of pixel is over threshold.</p> <code>edges_result</code> <code>list</code> <p>Centers of the pixels in the x and y direction.</p> <code>summary_result</code> <code>DataFrame</code> <p>Summary statistics for each blob of estimates.</p> Source code in <code>src/pyelq/support_functions/post_processing.py</code> <pre><code>def calculate_rectangular_statistics(\n    emission_rates: np.ndarray,\n    source_locations: ENU,\n    bin_size_x: float = 1,\n    bin_size_y: float = 1,\n    burn_in: int = 0,\n    normalized_count_limit: float = 0.005,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, list, pd.DataFrame]:\n    \"\"\"Function which aggregates the pyELQ results into rectangular bins and outputs the related summary statistics.\n\n    The function creates a pixel grid (binning) in East-North coordinates based on the bin_size_x and bin_size_y\n    parameters. For each bin both a count as well as a weighted sum of the emission rate estimates is calculated. The\n    count is normalized by the number of iterations used in the MCMC and a boolean array is created which indicates if\n    the count is above a certain threshold. Connected pixels where the count is above this threshold are considered to\n    be a single blob/source and emission estimates per blob are summed over all pixels in the blob. The function\n    then calculates the summary statistics for each blob of estimates which are connected pixels. The summary\n    statistics include the median and IQR of the emission rate estimates, the mean location of the blob and the\n    likelihood of the blob.\n\n    Args:\n        emission_rates (np.ndarray): and array of shape (number_of_sources, number_of_iterations)\n        containing emission rate estimates from the MCMC run.\n        source_locations (ENU): An object containing the east, north, and up coordinates of source locations,\n        as well as reference latitude, longitude, and altitude.\n        bin_size_x (float, optional): Size of the bins in the x-direction. Defaults to 1.\n        bin_size_y  (float, optional): Size of the bins in the y-direction. Defaults to 1.\n        burn_in (int, optional): Number of burn-in iterations used in the MCMC. Defaults to 0.\n        normalized_count_limit  (float, optional): Threshold for the normalized count to be considered a blob.\n\n    Returns:\n        result_weighted (np.ndarray): Weighted sum of the emission rate estimates in each bin.\n        overall_count (np.ndarray): Count of the number of estimates in each bin.\n        normalized_count (np.ndarray): Normalized count of the number of estimates in each bin.\n        count_boolean (np.ndarray): Boolean array which indicates if likelihood of pixel is over threshold.\n        edges_result (list): Centers of the pixels in the x and y direction.\n        summary_result (pd.DataFrame): Summary statistics for each blob of estimates.\n\n    \"\"\"\n    nof_iterations = emission_rates.shape[1]\n\n    if np.all(np.isnan(source_locations.east)):\n        warnings.warn(\"No sources found\")\n        result_weighted = np.array([[[np.nan]]])\n        overall_count = np.array([[0]])\n        normalized_count = np.array([[0]])\n        count_boolean = np.array([[False]])\n        edges_result = [np.array([np.nan])] * 2\n        summary_result = return_empty_summary_dataframe()\n\n        return result_weighted, overall_count, normalized_count, count_boolean, edges_result[:2], summary_result\n\n    min_x = np.nanmin(source_locations.east)\n    max_x = np.nanmax(source_locations.east)\n    min_y = np.nanmin(source_locations.north)\n    max_y = np.nanmax(source_locations.north)\n\n    bin_min_x = np.floor(min_x - 0.1)\n    bin_max_x = np.ceil(max_x + 0.1)\n    bin_min_y = np.floor(min_y - 0.1)\n    bin_max_y = np.ceil(max_y + 0.1)\n    bin_min_iteration = burn_in + 0.5\n    bin_max_iteration = nof_iterations + 0.5\n\n    max_nof_sources = source_locations.east.shape[0]\n\n    x_edges = np.arange(start=bin_min_x, stop=bin_max_x + bin_size_x, step=bin_size_x)\n    y_edges = np.arange(start=bin_min_y, stop=bin_max_y + bin_size_y, step=bin_size_y)\n    iteration_edges = np.arange(start=bin_min_iteration, stop=bin_max_iteration + bin_size_y, step=1)\n\n    result_x_vals = source_locations.east.flatten()\n    result_y_vals = source_locations.north.flatten()\n    result_z_vals = source_locations.up.flatten()\n\n    result_iteration_vals = np.array(range(nof_iterations)).reshape(1, -1) + 1\n    result_iteration_vals = np.tile(result_iteration_vals, (max_nof_sources, 1)).flatten()\n\n    results_estimates = emission_rates.flatten()\n\n    result_weighted, _ = np.histogramdd(\n        sample=np.array([result_x_vals, result_y_vals, result_iteration_vals]).T,\n        bins=[x_edges, y_edges, iteration_edges],\n        weights=results_estimates,\n        density=False,\n    )\n\n    count_result, edges_result = np.histogramdd(\n        sample=np.array([result_x_vals, result_y_vals, result_iteration_vals]).T,\n        bins=[x_edges, y_edges, iteration_edges],\n        density=False,\n    )\n\n    overall_count = np.array(np.sum(count_result, axis=2))\n    normalized_count = overall_count / (nof_iterations - burn_in)\n    count_boolean = normalized_count &gt;= normalized_count_limit\n\n    summary_result = create_aggregation(\n        result_iteration_vals=result_iteration_vals,\n        burn_in=burn_in,\n        result_x_vals=result_x_vals,\n        result_y_vals=result_y_vals,\n        result_z_vals=result_z_vals,\n        results_estimates=results_estimates,\n        count_boolean=count_boolean,\n        x_edges=x_edges,\n        y_edges=y_edges,\n        nof_iterations=nof_iterations,\n        ref_latitude=source_locations.ref_latitude,\n        ref_longitude=source_locations.ref_longitude,\n        ref_altitude=source_locations.ref_altitude,\n    )\n    return result_weighted, overall_count, normalized_count, count_boolean, edges_result[:2], summary_result\n</code></pre>"},{"location":"pyelq/support_functions/post_processing/#pyelq.support_functions.post_processing.create_lla_polygons_from_xy_points","title":"<code>create_lla_polygons_from_xy_points(points_array, ref_latitude, ref_longitude, ref_altitude, boolean_mask=None)</code>","text":"<p>Function to create polygons in LLA coordinates from a grid of points in ENU coordinates.</p> <p>This function takes a grid of East-North points, these points are used as center points for a pixel grid. The pixel grid is then converted to LLA coordinates and these center points are used to create a polygon in LLA coordinates. A polygon is only created if the boolean mask for that pixel is True. In case one unique East-North point is available, a predefined grid size of 1e-6 (equaling to 0.0036 seconds) is assumed.</p> <p>Parameters:</p> Name Type Description Default <code>points_array</code> <code>list[ndarray]</code> <p>List of arrays of grid of points in ENU coordinates.</p> required <code>ref_latitude</code> <code>float</code> <p>Reference latitude in degrees of ENU coordinate system.</p> required <code>ref_longitude</code> <code>float</code> <p>Reference longitude in degrees of ENU coordinate system.</p> required <code>ref_altitude</code> <code>float</code> <p>Reference altitude in meters of ENU coordinate system.</p> required <code>boolean_mask</code> <code>ndarray</code> <p>Boolean mask to indicate which pixels to create polygons for. Defaults to None which means all pixels are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Polygon]</code> <p>list[geometry.Polygon]: List of polygons in LLA coordinates</p> Source code in <code>src/pyelq/support_functions/post_processing.py</code> <pre><code>def create_lla_polygons_from_xy_points(\n    points_array: list[np.ndarray],\n    ref_latitude: float,\n    ref_longitude: float,\n    ref_altitude: float,\n    boolean_mask: Union[np.ndarray, None] = None,\n) -&gt; list[geometry.Polygon]:\n    \"\"\"Function to create polygons in LLA coordinates from a grid of points in ENU coordinates.\n\n    This function takes a grid of East-North points, these points are used as center points for a pixel grid. The pixel\n    grid is then converted to LLA coordinates and these center points are used to create a polygon in LLA coordinates.\n    A polygon is only created if the boolean mask for that pixel is True. In case one unique East-North point is\n    available, a predefined grid size of 1e-6 (equaling to 0.0036 seconds) is assumed.\n\n    Args:\n        points_array (list[np.ndarray]): List of arrays of grid of points in ENU coordinates.\n        ref_latitude (float): Reference latitude in degrees of ENU coordinate system.\n        ref_longitude (float): Reference longitude in degrees of ENU coordinate system.\n        ref_altitude (float): Reference altitude in meters of ENU coordinate system.\n        boolean_mask (np.ndarray, optional): Boolean mask to indicate which pixels to create polygons for.\n            Defaults to None which means all pixels are used.\n\n    Returns:\n        list[geometry.Polygon]: List of polygons in LLA coordinates\n    \"\"\"\n    if boolean_mask is None:\n        boolean_mask = np.ones_like(points_array, dtype=bool)\n\n    enu_x = points_array[0]\n    enu_x = enu_x[:-1] + np.diff(enu_x) / 2\n    enu_y = points_array[1]\n    enu_y = enu_y[:-1] + np.diff(enu_y) / 2\n\n    enu_x, enu_y = np.meshgrid(enu_x, enu_y, indexing=\"ij\")\n\n    enu_object_full_grid = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n    enu_object_full_grid.east = enu_x.flatten()\n    enu_object_full_grid.north = enu_y.flatten()\n    enu_object_full_grid.up = np.zeros_like(enu_object_full_grid.north)\n    lla_object_full_grid = enu_object_full_grid.to_lla()\n\n    _, gridsize_lat = is_regularly_spaced(lla_object_full_grid.latitude, tolerance=1e-6)\n    _, gridsize_lon = is_regularly_spaced(lla_object_full_grid.longitude, tolerance=1e-6)\n\n    if np.isnan(gridsize_lat):\n        gridsize_lat = 1e-6\n    if np.isnan(gridsize_lon):\n        gridsize_lon = 1e-6\n\n    polygons = [\n        geometry.box(\n            lla_object_full_grid.longitude[idx] - gridsize_lon / 2,\n            lla_object_full_grid.latitude[idx] - gridsize_lat / 2,\n            lla_object_full_grid.longitude[idx] + gridsize_lon / 2,\n            lla_object_full_grid.latitude[idx] + gridsize_lat / 2,\n        )\n        for idx in np.argwhere(boolean_mask.flatten()).flatten()\n    ]\n\n    return polygons\n</code></pre>"},{"location":"pyelq/support_functions/post_processing/#pyelq.support_functions.post_processing.create_aggregation","title":"<code>create_aggregation(result_x_vals, result_y_vals, result_z_vals, results_estimates, result_iteration_vals, count_boolean, x_edges, y_edges, nof_iterations, burn_in, ref_latitude, ref_longitude, ref_altitude)</code>","text":"<p>Function to create the aggregated information for the blobs of estimates.</p> <p>We identify all blobs of estimates which appear close together on the map by looking at connected pixels in the count_boolean array. Next we find the summary statistics for all estimates in that blob like overall median and IQR estimate, mean location and the likelihood of that blob.</p> <p>When multiple sources are present in the same blob at the same iteration we first sum those emission rate estimates before taking the median.</p> <p>If no blobs are found a dataframe with nan values is return to avoid breaking plotting code which calls this function.</p> <p>Parameters:</p> Name Type Description Default <code>result_x_vals</code> <code>ndarray</code> <p>X-coordinate of estimates, flattened array of (n_sources_max * nof_iterations,).</p> required <code>result_y_vals</code> <code>ndarray</code> <p>Y-coordinate of estimates, flattened array of (n_sources_max * nof_iterations,).</p> required <code>result_z_vals</code> <code>ndarray</code> <p>Z-coordinate of estimates, flattened array of (n_sources_max * nof_iterations,).</p> required <code>results_estimates</code> <code>ndarray</code> <p>Emission rate estimates, flattened array of (n_sources_max * nof_iterations,).</p> required <code>result_iteration_vals</code> <code>ndarray</code> <p>Iteration number corresponding each estimated value, flattened array of (n_sources_max * nof_iterations,).</p> required <code>count_boolean</code> <code>ndarray</code> <p>Boolean array which indicates if likelihood of pixel is over threshold.</p> required <code>x_edges</code> <code>ndarray</code> <p>Pixel edges x-coordinates.</p> required <code>y_edges</code> <code>ndarray</code> <p>Pixel edges y-coordinates.</p> required <code>nof_iterations</code> <code>int</code> <p>Number of iterations used in MCMC.</p> required <code>burn_in</code> <code>int</code> <p>Burn-in used in MCMC.</p> required <code>ref_latitude</code> <code>float</code> <p>Reference latitude in degrees of ENU coordinate system.</p> required <code>ref_longitude</code> <code>float</code> <p>Reference longitude in degrees of ENU coordinate system.</p> required <code>ref_altitude</code> <code>float</code> <p>Reference altitude in meters of ENU coordinate system.</p> required <p>Returns:</p> Name Type Description <code>summary_result</code> <code>DataFrame</code> <p>Summary statistics for each blob of estimates.</p> Source code in <code>src/pyelq/support_functions/post_processing.py</code> <pre><code>def create_aggregation(\n    result_x_vals: np.ndarray,\n    result_y_vals: np.ndarray,\n    result_z_vals: np.ndarray,\n    results_estimates: np.ndarray,\n    result_iteration_vals: np.ndarray,\n    count_boolean: np.ndarray,\n    x_edges: np.ndarray,\n    y_edges: np.ndarray,\n    nof_iterations: int,\n    burn_in: int,\n    ref_latitude: float,\n    ref_longitude: float,\n    ref_altitude: float,\n) -&gt; pd.DataFrame:\n    \"\"\"Function to create the aggregated information for the blobs of estimates.\n\n    We identify all blobs of estimates which appear close together on the map by looking at connected pixels in the\n    count_boolean array. Next we find the summary statistics for all estimates in that blob like overall median and\n    IQR estimate, mean location and the likelihood of that blob.\n\n    When multiple sources are present in the same blob at the same iteration we first sum those emission rate\n    estimates before taking the median.\n\n    If no blobs are found a dataframe with nan values is return to avoid breaking plotting code which calls this\n    function.\n\n    Args:\n        result_x_vals (np.ndarray): X-coordinate of estimates, flattened array of (n_sources_max * nof_iterations,).\n        result_y_vals (np.ndarray): Y-coordinate of estimates, flattened array of (n_sources_max * nof_iterations,).\n        result_z_vals (np.ndarray): Z-coordinate of estimates, flattened array of (n_sources_max * nof_iterations,).\n        results_estimates (np.ndarray): Emission rate estimates, flattened array of\n            (n_sources_max * nof_iterations,).\n        result_iteration_vals (np.ndarray): Iteration number corresponding each estimated value, flattened array\n            of (n_sources_max * nof_iterations,).\n        count_boolean (np.ndarray): Boolean array which indicates if likelihood of pixel is over threshold.\n        x_edges (np.ndarray): Pixel edges x-coordinates.\n        y_edges (np.ndarray): Pixel edges y-coordinates.\n        nof_iterations (int): Number of iterations used in MCMC.\n        burn_in (int): Burn-in used in MCMC.\n        ref_latitude (float): Reference latitude in degrees of ENU coordinate system.\n        ref_longitude (float): Reference longitude in degrees of ENU coordinate system.\n        ref_altitude (float): Reference altitude in meters of ENU coordinate system.\n\n    Returns:\n        summary_result (pd.DataFrame): Summary statistics for each blob of estimates.\n\n    \"\"\"\n    labeled_array, num_features = label(input=count_boolean, structure=np.ones((3, 3)))\n\n    if num_features == 0:\n        summary_result = return_empty_summary_dataframe()\n        return summary_result\n\n    burn_in_bool = result_iteration_vals &gt; burn_in\n    nan_x_vals = np.isnan(result_x_vals)\n    nan_y_vals = np.isnan(result_y_vals)\n    nan_z_vals = np.isnan(result_z_vals)\n    no_nan_idx = np.logical_not(np.logical_or(np.logical_or(nan_x_vals, nan_y_vals), nan_z_vals))\n    no_nan_and_burn_in_bool = np.logical_and(no_nan_idx, burn_in_bool)\n    result_x_vals_no_nan = result_x_vals[no_nan_and_burn_in_bool]\n    result_y_vals_no_nan = result_y_vals[no_nan_and_burn_in_bool]\n    result_z_vals_no_nan = result_z_vals[no_nan_and_burn_in_bool]\n    results_estimates_no_nan = results_estimates[no_nan_and_burn_in_bool]\n    result_iteration_vals_no_nan = result_iteration_vals[no_nan_and_burn_in_bool]\n\n    x_idx = np.digitize(result_x_vals_no_nan, x_edges, right=False) - 1\n    y_idx = np.digitize(result_y_vals_no_nan, y_edges, right=False) - 1\n    bin_numbers = np.ravel_multi_index((x_idx, y_idx), labeled_array.shape)\n\n    bin_numbers_per_label = [\n        np.ravel_multi_index(np.nonzero(labeled_array == value), labeled_array.shape)\n        for value in np.array(range(num_features)) + 1\n    ]\n\n    summary_result = pd.DataFrame()\n    summary_result.index.name = \"source_ID\"\n\n    for label_idx, curr_bins in enumerate(bin_numbers_per_label):\n        boolean_for_result = np.isin(bin_numbers, curr_bins)\n        mean_x = np.mean(result_x_vals_no_nan[boolean_for_result])\n        mean_y = np.mean(result_y_vals_no_nan[boolean_for_result])\n        mean_z = np.mean(result_z_vals_no_nan[boolean_for_result])\n\n        unique_iteration_vals, indices, counts = np.unique(\n            result_iteration_vals_no_nan[boolean_for_result], return_inverse=True, return_counts=True\n        )\n        nof_iterations_present = unique_iteration_vals.size\n        blob_likelihood = nof_iterations_present / (nof_iterations - burn_in)\n        single_idx = np.argwhere(counts == 1)\n        results_estimates_for_blob = results_estimates_no_nan[boolean_for_result]\n        temp_estimate_result = results_estimates_for_blob[indices[single_idx.flatten()]]\n        multiple_idx = np.argwhere(counts &gt; 1)\n        for single_idx in multiple_idx:\n            temp_val = np.sum(results_estimates_for_blob[indices == single_idx])\n            temp_estimate_result = np.append(temp_estimate_result, temp_val)\n\n        median_estimate = np.median(temp_estimate_result)\n        iqr_estimate = np.nanquantile(a=temp_estimate_result, q=0.75) - np.nanquantile(a=temp_estimate_result, q=0.25)\n        lower_bound = np.nanquantile(a=temp_estimate_result, q=0.025)\n        upper_bound = np.nanquantile(a=temp_estimate_result, q=0.975)\n        enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n        enu_object.east = mean_x\n        enu_object.north = mean_y\n        enu_object.up = mean_z\n        lla_object = enu_object.to_lla()\n\n        summary_result.loc[label_idx, \"latitude\"] = lla_object.latitude\n        summary_result.loc[label_idx, \"longitude\"] = lla_object.longitude\n        summary_result.loc[label_idx, \"altitude\"] = lla_object.altitude\n        summary_result.loc[label_idx, \"height\"] = mean_z\n        summary_result.loc[label_idx, \"median_estimate\"] = median_estimate\n        summary_result.loc[label_idx, \"quantile_025\"] = lower_bound\n        summary_result.loc[label_idx, \"quantile_975\"] = upper_bound\n        summary_result.loc[label_idx, \"iqr_estimate\"] = iqr_estimate\n        summary_result.loc[label_idx, \"absolute_count_iterations\"] = nof_iterations_present\n        summary_result.loc[label_idx, \"blob_likelihood\"] = blob_likelihood\n\n    summary_result = summary_result.astype({\"absolute_count_iterations\": \"int\"})\n\n    return summary_result\n</code></pre>"},{"location":"pyelq/support_functions/post_processing/#pyelq.support_functions.post_processing.return_empty_summary_dataframe","title":"<code>return_empty_summary_dataframe()</code>","text":"<p>Helper function to create and return an empty summary dataframe with predifined columns.</p> Source code in <code>src/pyelq/support_functions/post_processing.py</code> <pre><code>def return_empty_summary_dataframe() -&gt; pd.DataFrame:\n    \"\"\"Helper function to create and return an empty summary dataframe with predifined columns.\"\"\"\n    summary_result = pd.DataFrame()\n    summary_result.index.name = \"source_ID\"\n    summary_result.loc[0, \"latitude\"] = np.nan\n    summary_result.loc[0, \"longitude\"] = np.nan\n    summary_result.loc[0, \"altitude\"] = np.nan\n    summary_result.loc[0, \"height\"] = np.nan\n    summary_result.loc[0, \"median_estimate\"] = np.nan\n    summary_result.loc[0, \"quantile_025\"] = np.nan\n    summary_result.loc[0, \"quantile_975\"] = np.nan\n    summary_result.loc[0, \"iqr_estimate\"] = np.nan\n    summary_result.loc[0, \"absolute_count_iterations\"] = np.nan\n    summary_result.loc[0, \"blob_likelihood\"] = np.nan\n    return summary_result\n</code></pre>"},{"location":"pyelq/support_functions/spatio_temporal_interpolation/","title":"Spatio Temporal Interpolation","text":""},{"location":"pyelq/support_functions/spatio_temporal_interpolation/#spatio-temporal-interpolation","title":"Spatio-Temporal Interpolation","text":"<p>Spatio-temporal interpolation module.</p> <p>Support function to perform interpolation in various ways</p>"},{"location":"pyelq/support_functions/spatio_temporal_interpolation/#pyelq.support_functions.spatio_temporal_interpolation.interpolate","title":"<code>interpolate(location_in=None, time_in=None, values_in=None, location_out=None, time_out=None, **kwargs)</code>","text":"<p>Interpolates data based on input.</p> <p>Interpolation using scipy.griddata function. Which in turn uses linear barycentric interpolation.</p> <p>It is assumed that the shape of location_in, time_in and values_in is consistent</p> <p>When time_out has the same size as number of rows of location_out, it is assumed these are aligned and be treated as consistent, hence the output will be a column vector. If this is not the case an interpolation will be performed for all combinations of rows in location out with times of time_out and output wil be shaped as [nof_location_values x dimension]</p> <p>If location_out == None, we only perform temporal (1D) interpolation. If time_out == None we only perform spatial  interpolation</p> <p>If linear interpolation is not possible for spatio or spatiotemporal interpolation, we use nearest neighbor interpolation, a warning will be displayed</p> <p>Parameters:</p> Name Type Description Default <code>location_in</code> <code>ndarray</code> <p>Array of size [nof_values x dimension] with locations to interpolate from</p> <code>None</code> <code>time_in</code> <code>Union[ndarray, DatetimeArray]</code> <p>Array of size [nof_values x 1] with timestamps or some form of time values (seconds) to interpolate from</p> <code>None</code> <code>values_in</code> <code>ndarray</code> <p>Array of size [nof_values x 1] with values to interpolate from</p> <code>None</code> <code>location_out</code> <code>ndarray</code> <p>Array of size [nof_location_values x dimension] with locations to interpolate to</p> <code>None</code> <code>time_out</code> <code>Union[ndarray, DatetimeArray]</code> <p>Array of size [nof_time_values x 1] with timestamps or some form of time values (seconds) to interpolate to</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Other keyword arguments which get passed into the griddata interpolation function</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>result</code> <code>ndarray</code> <p>Array of size [nof_location_values x nof_time_values] with interpolated values</p> Source code in <code>src/pyelq/support_functions/spatio_temporal_interpolation.py</code> <pre><code>def interpolate(\n    location_in: np.ndarray = None,\n    time_in: Union[np.ndarray, pd.arrays.DatetimeArray] = None,\n    values_in: np.ndarray = None,\n    location_out: np.ndarray = None,\n    time_out: Union[np.ndarray, pd.arrays.DatetimeArray] = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Interpolates data based on input.\n\n    Interpolation using scipy.griddata function. Which in turn uses linear barycentric interpolation.\n\n    It is assumed that the shape of location_in, time_in and values_in is consistent\n\n    When time_out has the same size as number of rows of location_out, it is assumed these are aligned and be treated as\n    consistent, hence the output will be a column vector.\n    If this is not the case an interpolation will be performed for all combinations of rows in location out with times\n    of time_out and output wil be shaped as [nof_location_values x dimension]\n\n    If location_out == None, we only perform temporal (1D) interpolation.\n    If time_out == None we only perform spatial  interpolation\n\n    If linear interpolation is not possible for spatio or spatiotemporal interpolation, we use nearest neighbor\n    interpolation, a warning will be displayed\n\n    Args:\n        location_in (np.ndarray): Array of size [nof_values x dimension] with locations to interpolate from\n        time_in (Union[np.ndarray, pd.arrays.DatetimeArray]): Array of size [nof_values x 1] with timestamps or some\n            form of time values (seconds) to interpolate from\n        values_in (np.ndarray): Array of size [nof_values x 1] with values to interpolate from\n        location_out (np.ndarray): Array of size [nof_location_values x dimension] with locations to interpolate to\n        time_out (Union[np.ndarray, pd.arrays.DatetimeArray]): Array of size [nof_time_values x 1] with\n            timestamps or some form of time values (seconds) to interpolate to\n        **kwargs (dict): Other keyword arguments which get passed into the griddata interpolation function\n\n    Returns:\n        result (np.ndarray): Array of size [nof_location_values x nof_time_values] with interpolated values\n\n    \"\"\"\n    _sense_check_interpolate_inputs(\n        location_in=location_in, time_in=time_in, values_in=values_in, location_out=location_out, time_out=time_out\n    )\n\n    if (\n        time_out is not None\n        and isinstance(time_out, pd.arrays.DatetimeArray)\n        and isinstance(time_in, pd.arrays.DatetimeArray)\n    ):\n        min_time_out = np.amin(time_out)\n        time_out = (time_out - min_time_out).total_seconds()\n        time_in = (time_in - min_time_out).total_seconds()\n\n    if location_out is None:\n        return _griddata(points_in=time_in, values=values_in, points_out=time_out, **kwargs)\n\n    if time_out is None:\n        return _griddata(points_in=location_in, values=values_in, points_out=location_out, **kwargs)\n\n    if location_in.shape[0] != time_in.size:\n        raise ValueError(\"Location and time are do not have consistent sizes\")\n\n    if location_out.shape[0] != time_out.size:\n        location_temp = np.tile(location_out, (time_out.size, 1))\n        time_temp = np.repeat(time_out.squeeze(), location_out.shape[0])\n        out_array = np.column_stack((location_temp, time_temp))\n    else:\n        out_array = np.column_stack((location_out, time_out))\n\n    in_array = np.column_stack((location_in, time_in))\n\n    result = _griddata(points_in=in_array, values=values_in, points_out=out_array, **kwargs)\n\n    if location_out.shape[0] != time_out.size:\n        result = result.reshape((location_out.shape[0], time_out.size), order=\"C\")\n\n    return result\n</code></pre>"},{"location":"pyelq/support_functions/spatio_temporal_interpolation/#pyelq.support_functions.spatio_temporal_interpolation.temporal_resampling","title":"<code>temporal_resampling(time_in, values_in, time_bin_edges, aggregate_function='mean', side='center')</code>","text":"<p>Resamples data into a set of time bins.</p> <p>Checks which values of time_in are withing 2 consecutive values of time_bin_edges and performs the aggregate function on the corresponding values from values_in. time_in values outside the time_bin_edges are ignored. Empty bins will be assigned a 'NaN' value.</p> <p>When 'time_in' is a sequence of time stamps, a DatetimeArray should be used. Otherwise, a np.ndarray should be used.</p> <p>Parameters:</p> Name Type Description Default <code>time_in</code> <code>Union[ndarray, DatetimeArray]</code> <p>A vector of times which correspond to values_in.</p> required <code>values_in</code> <code>ndarray</code> <p>A vector of the values to be resampled.</p> required <code>time_bin_edges</code> <code>Union[ndarray, DatetimeArray]</code> <p>A vector of times which define the edges of the                                                          bins into which the data will be resampled.</p> required <code>aggregate_function</code> <code>str</code> <p>The function which is used to aggregate the data after it has been                                 sorted into bins. Defaults to mean.</p> <code>'mean'</code> <code>side</code> <code>str</code> <p>Which side of the time bins should be used to generate times_out. Possible values are:                   'left', 'center', and 'right'. Defaults to 'center'.</p> <code>'center'</code> <p>Returns:</p> Name Type Description <code>time_out</code> <code>Union[ndarray, DatetimeArray]</code> <p>Vector-like object containing the times of the resampled                                                    values consistent with time_in dtype and side input                                                    argument.</p> <code>values_out</code> <code>ndarray</code> <p>A vector of resampled values, according to the time bins and the aggregate function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input arguments are not of the correct type or shape, this error is raised.</p> Source code in <code>src/pyelq/support_functions/spatio_temporal_interpolation.py</code> <pre><code>def temporal_resampling(\n    time_in: Union[np.ndarray, pd.arrays.DatetimeArray],\n    values_in: np.ndarray,\n    time_bin_edges: Union[np.ndarray, pd.arrays.DatetimeArray],\n    aggregate_function: str = \"mean\",\n    side: str = \"center\",\n) -&gt; Tuple[Union[np.ndarray, pd.arrays.DatetimeArray], np.ndarray]:\n    \"\"\"Resamples data into a set of time bins.\n\n    Checks which values of time_in are withing 2 consecutive values of time_bin_edges and performs the aggregate\n    function on the corresponding values from values_in. time_in values outside the time_bin_edges are ignored.\n    Empty bins will be assigned a 'NaN' value.\n\n    When 'time_in' is a sequence of time stamps, a DatetimeArray should be used. Otherwise, a np.ndarray should be used.\n\n    Args:\n        time_in (Union[np.ndarray, pd.arrays.DatetimeArray]): A vector of times which correspond to values_in.\n        values_in (np.ndarray): A vector of the values to be resampled.\n        time_bin_edges (Union[np.ndarray, pd.arrays.DatetimeArray]): A vector of times which define the edges of the\n                                                                     bins into which the data will be resampled.\n        aggregate_function (str, optional): The function which is used to aggregate the data after it has been\n                                            sorted into bins. Defaults to mean.\n        side (str, optional): Which side of the time bins should be used to generate times_out. Possible values are:\n                              'left', 'center', and 'right'. Defaults to 'center'.\n\n    Returns:\n        time_out (Union[np.ndarray, pd.arrays.DatetimeArray]): Vector-like object containing the times of the resampled\n                                                               values consistent with time_in dtype and side input\n                                                               argument.\n        values_out (np.ndarray): A vector of resampled values, according to the time bins and the aggregate function.\n\n    Raises:\n        ValueError: If any of the input arguments are not of the correct type or shape, this error is raised.\n\n    \"\"\"\n    if not isinstance(time_bin_edges, type(time_in)) or values_in.size != time_in.size:\n        raise ValueError(\"Arguments 'time_in', 'time_bin_edges' and/or 'values_in' are not of consistent type or size.\")\n\n    if not isinstance(aggregate_function, str):\n        raise ValueError(\"The supplied 'aggregate_function' is not a string.\")\n\n    if side == \"center\":\n        time_out = np.diff(time_bin_edges) / 2 + time_bin_edges[:-1]\n    elif side == \"left\":\n        time_out = time_bin_edges[:-1]\n    elif side == \"right\":\n        time_out = time_bin_edges[1:]\n    else:\n        raise ValueError(f\"The 'side' argument must be 'left', 'center', or 'right', but received '{side}'.\")\n\n    zero_value = 0\n    if isinstance(time_bin_edges, pd.arrays.DatetimeArray):\n        zero_value = np.array(0).astype(\"&lt;m8[ns]\")\n\n    if not np.all(np.diff(time_bin_edges) &gt; zero_value):\n        raise ValueError(\"Argument 'time_bin_edges' does not monotonically increase.\")\n\n    if np.any(time_in &lt; time_bin_edges[0]) or np.any(time_in &gt; time_bin_edges[-1]):\n        warnings.warn(\"Values in time_in are outside of range of time_bin_edges. These values will be ignored.\")\n\n    index = np.searchsorted(time_bin_edges, time_in, side=\"left\")\n    grouped_vals = pd.Series(values_in).groupby(index).agg(aggregate_function)\n    grouped_vals = grouped_vals.drop(index=[0, time_bin_edges.size], errors=\"ignore\").sort_index()\n\n    values_out = np.full(time_out.shape, np.nan)\n    values_out[grouped_vals.index - 1] = grouped_vals.to_numpy()\n\n    return time_out, values_out\n</code></pre>"}]}
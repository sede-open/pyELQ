{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyelq","title":"pyELQ","text":"<p>This repository contains the Python Emission Localization and Quantification software we call pyELQ. It is code used  for gas dispersion modelling, in particular methane emissions detection, localization and quantification. </p>"},{"location":"#background","title":"Background","text":"<p>The python Emission Localization and Quantification (pyELQ) code aims to maximize effective use of  existing measurement data, especially from continuous monitoring solutions. The code has been developed to detect,  localize, and quantify methane emissions from concentration and wind measurements. It can be used in combination with  point or beam sensors that are placed strategically on an area of interest.</p> <p>The algorithms in the pyELQ code are based a Bayesian statistics framework. pyELQ can ingest long-term concentration  and wind data, and it performs an inversion to predict the likely strengths and locations of persistent methane sources.  The goal is to arrive at a plausible estimate of methane emissions from an area of interest that matches the measured  data. The predictions from pyELQ come with uncertainty ranges that are representative of probability density functions  sampled by a Markov Chain Monte Carlo method. Time series of varying length can be processed by pyELQ: in general,  the Bayesian inversion leads to a more constrained solution if more high-precision measurement data is available.  We have tested our code under controlled conditions as well as in operating oil and gas facilities.</p> <p>The information on the strength and the approximate location of methane emission sources provided by pyELQ can help  operators with more efficient identification and quantification of (unexpected) methane sources, in order to start  appropriate mitigating actions accordingly. The pyELQ code is being made available in an open-source environment,  to support various assets in their quest to reduce methane emissions.</p> <p>Use cases where the pyELQ code has been applied are described in the following papers:</p> <ul> <li> <p>IJzermans, R., Jones, M., Weidmann, D. et al. \"Long-term continuous monitoring of methane emissions at an oil and gas facility using a multi-open-path laser dispersion spectrometer.\" Sci Rep 14, 623 (2024). (https://doi.org/10.1038/s41598-023-50081-9)</p> </li> <li> <p>Weidmann, D., Hirst, B. et al. \"Locating and Quantifying Methane Emissions by Inverse Analysis of Path-Integrated Concentration Data Using a Markov-Chain Monte Carlo Approach.\" ACS Earth and Space Chemistry 2022 6 (9), 2190-2198  (https://doi.org/10.1021/acsearthspacechem.2c00093)</p> </li> </ul>"},{"location":"#deployment-design","title":"Deployment design","text":"<p>The pyELQ code needs high-quality methane concentration and wind data to be able to provide reliable output on location  and quantification of methane emission sources. This requires methane concentration sensors of sufficiently high  precision in a layout that allows the detection of relevant methane emission sources, in combination with wind  measurements of high enough frequency and accuracy. The optimal sensor layout typically depends on the prevailing  meteorological conditions at the site of interest and requires multiple concentration sensors to cover the site under  different wind directions. </p>"},{"location":"#pyelq-data-interpretation","title":"pyELQ data interpretation","text":"<p>The results from pyELQ come with uncertainty ranges that are representative of probability density functions sampled  by a Markov Chain Monte Carlo method. One should take these uncertainty ranges into account when interpreting the pyELQ  output data. Remember that absence of evidence for methane emissions does not always imply evidence for absence of  methane emissions; for instance, when meteorological conditions are such that there is no sensor downwind of a methane  source during the selected monitoring period, then it will be impossible to detect, localize and quantify  this particular source.  Also, there are limitations to the forward dispersion model which is used in the analysis.  For example, the performance of the Gaussian plume dispersion model will degrade at lower wind speeds.  Therefore, careful interpretation of the data is always required. </p>"},{"location":"#installing-pyelq-as-a-package","title":"Installing pyELQ as a package","text":"<p>Suppose you want to use this pyELQ package in a different project. You can install it from PyPi through pip  <code>pip install pyelq</code>. Or you could clone the repository and install it from the source code.  After activating the environment you want to install pyELQ in, open a terminal, move to the main pyELQ folder where pyproject.toml is located and run <code>pip install .</code>, optionally you can pass the <code>-e</code> flag is for editable mode. All the main options, info and settings for the package are found in the pyproject.toml file which sits in this repo as well.</p>"},{"location":"#examples","title":"Examples","text":"<p>For some examples on how to use this package please check out these Examples</p>"},{"location":"#contribution","title":"Contribution","text":"<p>This project welcomes contributions and suggestions. If you have a suggestion that would make this better you can simply open an issue with a relevant title. Don't forget to give the project a star! Thanks again!</p> <p>For more details on contributing to this repository, see the Contributing guide.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>Distributed under the Apache License Version 2.0. See the license file for more information.</p>"},{"location":"pyelq/coordinate_system/","title":"Coordinate System","text":""},{"location":"pyelq/coordinate_system/#coordinate-system","title":"Coordinate System","text":"<p>Coordinate System.</p> <p>This code provides the definition of, and the functionality for, all the main coordinate systems that are used in pyELQ. Each coordinate system has relevant methods for features that are commonly required. Also provided is a set of conversions between each of the systems, alongside some functionality for interpolation.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate","title":"<code>Coordinate</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for coordinate transformations.</p> <p>Attributes:</p> Name Type Description <code>use_degrees</code> <code>bool</code> <p>Flag if reference uses degrees (True) or radians (False). Defaults to True.</p> <code>ellipsoid</code> <code>Ellipsoid</code> <p>Definition of the Ellipsoid used in the coordinate system, for which the default is WGS84. See: https://en.wikipedia.org/wiki/World_Geodetic_System.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass Coordinate(ABC):\n    \"\"\"Abstract base class for coordinate transformations.\n\n    Attributes:\n        use_degrees (bool): Flag if reference uses degrees (True) or radians (False). Defaults to True.\n        ellipsoid (pm.Ellipsoid): Definition of the Ellipsoid used in the coordinate system, for which the default is\n            WGS84. See: https://en.wikipedia.org/wiki/World_Geodetic_System.\n\n    \"\"\"\n\n    use_degrees: bool = field(init=False)\n    ellipsoid: pm.Ellipsoid = field(init=False)\n\n    def __post_init__(self):\n        self.use_degrees = True\n        self.ellipsoid = pm.Ellipsoid.from_name(\"wgs84\")\n\n    @property\n    @abstractmethod\n    def nof_observations(self) -&gt; int:\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n\n    @abstractmethod\n    def from_array(self, array: np.ndarray) -&gt; None:\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n\n    @abstractmethod\n    def to_array(self, dim: int = 3) -&gt; np.ndarray:\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            np.ndarray: Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n\n    @abstractmethod\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n\n    @abstractmethod\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n\n    @abstractmethod\n    def to_enu(self, ref_latitude: float = None, ref_longitude: float = None, ref_altitude: float = None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n\n    def to_object_type(self, coordinate_object):\n        \"\"\"Converts current object to same class as input coordinate_object.\n\n        Args:\n            coordinate_object (Coordinate): An coordinate object which provides the coordinate system to convert self to\n\n        Returns:\n            (Coordinate): The converted coordinate object\n\n        \"\"\"\n        if type(coordinate_object) is not type(self):\n            if isinstance(coordinate_object, LLA):\n                temp_object = self.to_lla()\n            elif isinstance(coordinate_object, ENU):\n                temp_object = self.to_enu(\n                    ref_latitude=coordinate_object.ref_latitude,\n                    ref_longitude=coordinate_object.ref_longitude,\n                    ref_altitude=coordinate_object.ref_altitude,\n                )\n            elif isinstance(coordinate_object, ECEF):\n                temp_object = self.to_ecef()\n            else:\n                raise TypeError(\"Please provide a valid coordinate type\")\n\n            return temp_object\n\n        return self\n\n    def interpolate(self, values: np.ndarray, locations, dim: int = 3, **kwargs) -&gt; np.ndarray:\n        \"\"\"Interpolate data using coordinate object.\n\n        If locations coordinate system does not match self's coordinate system it will be converted to same type as\n        self. In the ENU case extra checking needs to take place to check reference locations match up.\n\n        If only 1 value is provided which needs to be interpolated to many other locations we just set the value at all\n        these locations to the single input value\n\n        Args:\n            values (np.ndarray): Values to interpolate,  consistent with location in self\n            locations (Coordinate): Coordinate object containing locations to which you want to interpolate\n            dim (int): Number of dimensions to use for interpolation (2 or 3)\n            **kwargs (dict):  Other arguments available in scipy.interpolate.griddata e.g. method, fill_value\n\n        Returns:\n            Result (np.ndarray): Interpolated values at requested locations.\n\n        \"\"\"\n        locations = locations.to_object_type(coordinate_object=self)\n\n        if isinstance(self, ENU):\n            if (\n                self.ref_latitude != locations.ref_latitude\n                or self.ref_longitude != locations.ref_longitude\n                or self.ref_altitude != locations.ref_altitude\n            ):\n                locations = locations.to_lla()\n                locations = locations.to_enu(\n                    ref_latitude=self.ref_latitude, ref_longitude=self.ref_longitude, ref_altitude=self.ref_altitude\n                )\n        result = sti.interpolate(\n            location_in=self.to_array(dim),\n            values_in=values.flatten(),\n            location_out=locations.to_array(dim=dim),\n            **kwargs,\n        )\n\n        return result\n\n    def make_grid(\n        self, bounds: np.ndarray, grid_type: str = \"rectangular\", shape: Union[tuple, np.ndarray] = (5, 5, 1)\n    ) -&gt; np.ndarray:\n        \"\"\"Generates grid of values locations based on specified inputs.\n\n        If the grid type is 'spherical', we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the\n        use in temp_lat_rad and temp_lon_rad.\n\n        Args:\n            bounds (np.ndarray): Limits of the grid on which to generate the grid of size [dim x 2]\n                if dim == 2 we assume the third dimension will be zeros\n            grid_type (str, optional): Type of grid to generate, default 'rectangular':\n                     rectangular == rectangular grid of shape grd_shape,\n                     spherical == grid of shape grid_shape taking into account a spherical spacing\n            shape: (tuple, optional): Number of grid cells to generate in each dimension, total number of\n                grid cells will be the product of the entries of this tuple\n\n        Returns\n            np.ndarray: gridded of locations\n\n        \"\"\"\n        dimension = bounds.shape[0]\n\n        if grid_type == \"rectangular\":\n            dim_0 = np.linspace(bounds[0, 0], bounds[0, 1], num=shape[0])\n            dim_1 = np.linspace(bounds[1, 0], bounds[1, 1], num=shape[1])\n            if dimension == 3:\n                dim_2 = np.linspace(bounds[2, 0], bounds[2, 1], num=shape[2])\n            else:\n                dim_2 = np.array(0)\n\n            dim_0, dim_1, dim_2 = np.meshgrid(dim_0, dim_1, dim_2)\n            array = np.stack([dim_0.flatten(), dim_1.flatten(), dim_2.flatten()], axis=1)\n        elif grid_type == \"spherical\":\n            temp_object = deepcopy(self)\n            temp_object.from_array(array=bounds)\n            temp_object = temp_object.to_lla()\n            temp_object.latitude = (temp_object.latitude - (-90)) / 180\n            temp_object.longitude = (temp_object.longitude - (-180)) / 360\n\n            temp_lat_rad = np.linspace(start=temp_object.latitude[0], stop=temp_object.latitude[1], num=shape[0])\n            temp_lon_rad = np.linspace(start=temp_object.longitude[0], stop=temp_object.longitude[1], num=shape[1])\n\n            longitude = (2 * np.pi * temp_lon_rad - np.pi) * 180 / np.pi\n            latitude = (np.arccos(1 - 2 * temp_lat_rad) - 0.5 * np.pi) * 180 / np.pi\n            if dimension == 3:\n                altitude = np.linspace(start=temp_object.altitude[0], stop=temp_object.altitude[1], num=shape[2])\n                latitude, longitude, altitude = np.meshgrid(latitude, longitude, altitude)\n                array = np.stack(\n                    [latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180, altitude.flatten()], axis=1\n                )\n            else:\n                latitude, longitude = np.meshgrid(latitude, longitude)\n                array = np.stack([latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180], axis=1)\n\n            temp_object.from_array(array=array)\n            temp_object = temp_object.to_object_type(self)\n            array = temp_object.to_array()\n        else:\n            raise NotImplementedError(\"Please provide a valid grid type\")\n\n        return array\n\n    def create_tree(self) -&gt; KDTree:\n        \"\"\"Create KD tree for the purpose of fast distance computation.\n\n        Returns:\n                KDTree: Spatial KD tree\n\n        \"\"\"\n        return KDTree(self.to_array())\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.nof_observations","title":"<code>nof_observations</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.from_array","title":"<code>from_array(array)</code>  <code>abstractmethod</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef from_array(self, array: np.ndarray) -&gt; None:\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_array","title":"<code>to_array(dim=3)</code>  <code>abstractmethod</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_array(self, dim: int = 3) -&gt; np.ndarray:\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        np.ndarray: Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_lla","title":"<code>to_lla()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_ecef","title":"<code>to_ecef()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>  <code>abstractmethod</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@abstractmethod\ndef to_enu(self, ref_latitude: float = None, ref_longitude: float = None, ref_altitude: float = None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.to_object_type","title":"<code>to_object_type(coordinate_object)</code>","text":"<p>Converts current object to same class as input coordinate_object.</p> <p>Parameters:</p> Name Type Description Default <code>coordinate_object</code> <code>Coordinate</code> <p>An coordinate object which provides the coordinate system to convert self to</p> required <p>Returns:</p> Type Description <code>Coordinate</code> <p>The converted coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_object_type(self, coordinate_object):\n    \"\"\"Converts current object to same class as input coordinate_object.\n\n    Args:\n        coordinate_object (Coordinate): An coordinate object which provides the coordinate system to convert self to\n\n    Returns:\n        (Coordinate): The converted coordinate object\n\n    \"\"\"\n    if type(coordinate_object) is not type(self):\n        if isinstance(coordinate_object, LLA):\n            temp_object = self.to_lla()\n        elif isinstance(coordinate_object, ENU):\n            temp_object = self.to_enu(\n                ref_latitude=coordinate_object.ref_latitude,\n                ref_longitude=coordinate_object.ref_longitude,\n                ref_altitude=coordinate_object.ref_altitude,\n            )\n        elif isinstance(coordinate_object, ECEF):\n            temp_object = self.to_ecef()\n        else:\n            raise TypeError(\"Please provide a valid coordinate type\")\n\n        return temp_object\n\n    return self\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.interpolate","title":"<code>interpolate(values, locations, dim=3, **kwargs)</code>","text":"<p>Interpolate data using coordinate object.</p> <p>If locations coordinate system does not match self's coordinate system it will be converted to same type as self. In the ENU case extra checking needs to take place to check reference locations match up.</p> <p>If only 1 value is provided which needs to be interpolated to many other locations we just set the value at all these locations to the single input value</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>Values to interpolate,  consistent with location in self</p> required <code>locations</code> <code>Coordinate</code> <p>Coordinate object containing locations to which you want to interpolate</p> required <code>dim</code> <code>int</code> <p>Number of dimensions to use for interpolation (2 or 3)</p> <code>3</code> <code>**kwargs</code> <code>dict</code> <p>Other arguments available in scipy.interpolate.griddata e.g. method, fill_value</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>ndarray</code> <p>Interpolated values at requested locations.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def interpolate(self, values: np.ndarray, locations, dim: int = 3, **kwargs) -&gt; np.ndarray:\n    \"\"\"Interpolate data using coordinate object.\n\n    If locations coordinate system does not match self's coordinate system it will be converted to same type as\n    self. In the ENU case extra checking needs to take place to check reference locations match up.\n\n    If only 1 value is provided which needs to be interpolated to many other locations we just set the value at all\n    these locations to the single input value\n\n    Args:\n        values (np.ndarray): Values to interpolate,  consistent with location in self\n        locations (Coordinate): Coordinate object containing locations to which you want to interpolate\n        dim (int): Number of dimensions to use for interpolation (2 or 3)\n        **kwargs (dict):  Other arguments available in scipy.interpolate.griddata e.g. method, fill_value\n\n    Returns:\n        Result (np.ndarray): Interpolated values at requested locations.\n\n    \"\"\"\n    locations = locations.to_object_type(coordinate_object=self)\n\n    if isinstance(self, ENU):\n        if (\n            self.ref_latitude != locations.ref_latitude\n            or self.ref_longitude != locations.ref_longitude\n            or self.ref_altitude != locations.ref_altitude\n        ):\n            locations = locations.to_lla()\n            locations = locations.to_enu(\n                ref_latitude=self.ref_latitude, ref_longitude=self.ref_longitude, ref_altitude=self.ref_altitude\n            )\n    result = sti.interpolate(\n        location_in=self.to_array(dim),\n        values_in=values.flatten(),\n        location_out=locations.to_array(dim=dim),\n        **kwargs,\n    )\n\n    return result\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.make_grid","title":"<code>make_grid(bounds, grid_type='rectangular', shape=(5, 5, 1))</code>","text":"<p>Generates grid of values locations based on specified inputs.</p> <p>If the grid type is 'spherical', we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad and temp_lon_rad.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>ndarray</code> <p>Limits of the grid on which to generate the grid of size [dim x 2] if dim == 2 we assume the third dimension will be zeros</p> required <code>grid_type</code> <code>str</code> <p>Type of grid to generate, default 'rectangular':      rectangular == rectangular grid of shape grd_shape,      spherical == grid of shape grid_shape taking into account a spherical spacing</p> <code>'rectangular'</code> <code>shape</code> <code>Union[tuple, ndarray]</code> <p>(tuple, optional): Number of grid cells to generate in each dimension, total number of grid cells will be the product of the entries of this tuple</p> <code>(5, 5, 1)</code> <p>Returns     np.ndarray: gridded of locations</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def make_grid(\n    self, bounds: np.ndarray, grid_type: str = \"rectangular\", shape: Union[tuple, np.ndarray] = (5, 5, 1)\n) -&gt; np.ndarray:\n    \"\"\"Generates grid of values locations based on specified inputs.\n\n    If the grid type is 'spherical', we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the\n    use in temp_lat_rad and temp_lon_rad.\n\n    Args:\n        bounds (np.ndarray): Limits of the grid on which to generate the grid of size [dim x 2]\n            if dim == 2 we assume the third dimension will be zeros\n        grid_type (str, optional): Type of grid to generate, default 'rectangular':\n                 rectangular == rectangular grid of shape grd_shape,\n                 spherical == grid of shape grid_shape taking into account a spherical spacing\n        shape: (tuple, optional): Number of grid cells to generate in each dimension, total number of\n            grid cells will be the product of the entries of this tuple\n\n    Returns\n        np.ndarray: gridded of locations\n\n    \"\"\"\n    dimension = bounds.shape[0]\n\n    if grid_type == \"rectangular\":\n        dim_0 = np.linspace(bounds[0, 0], bounds[0, 1], num=shape[0])\n        dim_1 = np.linspace(bounds[1, 0], bounds[1, 1], num=shape[1])\n        if dimension == 3:\n            dim_2 = np.linspace(bounds[2, 0], bounds[2, 1], num=shape[2])\n        else:\n            dim_2 = np.array(0)\n\n        dim_0, dim_1, dim_2 = np.meshgrid(dim_0, dim_1, dim_2)\n        array = np.stack([dim_0.flatten(), dim_1.flatten(), dim_2.flatten()], axis=1)\n    elif grid_type == \"spherical\":\n        temp_object = deepcopy(self)\n        temp_object.from_array(array=bounds)\n        temp_object = temp_object.to_lla()\n        temp_object.latitude = (temp_object.latitude - (-90)) / 180\n        temp_object.longitude = (temp_object.longitude - (-180)) / 360\n\n        temp_lat_rad = np.linspace(start=temp_object.latitude[0], stop=temp_object.latitude[1], num=shape[0])\n        temp_lon_rad = np.linspace(start=temp_object.longitude[0], stop=temp_object.longitude[1], num=shape[1])\n\n        longitude = (2 * np.pi * temp_lon_rad - np.pi) * 180 / np.pi\n        latitude = (np.arccos(1 - 2 * temp_lat_rad) - 0.5 * np.pi) * 180 / np.pi\n        if dimension == 3:\n            altitude = np.linspace(start=temp_object.altitude[0], stop=temp_object.altitude[1], num=shape[2])\n            latitude, longitude, altitude = np.meshgrid(latitude, longitude, altitude)\n            array = np.stack(\n                [latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180, altitude.flatten()], axis=1\n            )\n        else:\n            latitude, longitude = np.meshgrid(latitude, longitude)\n            array = np.stack([latitude.flatten() * np.pi / 180, longitude.flatten() * np.pi / 180], axis=1)\n\n        temp_object.from_array(array=array)\n        temp_object = temp_object.to_object_type(self)\n        array = temp_object.to_array()\n    else:\n        raise NotImplementedError(\"Please provide a valid grid type\")\n\n    return array\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.Coordinate.create_tree","title":"<code>create_tree()</code>","text":"<p>Create KD tree for the purpose of fast distance computation.</p> <p>Returns:</p> Name Type Description <code>KDTree</code> <code>KDTree</code> <p>Spatial KD tree</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def create_tree(self) -&gt; KDTree:\n    \"\"\"Create KD tree for the purpose of fast distance computation.\n\n    Returns:\n            KDTree: Spatial KD tree\n\n    \"\"\"\n    return KDTree(self.to_array())\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA","title":"<code>LLA</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Coordinate</code></p> <p>Defines the properties and functionality of the latitude/ longitude/ altitude coordinate system.</p> <p>Attributes:</p> Name Type Description <code>latitude</code> <code>ndarray</code> <p>Latitude values in degrees.</p> <code>longitude</code> <code>ndarray</code> <p>Longitude values in degrees.</p> <code>altitude</code> <code>ndarray</code> <p>Altitude values in meters with respect to a spheroid.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass LLA(Coordinate):\n    \"\"\"Defines the properties and functionality of the latitude/ longitude/ altitude coordinate system.\n\n    Attributes:\n        latitude (np.ndarray): Latitude values in degrees.\n        longitude (np.ndarray): Longitude values in degrees.\n        altitude (np.ndarray): Altitude values in meters with respect to a spheroid.\n\n    \"\"\"\n\n    latitude: np.ndarray = None\n    longitude: np.ndarray = None\n    altitude: np.ndarray = None\n\n    @property\n    def nof_observations(self):\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n        if self.latitude is None:\n            return 0\n        return self.latitude.size\n\n    def from_array(self, array):\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n        dim = array.shape[1]\n        self.latitude = array[:, 0]\n        self.longitude = array[:, 1]\n        self.altitude = np.zeros_like(self.latitude)\n        if dim == 3:\n            self.altitude = array[:, 2]\n\n    def to_array(self, dim=3):\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n        if dim == 2:\n            return np.stack((self.latitude.flatten(), self.longitude.flatten()), axis=1)\n        return np.stack((self.latitude.flatten(), self.longitude.flatten(), self.altitude.flatten()), axis=1)\n\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n        return self\n\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n        if self.altitude is None:\n            self.altitude = np.zeros(self.latitude.shape)\n        ecef_object = ECEF()\n        ecef_object.x, ecef_object.y, ecef_object.z = pm.geodetic2ecef(\n            lat=self.latitude, lon=self.longitude, alt=self.altitude, ell=self.ellipsoid, deg=self.use_degrees\n        )\n\n        return ecef_object\n\n    def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n        if self.altitude is None:\n            self.altitude = np.zeros(self.latitude.shape)\n\n        if ref_altitude is None:\n            ref_altitude = np.amin(self.altitude)\n\n        if ref_latitude is None:\n            ref_latitude = np.amin(self.latitude)\n\n        if ref_longitude is None:\n            ref_longitude = np.amin(self.longitude)\n\n        enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n        enu_object.east, enu_object.north, enu_object.up = pm.geodetic2enu(\n            lat=self.latitude,\n            lon=self.longitude,\n            h=self.altitude,\n            lat0=ref_latitude,\n            lon0=ref_longitude,\n            h0=ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.from_array","title":"<code>from_array(array)</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def from_array(self, array):\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n    dim = array.shape[1]\n    self.latitude = array[:, 0]\n    self.longitude = array[:, 1]\n    self.altitude = np.zeros_like(self.latitude)\n    if dim == 3:\n        self.altitude = array[:, 2]\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_array","title":"<code>to_array(dim=3)</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_array(self, dim=3):\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n    if dim == 2:\n        return np.stack((self.latitude.flatten(), self.longitude.flatten()), axis=1)\n    return np.stack((self.latitude.flatten(), self.longitude.flatten(), self.altitude.flatten()), axis=1)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_lla","title":"<code>to_lla()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n    return self\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_ecef","title":"<code>to_ecef()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n    if self.altitude is None:\n        self.altitude = np.zeros(self.latitude.shape)\n    ecef_object = ECEF()\n    ecef_object.x, ecef_object.y, ecef_object.z = pm.geodetic2ecef(\n        lat=self.latitude, lon=self.longitude, alt=self.altitude, ell=self.ellipsoid, deg=self.use_degrees\n    )\n\n    return ecef_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.LLA.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n    if self.altitude is None:\n        self.altitude = np.zeros(self.latitude.shape)\n\n    if ref_altitude is None:\n        ref_altitude = np.amin(self.altitude)\n\n    if ref_latitude is None:\n        ref_latitude = np.amin(self.latitude)\n\n    if ref_longitude is None:\n        ref_longitude = np.amin(self.longitude)\n\n    enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n    enu_object.east, enu_object.north, enu_object.up = pm.geodetic2enu(\n        lat=self.latitude,\n        lon=self.longitude,\n        h=self.altitude,\n        lat0=ref_latitude,\n        lon0=ref_longitude,\n        h0=ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU","title":"<code>ENU</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Coordinate</code></p> <p>Defines the properties and functionality of a local East-North-Up coordinate system.</p> <p>Positions relative to some reference location in metres.</p> <p>Attributes:</p> Name Type Description <code>ref_latitude</code> <code>float</code> <p>Reference latitude for current ENU system.</p> <code>ref_longitude</code> <code>float</code> <p>Reference longitude for current ENU system.</p> <code>ref_altitude</code> <code>float</code> <p>Reference altitude for current ENU system.</p> <code>east</code> <code>ndarray</code> <p>East values.</p> <code>north</code> <code>ndarray</code> <p>North values.</p> <code>up</code> <code>ndarray</code> <p>(np.ndarray): Up values.</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass ENU(Coordinate):\n    \"\"\"Defines the properties and functionality of a local East-North-Up coordinate system.\n\n     Positions relative to some reference location in metres.\n\n    Attributes:\n        ref_latitude (float): Reference latitude for current ENU system.\n        ref_longitude (float): Reference longitude for current ENU system.\n        ref_altitude (float): Reference altitude for current ENU system.\n        east (np.ndarray): East values.\n        north (np.ndarray): North values.\n        up: (np.ndarray): Up values.\n\n    \"\"\"\n\n    ref_latitude: float\n    ref_longitude: float\n    ref_altitude: float\n    east: np.ndarray = None\n    north: np.ndarray = None\n    up: np.ndarray = None\n\n    @property\n    def nof_observations(self):\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n        if self.east is None:\n            return 0\n        return self.east.size\n\n    def from_array(self, array):\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n        dim = array.shape[1]\n        self.east = array[:, 0]\n        self.north = array[:, 1]\n        self.up = np.zeros_like(self.east)\n        if dim == 3:\n            self.up = array[:, 2]\n\n    def to_array(self, dim=3):\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n        if dim == 2:\n            return np.stack((self.east.flatten(), self.north.flatten()), axis=1)\n        return np.stack((self.east.flatten(), self.north.flatten(), self.up.flatten()), axis=1)\n\n    def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n        if ref_latitude is None:\n            ref_latitude = self.ref_latitude\n\n        if ref_longitude is None:\n            ref_longitude = self.ref_longitude\n\n        if ref_altitude is None:\n            ref_altitude = self.ref_altitude\n\n        if (\n            self.ref_latitude == ref_latitude\n            and self.ref_longitude == ref_longitude\n            and self.ref_altitude == ref_altitude\n        ):\n            return self\n\n        ecef_temp = self.to_ecef()\n\n        return ecef_temp.to_enu(ref_longitude=ref_longitude, ref_latitude=ref_latitude, ref_altitude=ref_altitude)\n\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n        lla_object = LLA()\n\n        lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.enu2geodetic(\n            e=self.east,\n            n=self.north,\n            u=self.up,\n            lat0=self.ref_latitude,\n            lon0=self.ref_longitude,\n            h0=self.ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return lla_object\n\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n        ecef_object = ECEF()\n\n        ecef_object.x, ecef_object.y, ecef_object.z = pm.enu2ecef(\n            e1=self.east,\n            n1=self.north,\n            u1=self.up,\n            lat0=self.ref_latitude,\n            lon0=self.ref_longitude,\n            h0=self.ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return ecef_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.from_array","title":"<code>from_array(array)</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def from_array(self, array):\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n    dim = array.shape[1]\n    self.east = array[:, 0]\n    self.north = array[:, 1]\n    self.up = np.zeros_like(self.east)\n    if dim == 3:\n        self.up = array[:, 2]\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_array","title":"<code>to_array(dim=3)</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_array(self, dim=3):\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n    if dim == 2:\n        return np.stack((self.east.flatten(), self.north.flatten()), axis=1)\n    return np.stack((self.east.flatten(), self.north.flatten(), self.up.flatten()), axis=1)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n    if ref_latitude is None:\n        ref_latitude = self.ref_latitude\n\n    if ref_longitude is None:\n        ref_longitude = self.ref_longitude\n\n    if ref_altitude is None:\n        ref_altitude = self.ref_altitude\n\n    if (\n        self.ref_latitude == ref_latitude\n        and self.ref_longitude == ref_longitude\n        and self.ref_altitude == ref_altitude\n    ):\n        return self\n\n    ecef_temp = self.to_ecef()\n\n    return ecef_temp.to_enu(ref_longitude=ref_longitude, ref_latitude=ref_latitude, ref_altitude=ref_altitude)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_lla","title":"<code>to_lla()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n    lla_object = LLA()\n\n    lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.enu2geodetic(\n        e=self.east,\n        n=self.north,\n        u=self.up,\n        lat0=self.ref_latitude,\n        lon0=self.ref_longitude,\n        h0=self.ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return lla_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ENU.to_ecef","title":"<code>to_ecef()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n    ecef_object = ECEF()\n\n    ecef_object.x, ecef_object.y, ecef_object.z = pm.enu2ecef(\n        e1=self.east,\n        n1=self.north,\n        u1=self.up,\n        lat0=self.ref_latitude,\n        lon0=self.ref_longitude,\n        h0=self.ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return ecef_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF","title":"<code>ECEF</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Coordinate</code></p> <p>Defines the properties and functionality of an Earth-Centered, Earth-Fixed coordinate system.</p> <p>See: https://en.wikipedia.org/wiki/Earth-centered,_Earth-fixed_coordinate_system</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Eastings values [metres]</p> <code>y</code> <code>ndarray</code> <p>Northings values [metres]</p> <code>z</code> <code>ndarray</code> <p>Altitude values [metres]</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>@dataclass\nclass ECEF(Coordinate):\n    \"\"\"Defines the properties and functionality of an Earth-Centered, Earth-Fixed coordinate system.\n\n    See: https://en.wikipedia.org/wiki/Earth-centered,_Earth-fixed_coordinate_system\n\n    Attributes:\n        x (np.ndarray): Eastings values [metres]\n        y (np.ndarray): Northings values [metres]\n        z (np.ndarray): Altitude values [metres]\n\n    \"\"\"\n\n    x: np.ndarray = None\n    y: np.ndarray = None\n    z: np.ndarray = None\n\n    @property\n    def nof_observations(self):\n        \"\"\"Number of observations contained in the class instance, implemented as dependent property.\"\"\"\n        if self.x is None:\n            return 0\n        return self.x.size\n\n    def from_array(self, array):\n        \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n        The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n        Args:\n            array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n                array\n\n        \"\"\"\n        dim = array.shape[1]\n        self.x = array[:, 0]\n        self.y = array[:, 1]\n        self.z = np.zeros_like(self.x)\n        if dim == 3:\n            self.z = array[:, 2]\n\n    def to_array(self, dim=3):\n        \"\"\"Stacks coordinates together into a numpy array.\n\n        Args:\n            dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n        Returns:\n            (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n        \"\"\"\n        if dim == 2:\n            return np.stack((self.x.flatten(), self.y.flatten()), axis=1)\n        return np.stack((self.x.flatten(), self.y.flatten(), self.z.flatten()), axis=1)\n\n    def to_ecef(self):\n        \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n        return self\n\n    def to_lla(self):\n        \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n        lla_object = LLA()\n\n        lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.ecef2geodetic(\n            self.x, self.y, self.z, ell=self.ellipsoid, deg=self.use_degrees\n        )\n\n        return lla_object\n\n    def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n        \"\"\"Converts coordinates to East North Up system.\n\n        If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n        Args:\n            ref_latitude (float, optional): reference latitude for ENU\n            ref_longitude (float, optional): reference longitude for ENU\n            ref_altitude (float, optional):  reference altitude for ENU\n\n        Returns:\n           (ENU): East North Up coordinate object\n\n        \"\"\"\n        if ref_latitude is None or ref_longitude is None or ref_altitude is None:\n            lla_object = self.to_lla()\n            return lla_object.to_enu()\n\n        enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n        enu_object.east, enu_object.north, enu_object.up = pm.ecef2enu(\n            x=self.x,\n            y=self.y,\n            z=self.z,\n            lat0=ref_latitude,\n            lon0=ref_longitude,\n            h0=ref_altitude,\n            ell=self.ellipsoid,\n            deg=self.use_degrees,\n        )\n\n        return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations contained in the class instance, implemented as dependent property.</p>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.from_array","title":"<code>from_array(array)</code>","text":"<p>Unstack a numpy array into the corresponding coordinates.</p> <p>The method has no return as it sets the corresponding attributes of the coordinate class instance.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> required Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def from_array(self, array):\n    \"\"\"Unstack a numpy array into the corresponding coordinates.\n\n    The method has no return as it sets the corresponding attributes of the coordinate class instance.\n\n    Args:\n        array (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single\n            array\n\n    \"\"\"\n    dim = array.shape[1]\n    self.x = array[:, 0]\n    self.y = array[:, 1]\n    self.z = np.zeros_like(self.x)\n    if dim == 3:\n        self.z = array[:, 2]\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_array","title":"<code>to_array(dim=3)</code>","text":"<p>Stacks coordinates together into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of dimensions to use, which is either 2 or 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_array(self, dim=3):\n    \"\"\"Stacks coordinates together into a numpy array.\n\n    Args:\n        dim (int, optional): Number of dimensions to use, which is either 2 or 3.\n\n    Returns:\n        (np.ndarray): Numpy array of size [n x dim] with n&gt;0 containing the coordinates stacked into a single array\n\n    \"\"\"\n    if dim == 2:\n        return np.stack((self.x.flatten(), self.y.flatten()), axis=1)\n    return np.stack((self.x.flatten(), self.y.flatten(), self.z.flatten()), axis=1)\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_ecef","title":"<code>to_ecef()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_ecef(self):\n    \"\"\"ECEF: Convert coordinates to earth centered earth fixed coordinates.\"\"\"\n    return self\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_lla","title":"<code>to_lla()</code>","text":"Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_lla(self):\n    \"\"\"LLA: Converts coordinates to latitude/longitude/altitude system.\"\"\"\n    lla_object = LLA()\n\n    lla_object.latitude, lla_object.longitude, lla_object.altitude = pm.ecef2geodetic(\n        self.x, self.y, self.z, ell=self.ellipsoid, deg=self.use_degrees\n    )\n\n    return lla_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.ECEF.to_enu","title":"<code>to_enu(ref_latitude=None, ref_longitude=None, ref_altitude=None)</code>","text":"<p>Converts coordinates to East North Up system.</p> <p>If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>reference latitude for ENU</p> <code>None</code> <code>ref_longitude</code> <code>float</code> <p>reference longitude for ENU</p> <code>None</code> <code>ref_altitude</code> <code>float</code> <p>reference altitude for ENU</p> <code>None</code> <p>Returns:</p> Type Description <code>ENU</code> <p>East North Up coordinate object</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def to_enu(self, ref_latitude=None, ref_longitude=None, ref_altitude=None):\n    \"\"\"Converts coordinates to East North Up system.\n\n    If a reference is not provided, the  minimum of coordinates in Lat/Lon/Alt is used as the reference.\n\n    Args:\n        ref_latitude (float, optional): reference latitude for ENU\n        ref_longitude (float, optional): reference longitude for ENU\n        ref_altitude (float, optional):  reference altitude for ENU\n\n    Returns:\n       (ENU): East North Up coordinate object\n\n    \"\"\"\n    if ref_latitude is None or ref_longitude is None or ref_altitude is None:\n        lla_object = self.to_lla()\n        return lla_object.to_enu()\n\n    enu_object = ENU(ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude)\n\n    enu_object.east, enu_object.north, enu_object.up = pm.ecef2enu(\n        x=self.x,\n        y=self.y,\n        z=self.z,\n        lat0=ref_latitude,\n        lon0=ref_longitude,\n        h0=ref_altitude,\n        ell=self.ellipsoid,\n        deg=self.use_degrees,\n    )\n\n    return enu_object\n</code></pre>"},{"location":"pyelq/coordinate_system/#pyelq.coordinate_system.make_latin_hypercube","title":"<code>make_latin_hypercube(bounds, nof_samples)</code>","text":"<p>Latin Hypercube samples.</p> <p>Draw samples according to a Latin Hypercube design within the specified bounds.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>ndarray</code> <p>Limits of the resulting hypercube of size [dim x 2]</p> required <code>nof_samples</code> <code>int</code> <p>Number of samples to draw</p> required <p>Returns:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>Samples forming the Latin Hypercube</p> Source code in <code>src/pyelq/coordinate_system.py</code> <pre><code>def make_latin_hypercube(bounds: np.ndarray, nof_samples: int) -&gt; np.ndarray:\n    \"\"\"Latin Hypercube samples.\n\n    Draw samples according to a Latin Hypercube design within the specified bounds.\n\n    Args:\n        bounds (np.ndarray): Limits of the resulting hypercube of size [dim x 2]\n        nof_samples (int): Number of samples to draw\n\n    Returns:\n        array (np.ndarray): Samples forming the Latin Hypercube\n\n    \"\"\"\n    dimension = bounds.shape[0]\n    sampler = qmc.LatinHypercube(d=dimension)\n    sample = sampler.random(n=nof_samples)\n    array = qmc.scale(sample, np.min(bounds, axis=1), np.max(bounds, axis=1))\n    return array\n</code></pre>"},{"location":"pyelq/dlm/","title":"DLM","text":""},{"location":"pyelq/dlm/#dlm","title":"DLM","text":"<p>DLM module.</p> <p>This module provides a class definition for the Dynamic Linear Models following Harrison and West 'Bayesian Forecasting and Dynamic Models' (2nd ed), Springer New York, NY, Chapter 4, https://doi.org/10.1007/b98971</p>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM","title":"<code>DLM</code>  <code>dataclass</code>","text":"<p>Defines the DLM in line with Harrison and West (2nd edition) Chapter 4.</p> <p>Attributes:</p> Name Type Description <code>f_matrix</code> <code>ndarray</code> <p>F matrix linking the state to the observables of size [nof_state_parameters x nof_observables]</p> <code>g_matrix</code> <code>ndarray</code> <p>G matrix characterizing the state evolution of size [nof_state_parameters x nof_state parameters]</p> <code>v_matrix</code> <code>ndarray</code> <p>V matrix being the covariance matrix of the zero mean observation noise of size [nof_state_parameters x nof_observables]</p> <code>w_matrix</code> <code>ndarray</code> <p>W matrix being the covariance matrix of the zero mean system noise of size [nof_state_parameters x nof_state parameters]</p> <code>g_power</code> <code>ndarray</code> <p>Attribute to store G^k, does not get initialized</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>@dataclass\nclass DLM:\n    \"\"\"Defines the DLM in line with Harrison and West (2nd edition) Chapter 4.\n\n    Attributes:\n        f_matrix (np.ndarray, optional): F matrix linking the state to the observables of\n            size [nof_state_parameters x nof_observables]\n        g_matrix (np.ndarray, optional): G matrix characterizing the state evolution of\n            size [nof_state_parameters x nof_state parameters]\n        v_matrix (np.ndarray, optional): V matrix being the covariance matrix of the zero mean observation noise\n            of size [nof_state_parameters x nof_observables]\n        w_matrix (np.ndarray, optional): W matrix being the covariance matrix of the zero mean system noise of\n            size [nof_state_parameters x nof_state parameters]\n        g_power (np.ndarray, optional): Attribute to store G^k, does not get initialized\n\n    \"\"\"\n\n    f_matrix: np.ndarray = None\n    g_matrix: np.ndarray = None\n    v_matrix: np.ndarray = None\n    w_matrix: np.ndarray = None\n    g_power: np.ndarray = field(init=False)\n\n    @property\n    def nof_observables(self) -&gt; int:\n        \"\"\"Int: Number of observables as derived from the associated F matrix.\"\"\"\n        if self.f_matrix is not None and isinstance(self.f_matrix, np.ndarray):\n            return self.f_matrix.shape[1]\n        return 0\n\n    @property\n    def nof_state_parameters(self) -&gt; int:\n        \"\"\"Int: Number of state parameters as derived from the associated G matrix.\"\"\"\n        if self.g_matrix is not None and isinstance(self.g_matrix, np.ndarray):\n            return self.g_matrix.shape[0]\n        return 0\n\n    def calculate_g_power(self, max_power: int) -&gt; None:\n        \"\"\"Calculate the powers of the G matrix.\n\n        Calculate the powers upfront, so we don't have to calculate it at every iteration. Result gets stored in the\n        g_power attribute of the DLM class. We use an iterative way of calculating the power to have the fewest matrix\n        multiplications necessary, i.e. we are not using numpy.linalg.matrix_power as that would leak to k factorial\n        multiplications instead of the k we have now.\n\n        Args:\n            max_power (int): Maximum power to compute\n\n        \"\"\"\n        if self.nof_state_parameters == 1:\n            self.g_power = self.g_matrix ** np.array([[range(max_power + 1)]])\n        else:\n            self.g_power = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_power + 1))\n            self.g_power[:, :, 0] = np.identity(self.nof_state_parameters)\n            for i in range(max_power):\n                self.g_power[:, :, i + 1] = self.g_power[:, :, i] @ self.g_matrix\n\n    def polynomial_f_g(self, nof_observables: int, order: int) -&gt; None:\n        \"\"\"Create F and G matrices associated with a polynomial DLM.\n\n        Following Harrison and West (Chapter 7 on polynomial DLMs) with the exception that we use order==0 for a\n        \"constant\" DLM and order==1 for linear growth DLM, order==2 for quadratic growth etc.\n        Hence, the definition of n-th order polynomial DLM in Harrison &amp; West is implemented here with order=n-1\n        We stack the observables in a block diagonal form. So the first #order of rows belong to the first observable,\n        the second #order rows belong to the second observable etc.\n        Results are being stored in the f_matrix and g_matrix attributes respectively\n\n        Args:\n            nof_observables (int): Dimension of observation\n            order (int): Polynomial order (0=constant, 1=linear, 2=quadratic etc.)\n\n        \"\"\"\n        e_n = np.append(1, np.zeros(order))[:, None]\n        self.f_matrix = np.kron(np.eye(nof_observables), e_n)\n\n        l_n = np.triu(np.ones((order + 1, order + 1)))\n        self.g_matrix = np.kron(np.eye(nof_observables), l_n)\n\n    def simulate_data(self, init_state: np.ndarray, nof_timesteps: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Simulate data from DLM model.\n\n        Function to simulate state evolution and corresponding observations according to model as specified through DLM\n        class attributes (F, G, V and W matrices)\n\n        Args:\n            init_state (np.ndarray): Initial state vector to start simulating from of size [nof_state_parameters x 1]\n            nof_timesteps (int): Number of timesteps to simulate\n\n        Returns:\n            state (np.ndarray): Simulated state vectors of size [nof_state_parameters x nof_timesteps]\n            obs (np.ndarray): Simulated observations of size [nof_observables x nof_timesteps]\n\n        \"\"\"\n        if self.f_matrix is None or self.g_matrix is None or self.v_matrix is None or self.w_matrix is None:\n            raise ValueError(\"Please specify all matrices (F, G, V and W)\")\n\n        obs = np.empty((self.nof_observables, nof_timesteps))\n        state = np.empty((self.nof_state_parameters, nof_timesteps))\n\n        state[:, [0]] = init_state\n        mean_state_noise = np.zeros(self.nof_state_parameters)\n        mean_observation_noise = np.zeros(self.nof_observables)\n\n        random_generator = np.random.default_rng(seed=None)\n\n        for i in range(nof_timesteps):\n            if i == 0:\n                state[:, [i]] = (\n                    self.g_matrix @ init_state\n                    + random_generator.multivariate_normal(mean_state_noise, self.w_matrix, size=1).T\n                )\n            else:\n                state[:, [i]] = (\n                    self.g_matrix @ state[:, [i - 1]]\n                    + random_generator.multivariate_normal(mean_state_noise, self.w_matrix, size=1).T\n                )\n            obs[:, [i]] = (\n                self.f_matrix.T @ state[:, [i]]\n                + random_generator.multivariate_normal(mean_observation_noise, self.v_matrix, size=1).T\n            )\n\n        return state, obs\n\n    def forecast_mean(\n        self, current_mean_state: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Perform forecasting of the state and observation mean parameters.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), corollary 4.1, assuming F and G are\n        constant over time.\n        Note that in the output the second axis of the output arrays is the forecast dimension consistent with the\n        forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n\n        Args:\n            current_mean_state (np.ndarray): Current mean parameter for the state of size [nof_state_parameters x 1]\n            forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n        Returns:\n            a_t_k (np.array): Forecast values of state mean parameter of the size\n                [nof_observables x size(forecast_steps)]\n            f_t_k (np.array): Forecast values of observation mean parameter of the size\n                [nof_observables x size(forecast_steps)]\n\n        \"\"\"\n        min_forecast = np.amin(forecast_steps)\n\n        if min_forecast &lt; 1:\n            raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n        if isinstance(forecast_steps, int):\n            forecast_steps = [forecast_steps]\n\n        a_t_k = np.hstack([self.g_power[:, :, step] @ current_mean_state for step in forecast_steps])\n        f_t_k = self.f_matrix.T @ a_t_k\n\n        return a_t_k, f_t_k\n\n    def forecast_covariance(\n        self, c_matrix: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Perform forecasting of the state and observation covariance parameters.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n        constant over time.\n        Note that in the output the third axis of the output arrays is the forecast dimension consistent with the\n        forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n        sum_g_w_g is initialized as G^k @ W @ G^k for k==0, hence we initialize as W\n        Because of zero based indexing, in the for loop i==1 means 2-step ahead forecast which requires element\n        (i+1) of the g_power attribute as the third dimension serves as the actual power of the G matrix\n\n        Args:\n            c_matrix (np.ndarray): Current posterior covariance estimate for the state of size\n                [nof_state_parameters x nof_state_parameters]\n            forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n        Returns:\n            r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n                [nof_state_parameters x nof_state_parameters x size(forecast_steps)]\n            q_t_k (np.array): Forecast values of estimated observation covariance of the size\n                [nof_observables x nof_observables x size(forecast_steps)]\n\n        \"\"\"\n        min_forecast = np.amin(forecast_steps)\n        max_forecast = np.amax(forecast_steps)\n\n        if min_forecast &lt; 1:\n            raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n        if isinstance(forecast_steps, int):\n            forecast_steps = [forecast_steps]\n\n        sum_g_w_g = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_forecast))\n        sum_g_w_g[:, :, 0] = self.w_matrix\n        for i in np.arange(1, max_forecast, step=1):\n            sum_g_w_g[:, :, i] = (\n                sum_g_w_g[:, :, i - 1] + self.g_power[:, :, i] @ self.w_matrix @ self.g_power[:, :, i].T\n            )\n\n        r_t_k = np.dstack(\n            [\n                self.g_power[:, :, step] @ c_matrix @ self.g_power[:, :, step].T + sum_g_w_g[:, :, step - 1]\n                for step in forecast_steps\n            ]\n        )\n        q_t_k = np.dstack(\n            [self.f_matrix.T @ r_t_k[:, :, idx] @ self.f_matrix + self.v_matrix for idx in range(r_t_k.shape[2])]\n        )\n\n        return r_t_k, q_t_k\n\n    def update_posterior(\n        self, a_t: np.ndarray, r_matrix_t: np.ndarray, q_matrix_t: np.ndarray, error: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Update of the posterior mean and covariance of the state.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n        constant over time.\n        We are using a solver instead of calculating the inverse of Q directly\n        Setting inf values in Q equal to 0 after the solver function for computational issues, otherwise we would\n        get 0 * inf = nan, where we want the result to be 0.\n\n        Args:\n            a_t (np.ndarray): Current prior mean of the state of size [nof_state_parameters x 1]\n            r_matrix_t (np.ndarray): Current prior covariance of the state of size [nof_state_parameters x nof_state_parameters]\n            q_matrix_t (np.ndarray): Current one step ahead forecast covariance estimate of the observations of size [nof_observables x nof_observables]\n            error (np.ndarray): Error associated with the one step ahead forecast (observation - forecast) of size [nof_observables x 1]\n\n        Returns:\n            m_t (np.array): Posterior mean estimate of the state of size [nof_state_parameters x 1]\n            c_matrix (np.array): Posterior covariance estimate of the state of size [nof_state_parameters x nof_state_parameters]\n\n        \"\"\"\n        if self.nof_state_parameters == 1:\n            a_matrix_t = r_matrix_t @ self.f_matrix.T @ (1 / q_matrix_t)\n        else:\n            a_matrix_t = r_matrix_t @ np.linalg.solve(q_matrix_t.T, self.f_matrix.T).T\n        m_t = a_t + a_matrix_t @ error\n        q_matrix_t[np.isinf(q_matrix_t)] = 0\n        c_matrix = r_matrix_t - a_matrix_t @ q_matrix_t @ a_matrix_t.T\n\n        return m_t, c_matrix\n\n    def dlm_full_update(\n        self,\n        new_observation: np.ndarray,\n        current_mean_state: np.ndarray,\n        current_cov_state: np.ndarray,\n        mode: str = \"learn\",\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform 1 step of the full DLM update.\n\n        Following Harrison and West (2nd ed) we perform all steps to update the entire DLM model and obtain new\n        estimates for all parameters involved, including nan value handling.\n        When mode == 'learn' the parameters are updated, when mode == 'ignore' the current observation is ignored and\n        the posterior is set equal to the prior\n        When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n        such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n        terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n        we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n        forecast gets increased. We set the error equal to zero for computational issues, first but finally set it equal\n        to nan in the end.\n\n        Args:\n            new_observation (np.ndarray): New observations to use in the updating of the estimates of size [nof_observables x 1]\n            current_mean_state (np.ndarray):  Current mean estimate for the state of size [nof_state_parameters x 1]\n            current_cov_state (np.ndarray):  Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n            mode (str, optional): String indicating whether the DLM needs to be updated using the new observation or not. Currently, `learn` and `ignore` are implemented\n\n        Returns:\n            new_mean_state (np.ndarray): New mean estimate for the state of size [nof_state_parameters x 1]\n            new_cov_state (np.ndarray): New covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n            error (np.ndarray): Error between the observation and the forecast (observation - forecast) of size [nof_observables x 1]\n\n        \"\"\"\n        a_t, f_t = self.forecast_mean(current_mean_state, forecast_steps=1)\n        r_matrix_t, q_matrix_t = self.forecast_covariance(current_cov_state, forecast_steps=1)\n        error = new_observation - f_t\n\n        nan_bool = np.isnan(new_observation)\n        nan_idx = np.argwhere(nan_bool.flatten())\n        if np.any(nan_bool):\n            q_matrix_t[nan_idx, :, 0] -= self.v_matrix[nan_idx, :]\n            q_matrix_t[:, nan_idx, 0] -= self.v_matrix[:, nan_idx]\n            q_matrix_t[nan_idx, nan_idx, 0] = np.inf\n            error[nan_idx] = 0\n\n        if mode == \"learn\":\n            new_mean_state, new_cov_state = self.update_posterior(a_t, r_matrix_t[:, :, 0], q_matrix_t[:, :, 0], error)\n        elif mode == \"ignore\":\n            new_mean_state = a_t\n            new_cov_state = r_matrix_t\n        else:\n            raise TypeError(f\"Mode {mode} not implemented\")\n\n        error[nan_idx] = np.nan\n\n        return new_mean_state, new_cov_state, error\n\n    def calculate_mahalanobis_distance(\n        self,\n        new_observations: np.ndarray,\n        current_mean_state: np.ndarray,\n        current_cov_state: np.ndarray,\n        forecast_steps: int = 1,\n        return_statistics=False,\n    ) -&gt; Union[Tuple[float, np.ndarray], Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"Calculate the mahalanobis distance.\n\n        Calculating the Mahalanobis distance which is defined as error.T @ covariance^(-1) @ error\n        The error is flatted in row-major (C-style) This returns the stacked rows, which in our case is the errors per\n        observation parameter stacked and this is exactly what we want: array([[1, 2], [3, 4]]).reshape((-1, 1),\n        order='C') becomes column array([1, 2 3, 4])\n        Using a solve method instead of calculating inverse matrices directly\n        When calculating mhd_per_obs_param we use the partial result and reshape the temporary output such that we can\n        sum the correct elements associated with the same observable together\n        When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n        such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n        terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n        we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n        forecast gets increased. We set the error equal to zero for computational issues, but this does decrease the\n        number of degrees of freedom for that particular Mahalanobis distance calculation, basically decreasing the\n        Mahalanobis distance. We allow the option to output the number of degrees of freedom and chi2 statistic which\n        allows to take this decrease in degrees of freedom into account.\n\n        Args:\n            new_observations (np.ndarray): New observations to use in the calculation of the mahalanobis distance of\n                size [nof_observables x forecast_steps]\n            current_mean_state (np.ndarray): Current mean estimate for the state of size [nof_state_parameters x 1]\n            current_cov_state (np.ndarray): Current covariance estimate for the state of size\n                [nof_state_parameters x nof_state_parameters]\n            forecast_steps (int, optional): Number of steps ahead to forecast and use in the mahalanobis distance\n                calculation\n            return_statistics (bool, optional): Boolean to return used degrees of freedom and chi2 statistic\n        Returns:\n            mhd_overall (float): mahalanobis distance over all observables\n            mhd_per_obs_param (np.ndarray): mahalanobis distance per observation parameter of size [nof_observables, 1]\n\n        \"\"\"\n        if forecast_steps &lt;= 0:\n            raise AttributeError(\"Forecast steps should be a positive integer\")\n\n        if new_observations.size / self.nof_observables != forecast_steps:\n            raise AttributeError(\"Sizes of new observations and forecast steps are not aligning\")\n\n        _, f_t_k = self.forecast_mean(current_mean_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n        if new_observations.shape != f_t_k.shape:\n            raise AttributeError(\"Dimensions of new_observations are not aligning with dimensions of forecast\")\n\n        error = np.subtract(new_observations, f_t_k).reshape((-1, 1), order=\"C\")\n\n        r_t_k, q_t_k = self.forecast_covariance(current_cov_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n        nan_bool = np.isnan(new_observations)\n        if np.any(nan_bool):\n            nan_idx = np.argwhere(nan_bool)\n            for value in nan_idx:\n                q_t_k[value[0], :, value[1]] -= self.v_matrix[value[0], :]\n                q_t_k[:, value[0], value[1]] -= self.v_matrix[:, value[0]]\n\n            q_t_k[nan_idx[:, 0], nan_idx[:, 0], nan_idx[:, 1]] = np.inf\n            error[nan_bool.reshape((-1, 1), order=\"C\")] = 0\n\n        if forecast_steps &gt; 1:\n            full_covariance = self.create_full_covariance(r_t_k=r_t_k, q_t_k=q_t_k, forecast_steps=forecast_steps)\n        else:\n            full_covariance = q_t_k[:, :, 0]\n\n        mhd_overall = mahalanobis_distance(error=error, cov_matrix=full_covariance)\n        mhd_per_obs_param = np.empty((self.nof_observables, 1))\n\n        for i_obs in range(self.nof_observables):\n            ind_hrz = np.array(range(forecast_steps)) + i_obs * forecast_steps\n            mhd_per_obs_param[i_obs] = mahalanobis_distance(\n                error=error[ind_hrz], cov_matrix=full_covariance[np.ix_(ind_hrz, ind_hrz)]\n            )\n\n        if self.nof_observables == 1:\n            mhd_per_obs_param = mhd_per_obs_param.item()\n\n        if return_statistics:\n            dof_per_obs_param = (nan_bool.shape[1] - np.count_nonzero(nan_bool, axis=1)).reshape(\n                self.nof_observables, 1\n            )\n            dof_overall = dof_per_obs_param.sum()\n            chi2_cdf_per_obs_param = chi2.cdf(mhd_per_obs_param.flatten(), dof_per_obs_param.flatten()).reshape(\n                self.nof_observables, 1\n            )\n            chi2_cdf_overall = chi2.cdf(mhd_overall, dof_overall)\n\n            return (\n                mhd_overall,\n                mhd_per_obs_param,\n                dof_overall,\n                dof_per_obs_param,\n                chi2_cdf_overall,\n                chi2_cdf_per_obs_param,\n            )\n\n        return mhd_overall, mhd_per_obs_param\n\n    def create_full_covariance(self, r_t_k: np.ndarray, q_t_k: np.ndarray, forecast_steps: int) -&gt; np.ndarray:\n        \"\"\"Helper function to construct the full covariance matrix.\n\n        Following Harrison and West (2nd ed) Chapter 4.4 (Forecast distributions) Theorem 4.2 and corollary 4.2\n        we construct the full covariance matrix. This full covariance matrix is the covariance matrix of all forecasted\n        observations with respect to each other. Hence, it's COV[Y_{t+k}, Y_{t+j}] with j and k 1&lt;=j,k&lt;=forecast steps\n        input argument and Y_{t+k} the k step ahead forecast of the observation at time t\n\n        The matrix is build up using the different blocks for different covariances between observations i and j.\n        The diagonals of each block are calculated first as q_t_k[i, j, :].\n        Next the i, j-th (lower triangular) entry of the m, n-th block is calculated as\n        (F.T @ G^(i-j) r_t_k[:, :, j] @ F)[i, j]\n        Next each upper triangular part of each lower diagonal block is calculated and next the entire upper triangular\n        part of the full matrix is calculated\n\n        Args:\n            r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n                [nof_state_parameters x nof_state_parameters x forecast_steps]\n            q_t_k (np.array): Forecast values of estimated observation covariance of the size\n                [nof_observables x nof_observables x forecast_steps]\n            forecast_steps (int): Maximum number of steps ahead to forecast and use all of those in the mahalanobis\n                distance calculation\n\n        Returns:\n            full_covariance (np.array): Full covariance matrix of all forecasted observations with respect to each other\n            having size [(nof_observables * forecast_steps) X (nof_observables * forecast_steps)]\n\n        \"\"\"\n        full_covariance = np.zeros((forecast_steps * self.nof_observables, forecast_steps * self.nof_observables))\n        base_idx = np.array(range(forecast_steps))\n        for block_i in range(self.nof_observables):\n            for block_j in range(block_i + 1):\n                block_rows = base_idx + block_i * forecast_steps\n                block_cols = base_idx + block_j * forecast_steps\n                full_covariance[block_rows, block_cols] = q_t_k[block_i, block_j, :]\n\n        temp_idx = np.array(range(self.nof_observables))\n        for sub_i in np.arange(start=1, stop=forecast_steps, step=1):\n            sub_row = temp_idx * forecast_steps + sub_i\n            for sub_j in range(sub_i):\n                sub_col = temp_idx * forecast_steps + sub_j\n                sub_idx = np.ix_(sub_row, sub_col)\n                full_covariance[sub_idx] = (\n                    self.f_matrix.T @ self.g_power[:, :, sub_i - sub_j] @ r_t_k[:, :, sub_j] @ self.f_matrix\n                )\n\n        for block_i in range(self.nof_observables):\n            for block_j in range(block_i):\n                block_rows = base_idx + block_i * forecast_steps\n                block_cols = base_idx + block_j * forecast_steps\n                block_idx = np.ix_(block_rows, block_cols)\n                full_covariance[block_idx] = full_covariance[block_idx] + np.tril(full_covariance[block_idx], k=-1).T\n\n        full_covariance = np.tril(full_covariance) + np.tril(full_covariance, k=-1).T\n\n        return full_covariance\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.nof_observables","title":"<code>nof_observables</code>  <code>property</code>","text":""},{"location":"pyelq/dlm/#pyelq.dlm.DLM.nof_state_parameters","title":"<code>nof_state_parameters</code>  <code>property</code>","text":""},{"location":"pyelq/dlm/#pyelq.dlm.DLM.calculate_g_power","title":"<code>calculate_g_power(max_power)</code>","text":"<p>Calculate the powers of the G matrix.</p> <p>Calculate the powers upfront, so we don't have to calculate it at every iteration. Result gets stored in the g_power attribute of the DLM class. We use an iterative way of calculating the power to have the fewest matrix multiplications necessary, i.e. we are not using numpy.linalg.matrix_power as that would leak to k factorial multiplications instead of the k we have now.</p> <p>Parameters:</p> Name Type Description Default <code>max_power</code> <code>int</code> <p>Maximum power to compute</p> required Source code in <code>src/pyelq/dlm.py</code> <pre><code>def calculate_g_power(self, max_power: int) -&gt; None:\n    \"\"\"Calculate the powers of the G matrix.\n\n    Calculate the powers upfront, so we don't have to calculate it at every iteration. Result gets stored in the\n    g_power attribute of the DLM class. We use an iterative way of calculating the power to have the fewest matrix\n    multiplications necessary, i.e. we are not using numpy.linalg.matrix_power as that would leak to k factorial\n    multiplications instead of the k we have now.\n\n    Args:\n        max_power (int): Maximum power to compute\n\n    \"\"\"\n    if self.nof_state_parameters == 1:\n        self.g_power = self.g_matrix ** np.array([[range(max_power + 1)]])\n    else:\n        self.g_power = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_power + 1))\n        self.g_power[:, :, 0] = np.identity(self.nof_state_parameters)\n        for i in range(max_power):\n            self.g_power[:, :, i + 1] = self.g_power[:, :, i] @ self.g_matrix\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.polynomial_f_g","title":"<code>polynomial_f_g(nof_observables, order)</code>","text":"<p>Create F and G matrices associated with a polynomial DLM.</p> <p>Following Harrison and West (Chapter 7 on polynomial DLMs) with the exception that we use order==0 for a \"constant\" DLM and order==1 for linear growth DLM, order==2 for quadratic growth etc. Hence, the definition of n-th order polynomial DLM in Harrison &amp; West is implemented here with order=n-1 We stack the observables in a block diagonal form. So the first #order of rows belong to the first observable, the second #order rows belong to the second observable etc. Results are being stored in the f_matrix and g_matrix attributes respectively</p> <p>Parameters:</p> Name Type Description Default <code>nof_observables</code> <code>int</code> <p>Dimension of observation</p> required <code>order</code> <code>int</code> <p>Polynomial order (0=constant, 1=linear, 2=quadratic etc.)</p> required Source code in <code>src/pyelq/dlm.py</code> <pre><code>def polynomial_f_g(self, nof_observables: int, order: int) -&gt; None:\n    \"\"\"Create F and G matrices associated with a polynomial DLM.\n\n    Following Harrison and West (Chapter 7 on polynomial DLMs) with the exception that we use order==0 for a\n    \"constant\" DLM and order==1 for linear growth DLM, order==2 for quadratic growth etc.\n    Hence, the definition of n-th order polynomial DLM in Harrison &amp; West is implemented here with order=n-1\n    We stack the observables in a block diagonal form. So the first #order of rows belong to the first observable,\n    the second #order rows belong to the second observable etc.\n    Results are being stored in the f_matrix and g_matrix attributes respectively\n\n    Args:\n        nof_observables (int): Dimension of observation\n        order (int): Polynomial order (0=constant, 1=linear, 2=quadratic etc.)\n\n    \"\"\"\n    e_n = np.append(1, np.zeros(order))[:, None]\n    self.f_matrix = np.kron(np.eye(nof_observables), e_n)\n\n    l_n = np.triu(np.ones((order + 1, order + 1)))\n    self.g_matrix = np.kron(np.eye(nof_observables), l_n)\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.simulate_data","title":"<code>simulate_data(init_state, nof_timesteps)</code>","text":"<p>Simulate data from DLM model.</p> <p>Function to simulate state evolution and corresponding observations according to model as specified through DLM class attributes (F, G, V and W matrices)</p> <p>Parameters:</p> Name Type Description Default <code>init_state</code> <code>ndarray</code> <p>Initial state vector to start simulating from of size [nof_state_parameters x 1]</p> required <code>nof_timesteps</code> <code>int</code> <p>Number of timesteps to simulate</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>ndarray</code> <p>Simulated state vectors of size [nof_state_parameters x nof_timesteps]</p> <code>obs</code> <code>ndarray</code> <p>Simulated observations of size [nof_observables x nof_timesteps]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def simulate_data(self, init_state: np.ndarray, nof_timesteps: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Simulate data from DLM model.\n\n    Function to simulate state evolution and corresponding observations according to model as specified through DLM\n    class attributes (F, G, V and W matrices)\n\n    Args:\n        init_state (np.ndarray): Initial state vector to start simulating from of size [nof_state_parameters x 1]\n        nof_timesteps (int): Number of timesteps to simulate\n\n    Returns:\n        state (np.ndarray): Simulated state vectors of size [nof_state_parameters x nof_timesteps]\n        obs (np.ndarray): Simulated observations of size [nof_observables x nof_timesteps]\n\n    \"\"\"\n    if self.f_matrix is None or self.g_matrix is None or self.v_matrix is None or self.w_matrix is None:\n        raise ValueError(\"Please specify all matrices (F, G, V and W)\")\n\n    obs = np.empty((self.nof_observables, nof_timesteps))\n    state = np.empty((self.nof_state_parameters, nof_timesteps))\n\n    state[:, [0]] = init_state\n    mean_state_noise = np.zeros(self.nof_state_parameters)\n    mean_observation_noise = np.zeros(self.nof_observables)\n\n    random_generator = np.random.default_rng(seed=None)\n\n    for i in range(nof_timesteps):\n        if i == 0:\n            state[:, [i]] = (\n                self.g_matrix @ init_state\n                + random_generator.multivariate_normal(mean_state_noise, self.w_matrix, size=1).T\n            )\n        else:\n            state[:, [i]] = (\n                self.g_matrix @ state[:, [i - 1]]\n                + random_generator.multivariate_normal(mean_state_noise, self.w_matrix, size=1).T\n            )\n        obs[:, [i]] = (\n            self.f_matrix.T @ state[:, [i]]\n            + random_generator.multivariate_normal(mean_observation_noise, self.v_matrix, size=1).T\n        )\n\n    return state, obs\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.forecast_mean","title":"<code>forecast_mean(current_mean_state, forecast_steps=1)</code>","text":"<p>Perform forecasting of the state and observation mean parameters.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), corollary 4.1, assuming F and G are constant over time. Note that in the output the second axis of the output arrays is the forecast dimension consistent with the forecast steps input, all forecast steps contained in the forecast steps argument are returned.</p> <p>Parameters:</p> Name Type Description Default <code>current_mean_state</code> <code>ndarray</code> <p>Current mean parameter for the state of size [nof_state_parameters x 1]</p> required <code>forecast_steps</code> <code>Union[int, list, ndarray]</code> <p>Steps ahead to forecast</p> <code>1</code> <p>Returns:</p> Name Type Description <code>a_t_k</code> <code>array</code> <p>Forecast values of state mean parameter of the size [nof_observables x size(forecast_steps)]</p> <code>f_t_k</code> <code>array</code> <p>Forecast values of observation mean parameter of the size [nof_observables x size(forecast_steps)]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def forecast_mean(\n    self, current_mean_state: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform forecasting of the state and observation mean parameters.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), corollary 4.1, assuming F and G are\n    constant over time.\n    Note that in the output the second axis of the output arrays is the forecast dimension consistent with the\n    forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n\n    Args:\n        current_mean_state (np.ndarray): Current mean parameter for the state of size [nof_state_parameters x 1]\n        forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n    Returns:\n        a_t_k (np.array): Forecast values of state mean parameter of the size\n            [nof_observables x size(forecast_steps)]\n        f_t_k (np.array): Forecast values of observation mean parameter of the size\n            [nof_observables x size(forecast_steps)]\n\n    \"\"\"\n    min_forecast = np.amin(forecast_steps)\n\n    if min_forecast &lt; 1:\n        raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n    if isinstance(forecast_steps, int):\n        forecast_steps = [forecast_steps]\n\n    a_t_k = np.hstack([self.g_power[:, :, step] @ current_mean_state for step in forecast_steps])\n    f_t_k = self.f_matrix.T @ a_t_k\n\n    return a_t_k, f_t_k\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.forecast_covariance","title":"<code>forecast_covariance(c_matrix, forecast_steps=1)</code>","text":"<p>Perform forecasting of the state and observation covariance parameters.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are constant over time. Note that in the output the third axis of the output arrays is the forecast dimension consistent with the forecast steps input, all forecast steps contained in the forecast steps argument are returned. sum_g_w_g is initialized as G^k @ W @ G^k for k==0, hence we initialize as W Because of zero based indexing, in the for loop i==1 means 2-step ahead forecast which requires element (i+1) of the g_power attribute as the third dimension serves as the actual power of the G matrix</p> <p>Parameters:</p> Name Type Description Default <code>c_matrix</code> <code>ndarray</code> <p>Current posterior covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>forecast_steps</code> <code>Union[int, list, ndarray]</code> <p>Steps ahead to forecast</p> <code>1</code> <p>Returns:</p> Name Type Description <code>r_t_k</code> <code>array</code> <p>Forecast values of estimated prior state covariance of the size [nof_state_parameters x nof_state_parameters x size(forecast_steps)]</p> <code>q_t_k</code> <code>array</code> <p>Forecast values of estimated observation covariance of the size [nof_observables x nof_observables x size(forecast_steps)]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def forecast_covariance(\n    self, c_matrix: np.ndarray, forecast_steps: Union[int, list, np.ndarray] = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform forecasting of the state and observation covariance parameters.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n    constant over time.\n    Note that in the output the third axis of the output arrays is the forecast dimension consistent with the\n    forecast steps input, all forecast steps contained in the forecast steps argument are returned.\n    sum_g_w_g is initialized as G^k @ W @ G^k for k==0, hence we initialize as W\n    Because of zero based indexing, in the for loop i==1 means 2-step ahead forecast which requires element\n    (i+1) of the g_power attribute as the third dimension serves as the actual power of the G matrix\n\n    Args:\n        c_matrix (np.ndarray): Current posterior covariance estimate for the state of size\n            [nof_state_parameters x nof_state_parameters]\n        forecast_steps (Union[int, list, np.ndarray], optional): Steps ahead to forecast\n\n    Returns:\n        r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n            [nof_state_parameters x nof_state_parameters x size(forecast_steps)]\n        q_t_k (np.array): Forecast values of estimated observation covariance of the size\n            [nof_observables x nof_observables x size(forecast_steps)]\n\n    \"\"\"\n    min_forecast = np.amin(forecast_steps)\n    max_forecast = np.amax(forecast_steps)\n\n    if min_forecast &lt; 1:\n        raise ValueError(f\"Minimum forecast should be &gt;= 1, currently it is {min_forecast}\")\n    if isinstance(forecast_steps, int):\n        forecast_steps = [forecast_steps]\n\n    sum_g_w_g = np.zeros((self.nof_state_parameters, self.nof_state_parameters, max_forecast))\n    sum_g_w_g[:, :, 0] = self.w_matrix\n    for i in np.arange(1, max_forecast, step=1):\n        sum_g_w_g[:, :, i] = (\n            sum_g_w_g[:, :, i - 1] + self.g_power[:, :, i] @ self.w_matrix @ self.g_power[:, :, i].T\n        )\n\n    r_t_k = np.dstack(\n        [\n            self.g_power[:, :, step] @ c_matrix @ self.g_power[:, :, step].T + sum_g_w_g[:, :, step - 1]\n            for step in forecast_steps\n        ]\n    )\n    q_t_k = np.dstack(\n        [self.f_matrix.T @ r_t_k[:, :, idx] @ self.f_matrix + self.v_matrix for idx in range(r_t_k.shape[2])]\n    )\n\n    return r_t_k, q_t_k\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.update_posterior","title":"<code>update_posterior(a_t, r_matrix_t, q_matrix_t, error)</code>","text":"<p>Update of the posterior mean and covariance of the state.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are constant over time. We are using a solver instead of calculating the inverse of Q directly Setting inf values in Q equal to 0 after the solver function for computational issues, otherwise we would get 0 * inf = nan, where we want the result to be 0.</p> <p>Parameters:</p> Name Type Description Default <code>a_t</code> <code>ndarray</code> <p>Current prior mean of the state of size [nof_state_parameters x 1]</p> required <code>r_matrix_t</code> <code>ndarray</code> <p>Current prior covariance of the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>q_matrix_t</code> <code>ndarray</code> <p>Current one step ahead forecast covariance estimate of the observations of size [nof_observables x nof_observables]</p> required <code>error</code> <code>ndarray</code> <p>Error associated with the one step ahead forecast (observation - forecast) of size [nof_observables x 1]</p> required <p>Returns:</p> Name Type Description <code>m_t</code> <code>array</code> <p>Posterior mean estimate of the state of size [nof_state_parameters x 1]</p> <code>c_matrix</code> <code>array</code> <p>Posterior covariance estimate of the state of size [nof_state_parameters x nof_state_parameters]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def update_posterior(\n    self, a_t: np.ndarray, r_matrix_t: np.ndarray, q_matrix_t: np.ndarray, error: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Update of the posterior mean and covariance of the state.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast Distributions), assuming F, G, V and W are\n    constant over time.\n    We are using a solver instead of calculating the inverse of Q directly\n    Setting inf values in Q equal to 0 after the solver function for computational issues, otherwise we would\n    get 0 * inf = nan, where we want the result to be 0.\n\n    Args:\n        a_t (np.ndarray): Current prior mean of the state of size [nof_state_parameters x 1]\n        r_matrix_t (np.ndarray): Current prior covariance of the state of size [nof_state_parameters x nof_state_parameters]\n        q_matrix_t (np.ndarray): Current one step ahead forecast covariance estimate of the observations of size [nof_observables x nof_observables]\n        error (np.ndarray): Error associated with the one step ahead forecast (observation - forecast) of size [nof_observables x 1]\n\n    Returns:\n        m_t (np.array): Posterior mean estimate of the state of size [nof_state_parameters x 1]\n        c_matrix (np.array): Posterior covariance estimate of the state of size [nof_state_parameters x nof_state_parameters]\n\n    \"\"\"\n    if self.nof_state_parameters == 1:\n        a_matrix_t = r_matrix_t @ self.f_matrix.T @ (1 / q_matrix_t)\n    else:\n        a_matrix_t = r_matrix_t @ np.linalg.solve(q_matrix_t.T, self.f_matrix.T).T\n    m_t = a_t + a_matrix_t @ error\n    q_matrix_t[np.isinf(q_matrix_t)] = 0\n    c_matrix = r_matrix_t - a_matrix_t @ q_matrix_t @ a_matrix_t.T\n\n    return m_t, c_matrix\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.dlm_full_update","title":"<code>dlm_full_update(new_observation, current_mean_state, current_cov_state, mode='learn')</code>","text":"<p>Perform 1 step of the full DLM update.</p> <p>Following Harrison and West (2nd ed) we perform all steps to update the entire DLM model and obtain new estimates for all parameters involved, including nan value handling. When mode == 'learn' the parameters are updated, when mode == 'ignore' the current observation is ignored and the posterior is set equal to the prior When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance) terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively, we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new forecast gets increased. We set the error equal to zero for computational issues, first but finally set it equal to nan in the end.</p> <p>Parameters:</p> Name Type Description Default <code>new_observation</code> <code>ndarray</code> <p>New observations to use in the updating of the estimates of size [nof_observables x 1]</p> required <code>current_mean_state</code> <code>ndarray</code> <p>Current mean estimate for the state of size [nof_state_parameters x 1]</p> required <code>current_cov_state</code> <code>ndarray</code> <p>Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>mode</code> <code>str</code> <p>String indicating whether the DLM needs to be updated using the new observation or not. Currently, <code>learn</code> and <code>ignore</code> are implemented</p> <code>'learn'</code> <p>Returns:</p> Name Type Description <code>new_mean_state</code> <code>ndarray</code> <p>New mean estimate for the state of size [nof_state_parameters x 1]</p> <code>new_cov_state</code> <code>ndarray</code> <p>New covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> <code>error</code> <code>ndarray</code> <p>Error between the observation and the forecast (observation - forecast) of size [nof_observables x 1]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def dlm_full_update(\n    self,\n    new_observation: np.ndarray,\n    current_mean_state: np.ndarray,\n    current_cov_state: np.ndarray,\n    mode: str = \"learn\",\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform 1 step of the full DLM update.\n\n    Following Harrison and West (2nd ed) we perform all steps to update the entire DLM model and obtain new\n    estimates for all parameters involved, including nan value handling.\n    When mode == 'learn' the parameters are updated, when mode == 'ignore' the current observation is ignored and\n    the posterior is set equal to the prior\n    When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n    such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n    terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n    we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n    forecast gets increased. We set the error equal to zero for computational issues, first but finally set it equal\n    to nan in the end.\n\n    Args:\n        new_observation (np.ndarray): New observations to use in the updating of the estimates of size [nof_observables x 1]\n        current_mean_state (np.ndarray):  Current mean estimate for the state of size [nof_state_parameters x 1]\n        current_cov_state (np.ndarray):  Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n        mode (str, optional): String indicating whether the DLM needs to be updated using the new observation or not. Currently, `learn` and `ignore` are implemented\n\n    Returns:\n        new_mean_state (np.ndarray): New mean estimate for the state of size [nof_state_parameters x 1]\n        new_cov_state (np.ndarray): New covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]\n        error (np.ndarray): Error between the observation and the forecast (observation - forecast) of size [nof_observables x 1]\n\n    \"\"\"\n    a_t, f_t = self.forecast_mean(current_mean_state, forecast_steps=1)\n    r_matrix_t, q_matrix_t = self.forecast_covariance(current_cov_state, forecast_steps=1)\n    error = new_observation - f_t\n\n    nan_bool = np.isnan(new_observation)\n    nan_idx = np.argwhere(nan_bool.flatten())\n    if np.any(nan_bool):\n        q_matrix_t[nan_idx, :, 0] -= self.v_matrix[nan_idx, :]\n        q_matrix_t[:, nan_idx, 0] -= self.v_matrix[:, nan_idx]\n        q_matrix_t[nan_idx, nan_idx, 0] = np.inf\n        error[nan_idx] = 0\n\n    if mode == \"learn\":\n        new_mean_state, new_cov_state = self.update_posterior(a_t, r_matrix_t[:, :, 0], q_matrix_t[:, :, 0], error)\n    elif mode == \"ignore\":\n        new_mean_state = a_t\n        new_cov_state = r_matrix_t\n    else:\n        raise TypeError(f\"Mode {mode} not implemented\")\n\n    error[nan_idx] = np.nan\n\n    return new_mean_state, new_cov_state, error\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.calculate_mahalanobis_distance","title":"<code>calculate_mahalanobis_distance(new_observations, current_mean_state, current_cov_state, forecast_steps=1, return_statistics=False)</code>","text":"<p>Calculate the mahalanobis distance.</p> <p>Calculating the Mahalanobis distance which is defined as error.T @ covariance^(-1) @ error The error is flatted in row-major (C-style) This returns the stacked rows, which in our case is the errors per observation parameter stacked and this is exactly what we want: array([[1, 2], [3, 4]]).reshape((-1, 1), order='C') becomes column array([1, 2 3, 4]) Using a solve method instead of calculating inverse matrices directly When calculating mhd_per_obs_param we use the partial result and reshape the temporary output such that we can sum the correct elements associated with the same observable together When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance) terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively, we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new forecast gets increased. We set the error equal to zero for computational issues, but this does decrease the number of degrees of freedom for that particular Mahalanobis distance calculation, basically decreasing the Mahalanobis distance. We allow the option to output the number of degrees of freedom and chi2 statistic which allows to take this decrease in degrees of freedom into account.</p> <p>Parameters:</p> Name Type Description Default <code>new_observations</code> <code>ndarray</code> <p>New observations to use in the calculation of the mahalanobis distance of size [nof_observables x forecast_steps]</p> required <code>current_mean_state</code> <code>ndarray</code> <p>Current mean estimate for the state of size [nof_state_parameters x 1]</p> required <code>current_cov_state</code> <code>ndarray</code> <p>Current covariance estimate for the state of size [nof_state_parameters x nof_state_parameters]</p> required <code>forecast_steps</code> <code>int</code> <p>Number of steps ahead to forecast and use in the mahalanobis distance calculation</p> <code>1</code> <code>return_statistics</code> <code>bool</code> <p>Boolean to return used degrees of freedom and chi2 statistic</p> <code>False</code> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def calculate_mahalanobis_distance(\n    self,\n    new_observations: np.ndarray,\n    current_mean_state: np.ndarray,\n    current_cov_state: np.ndarray,\n    forecast_steps: int = 1,\n    return_statistics=False,\n) -&gt; Union[Tuple[float, np.ndarray], Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Calculate the mahalanobis distance.\n\n    Calculating the Mahalanobis distance which is defined as error.T @ covariance^(-1) @ error\n    The error is flatted in row-major (C-style) This returns the stacked rows, which in our case is the errors per\n    observation parameter stacked and this is exactly what we want: array([[1, 2], [3, 4]]).reshape((-1, 1),\n    order='C') becomes column array([1, 2 3, 4])\n    Using a solve method instead of calculating inverse matrices directly\n    When calculating mhd_per_obs_param we use the partial result and reshape the temporary output such that we can\n    sum the correct elements associated with the same observable together\n    When no observation is present (i.e. a nan value) we let the covariance (V matrix) for that particular sensor\n    such that we set the variance of that sensor for that time instance to infinity and set all cross (covariance)\n    terms to 0. Instead of changing this in the V matrix, we simply adjust the Q matrix accordingly. Effectively,\n    we set the posterior equal to the prior for that particular sensor and the uncertainty associated with the new\n    forecast gets increased. We set the error equal to zero for computational issues, but this does decrease the\n    number of degrees of freedom for that particular Mahalanobis distance calculation, basically decreasing the\n    Mahalanobis distance. We allow the option to output the number of degrees of freedom and chi2 statistic which\n    allows to take this decrease in degrees of freedom into account.\n\n    Args:\n        new_observations (np.ndarray): New observations to use in the calculation of the mahalanobis distance of\n            size [nof_observables x forecast_steps]\n        current_mean_state (np.ndarray): Current mean estimate for the state of size [nof_state_parameters x 1]\n        current_cov_state (np.ndarray): Current covariance estimate for the state of size\n            [nof_state_parameters x nof_state_parameters]\n        forecast_steps (int, optional): Number of steps ahead to forecast and use in the mahalanobis distance\n            calculation\n        return_statistics (bool, optional): Boolean to return used degrees of freedom and chi2 statistic\n    Returns:\n        mhd_overall (float): mahalanobis distance over all observables\n        mhd_per_obs_param (np.ndarray): mahalanobis distance per observation parameter of size [nof_observables, 1]\n\n    \"\"\"\n    if forecast_steps &lt;= 0:\n        raise AttributeError(\"Forecast steps should be a positive integer\")\n\n    if new_observations.size / self.nof_observables != forecast_steps:\n        raise AttributeError(\"Sizes of new observations and forecast steps are not aligning\")\n\n    _, f_t_k = self.forecast_mean(current_mean_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n    if new_observations.shape != f_t_k.shape:\n        raise AttributeError(\"Dimensions of new_observations are not aligning with dimensions of forecast\")\n\n    error = np.subtract(new_observations, f_t_k).reshape((-1, 1), order=\"C\")\n\n    r_t_k, q_t_k = self.forecast_covariance(current_cov_state, forecast_steps=np.array(range(forecast_steps)) + 1)\n\n    nan_bool = np.isnan(new_observations)\n    if np.any(nan_bool):\n        nan_idx = np.argwhere(nan_bool)\n        for value in nan_idx:\n            q_t_k[value[0], :, value[1]] -= self.v_matrix[value[0], :]\n            q_t_k[:, value[0], value[1]] -= self.v_matrix[:, value[0]]\n\n        q_t_k[nan_idx[:, 0], nan_idx[:, 0], nan_idx[:, 1]] = np.inf\n        error[nan_bool.reshape((-1, 1), order=\"C\")] = 0\n\n    if forecast_steps &gt; 1:\n        full_covariance = self.create_full_covariance(r_t_k=r_t_k, q_t_k=q_t_k, forecast_steps=forecast_steps)\n    else:\n        full_covariance = q_t_k[:, :, 0]\n\n    mhd_overall = mahalanobis_distance(error=error, cov_matrix=full_covariance)\n    mhd_per_obs_param = np.empty((self.nof_observables, 1))\n\n    for i_obs in range(self.nof_observables):\n        ind_hrz = np.array(range(forecast_steps)) + i_obs * forecast_steps\n        mhd_per_obs_param[i_obs] = mahalanobis_distance(\n            error=error[ind_hrz], cov_matrix=full_covariance[np.ix_(ind_hrz, ind_hrz)]\n        )\n\n    if self.nof_observables == 1:\n        mhd_per_obs_param = mhd_per_obs_param.item()\n\n    if return_statistics:\n        dof_per_obs_param = (nan_bool.shape[1] - np.count_nonzero(nan_bool, axis=1)).reshape(\n            self.nof_observables, 1\n        )\n        dof_overall = dof_per_obs_param.sum()\n        chi2_cdf_per_obs_param = chi2.cdf(mhd_per_obs_param.flatten(), dof_per_obs_param.flatten()).reshape(\n            self.nof_observables, 1\n        )\n        chi2_cdf_overall = chi2.cdf(mhd_overall, dof_overall)\n\n        return (\n            mhd_overall,\n            mhd_per_obs_param,\n            dof_overall,\n            dof_per_obs_param,\n            chi2_cdf_overall,\n            chi2_cdf_per_obs_param,\n        )\n\n    return mhd_overall, mhd_per_obs_param\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.DLM.create_full_covariance","title":"<code>create_full_covariance(r_t_k, q_t_k, forecast_steps)</code>","text":"<p>Helper function to construct the full covariance matrix.</p> <p>Following Harrison and West (2nd ed) Chapter 4.4 (Forecast distributions) Theorem 4.2 and corollary 4.2 we construct the full covariance matrix. This full covariance matrix is the covariance matrix of all forecasted observations with respect to each other. Hence, it's COV[Y_{t+k}, Y_{t+j}] with j and k 1&lt;=j,k&lt;=forecast steps input argument and Y_{t+k} the k step ahead forecast of the observation at time t</p> <p>The matrix is build up using the different blocks for different covariances between observations i and j. The diagonals of each block are calculated first as q_t_k[i, j, :]. Next the i, j-th (lower triangular) entry of the m, n-th block is calculated as (F.T @ G^(i-j) r_t_k[:, :, j] @ F)[i, j] Next each upper triangular part of each lower diagonal block is calculated and next the entire upper triangular part of the full matrix is calculated</p> <p>Parameters:</p> Name Type Description Default <code>r_t_k</code> <code>array</code> <p>Forecast values of estimated prior state covariance of the size [nof_state_parameters x nof_state_parameters x forecast_steps]</p> required <code>q_t_k</code> <code>array</code> <p>Forecast values of estimated observation covariance of the size [nof_observables x nof_observables x forecast_steps]</p> required <code>forecast_steps</code> <code>int</code> <p>Maximum number of steps ahead to forecast and use all of those in the mahalanobis distance calculation</p> required <p>Returns:</p> Name Type Description <code>full_covariance</code> <code>array</code> <p>Full covariance matrix of all forecasted observations with respect to each other</p> <code>ndarray</code> <p>having size [(nof_observables * forecast_steps) X (nof_observables * forecast_steps)]</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def create_full_covariance(self, r_t_k: np.ndarray, q_t_k: np.ndarray, forecast_steps: int) -&gt; np.ndarray:\n    \"\"\"Helper function to construct the full covariance matrix.\n\n    Following Harrison and West (2nd ed) Chapter 4.4 (Forecast distributions) Theorem 4.2 and corollary 4.2\n    we construct the full covariance matrix. This full covariance matrix is the covariance matrix of all forecasted\n    observations with respect to each other. Hence, it's COV[Y_{t+k}, Y_{t+j}] with j and k 1&lt;=j,k&lt;=forecast steps\n    input argument and Y_{t+k} the k step ahead forecast of the observation at time t\n\n    The matrix is build up using the different blocks for different covariances between observations i and j.\n    The diagonals of each block are calculated first as q_t_k[i, j, :].\n    Next the i, j-th (lower triangular) entry of the m, n-th block is calculated as\n    (F.T @ G^(i-j) r_t_k[:, :, j] @ F)[i, j]\n    Next each upper triangular part of each lower diagonal block is calculated and next the entire upper triangular\n    part of the full matrix is calculated\n\n    Args:\n        r_t_k (np.array): Forecast values of estimated prior state covariance of the size\n            [nof_state_parameters x nof_state_parameters x forecast_steps]\n        q_t_k (np.array): Forecast values of estimated observation covariance of the size\n            [nof_observables x nof_observables x forecast_steps]\n        forecast_steps (int): Maximum number of steps ahead to forecast and use all of those in the mahalanobis\n            distance calculation\n\n    Returns:\n        full_covariance (np.array): Full covariance matrix of all forecasted observations with respect to each other\n        having size [(nof_observables * forecast_steps) X (nof_observables * forecast_steps)]\n\n    \"\"\"\n    full_covariance = np.zeros((forecast_steps * self.nof_observables, forecast_steps * self.nof_observables))\n    base_idx = np.array(range(forecast_steps))\n    for block_i in range(self.nof_observables):\n        for block_j in range(block_i + 1):\n            block_rows = base_idx + block_i * forecast_steps\n            block_cols = base_idx + block_j * forecast_steps\n            full_covariance[block_rows, block_cols] = q_t_k[block_i, block_j, :]\n\n    temp_idx = np.array(range(self.nof_observables))\n    for sub_i in np.arange(start=1, stop=forecast_steps, step=1):\n        sub_row = temp_idx * forecast_steps + sub_i\n        for sub_j in range(sub_i):\n            sub_col = temp_idx * forecast_steps + sub_j\n            sub_idx = np.ix_(sub_row, sub_col)\n            full_covariance[sub_idx] = (\n                self.f_matrix.T @ self.g_power[:, :, sub_i - sub_j] @ r_t_k[:, :, sub_j] @ self.f_matrix\n            )\n\n    for block_i in range(self.nof_observables):\n        for block_j in range(block_i):\n            block_rows = base_idx + block_i * forecast_steps\n            block_cols = base_idx + block_j * forecast_steps\n            block_idx = np.ix_(block_rows, block_cols)\n            full_covariance[block_idx] = full_covariance[block_idx] + np.tril(full_covariance[block_idx], k=-1).T\n\n    full_covariance = np.tril(full_covariance) + np.tril(full_covariance, k=-1).T\n\n    return full_covariance\n</code></pre>"},{"location":"pyelq/dlm/#pyelq.dlm.mahalanobis_distance","title":"<code>mahalanobis_distance(error, cov_matrix)</code>","text":"<p>Calculate Mahalanobis distance for multivariate observations.</p> <p>m = e.T @ inv(cov) @ e Sometimes the solution does not exist when np.inf value is present in cov_matrix (computational limitations?) Hence, we set it to a large value instead</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>ndarray</code> <p>n x p   observation error</p> required <code>cov_matrix</code> <code>ndarray</code> <p>p x p covariance matrix</p> required <p>Returns:</p> Type Description <code>float</code> <p>np.ndarray: n x 1  mahalanobis distance score for each observation</p> Source code in <code>src/pyelq/dlm.py</code> <pre><code>def mahalanobis_distance(error: np.ndarray, cov_matrix: np.ndarray) -&gt; float:\n    \"\"\"Calculate Mahalanobis distance for multivariate observations.\n\n    m = e.T @ inv(cov) @ e\n    Sometimes the solution does not exist when np.inf value is present in cov_matrix (computational limitations?)\n    Hence, we set it to a large value instead\n\n    Args:\n        error (np.ndarray):  n x p   observation error\n        cov_matrix (np.ndarray): p x p covariance matrix\n\n    Returns:\n        np.ndarray: n x 1  mahalanobis distance score for each observation\n\n    \"\"\"\n    if cov_matrix.size == 1:\n        return error.item() ** 2 / cov_matrix.item()\n\n    partial_solution = np.linalg.solve(cov_matrix, error)\n    if np.any(np.isnan(partial_solution)):\n        cov_matrix[np.isinf(cov_matrix)] = 1e100\n        partial_solution = np.linalg.solve(cov_matrix, error)\n\n    return np.sum(error * partial_solution, axis=0).item()\n</code></pre>"},{"location":"pyelq/gas_species/","title":"Gas Species","text":""},{"location":"pyelq/gas_species/#gas-species","title":"Gas Species","text":"<p>Gas Species module.</p> <p>The superclass for the Gas species classes. It contains a few gas species with its properties and functionality to calculate the density of the gas and do emission rate conversions from m^3/s to kg/hr and back</p>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies","title":"<code>GasSpecies</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the properties of a particular gas species.</p> <p>Attributes:</p> Name Type Description <code>global_background</code> <code>float</code> <p>Global background concentration [ppm]</p> <code>half_life</code> <code>float</code> <p>Half life of gas [hr]</p> <code>__molar_gas_constant</code> <code>float</code> <p>R, molar gas constant [JK^-1mol^-1]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass GasSpecies(ABC):\n    \"\"\"Defines the properties of a particular gas species.\n\n    Attributes:\n        global_background (float, optional): Global background concentration [ppm]\n        half_life (float, optional): Half life of gas [hr]\n        __molar_gas_constant (float): R, molar gas constant [JK^-1mol^-1]\n\n    \"\"\"\n\n    global_background: float = field(init=False)\n    half_life: float = field(init=False)\n    __molar_gas_constant: float = 8.31446261815324\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Str: Name of gas.\"\"\"\n\n    @property\n    @abstractmethod\n    def molar_mass(self) -&gt; float:\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n\n    @property\n    @abstractmethod\n    def formula(self) -&gt; str:\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n\n    def gas_density(\n        self, temperature: Union[np.ndarray, float] = 273.15, pressure: Union[np.ndarray, float] = 101.325\n    ) -&gt; np.ndarray:\n        \"\"\"Calculating the density of the gas.\n\n        Calculating the density of the gas given temperature and pressure if temperature and pressure are not provided\n        we use Standard Temperature and Pressure (STP).\n\n        https://en.wikipedia.org/wiki/Ideal_gas_law\n\n        Args:\n            temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n                defaults to 273.15 [K]\n            pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n                defaults to 101.325 [kPa]\n\n        Returns:\n             density (np.ndarray): Array of gas density values [kg/m^3]\n\n        \"\"\"\n        specific_gas_constant = self.__molar_gas_constant / self.molar_mass\n        density = np.divide(pressure, (temperature * specific_gas_constant))\n        return density\n\n    def convert_emission_m3s_to_kghr(\n        self,\n        emission_m3s: Union[np.ndarray, float],\n        temperature: Union[np.ndarray, float] = 273.15,\n        pressure: Union[np.ndarray, float] = 101.325,\n    ) -&gt; np.ndarray:\n        \"\"\"Converting emission rates from m^3/s to kg/hr given temperature and pressure.\n\n         If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n        Args:\n            emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n            temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n                defaults to 273.15 [K]\n            pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n                defaults to 101.325 [kPa]\n\n        Returns:\n             emission_kghr (np.ndarray): [p x 1] array of emission rates in  [kg/hr]\n\n        \"\"\"\n        density = self.gas_density(temperature=temperature, pressure=pressure)\n        emission_kghr = np.multiply(emission_m3s, density) * 3600\n        return emission_kghr\n\n    def convert_emission_kghr_to_m3s(\n        self,\n        emission_kghr: Union[np.ndarray, float],\n        temperature: Union[np.ndarray, float] = 273.15,\n        pressure: Union[np.ndarray, float] = 101.325,\n    ) -&gt; np.ndarray:\n        \"\"\"Converting emission rates from  kg/hr to m^3/s given temperature and pressure.\n\n        If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n        Args:\n            emission_kghr (np.ndarray): Array of emission rates in  [kg/hr]\n            temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n                defaults to 273.15 [K]\n            pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n                defaults to 101.325 [kPa]\n\n        Returns:\n             emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n\n        \"\"\"\n        density = self.gas_density(temperature=temperature, pressure=pressure)\n        emission_m3s = np.divide(emission_kghr, density) / 3600\n        return emission_m3s\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.molar_mass","title":"<code>molar_mass</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.formula","title":"<code>formula</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.gas_density","title":"<code>gas_density(temperature=273.15, pressure=101.325)</code>","text":"<p>Calculating the density of the gas.</p> <p>Calculating the density of the gas given temperature and pressure if temperature and pressure are not provided we use Standard Temperature and Pressure (STP).</p> <p>https://en.wikipedia.org/wiki/Ideal_gas_law</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>Union[ndarray, float]</code> <p>Array of temperatures [Kelvin], defaults to 273.15 [K]</p> <code>273.15</code> <code>pressure</code> <code>Union[ndarray, float]</code> <p>Array of pressures [kPa], defaults to 101.325 [kPa]</p> <code>101.325</code> <p>Returns:</p> Name Type Description <code>density</code> <code>ndarray</code> <p>Array of gas density values [kg/m^3]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>def gas_density(\n    self, temperature: Union[np.ndarray, float] = 273.15, pressure: Union[np.ndarray, float] = 101.325\n) -&gt; np.ndarray:\n    \"\"\"Calculating the density of the gas.\n\n    Calculating the density of the gas given temperature and pressure if temperature and pressure are not provided\n    we use Standard Temperature and Pressure (STP).\n\n    https://en.wikipedia.org/wiki/Ideal_gas_law\n\n    Args:\n        temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n            defaults to 273.15 [K]\n        pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n            defaults to 101.325 [kPa]\n\n    Returns:\n         density (np.ndarray): Array of gas density values [kg/m^3]\n\n    \"\"\"\n    specific_gas_constant = self.__molar_gas_constant / self.molar_mass\n    density = np.divide(pressure, (temperature * specific_gas_constant))\n    return density\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.convert_emission_m3s_to_kghr","title":"<code>convert_emission_m3s_to_kghr(emission_m3s, temperature=273.15, pressure=101.325)</code>","text":"<p>Converting emission rates from m^3/s to kg/hr given temperature and pressure.</p> <p>If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).</p> <p>Parameters:</p> Name Type Description Default <code>emission_m3s</code> <code>Union[ndarray, float]</code> <p>Array of emission rates [m^3/s]</p> required <code>temperature</code> <code>Union[ndarray, float]</code> <p>Array of temperatures [Kelvin], defaults to 273.15 [K]</p> <code>273.15</code> <code>pressure</code> <code>Union[ndarray, float]</code> <p>Array of pressures [kPa], defaults to 101.325 [kPa]</p> <code>101.325</code> <p>Returns:</p> Name Type Description <code>emission_kghr</code> <code>ndarray</code> <p>[p x 1] array of emission rates in  [kg/hr]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>def convert_emission_m3s_to_kghr(\n    self,\n    emission_m3s: Union[np.ndarray, float],\n    temperature: Union[np.ndarray, float] = 273.15,\n    pressure: Union[np.ndarray, float] = 101.325,\n) -&gt; np.ndarray:\n    \"\"\"Converting emission rates from m^3/s to kg/hr given temperature and pressure.\n\n     If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n    Args:\n        emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n        temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n            defaults to 273.15 [K]\n        pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n            defaults to 101.325 [kPa]\n\n    Returns:\n         emission_kghr (np.ndarray): [p x 1] array of emission rates in  [kg/hr]\n\n    \"\"\"\n    density = self.gas_density(temperature=temperature, pressure=pressure)\n    emission_kghr = np.multiply(emission_m3s, density) * 3600\n    return emission_kghr\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.GasSpecies.convert_emission_kghr_to_m3s","title":"<code>convert_emission_kghr_to_m3s(emission_kghr, temperature=273.15, pressure=101.325)</code>","text":"<p>Converting emission rates from  kg/hr to m^3/s given temperature and pressure.</p> <p>If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).</p> <p>Parameters:</p> Name Type Description Default <code>emission_kghr</code> <code>ndarray</code> <p>Array of emission rates in  [kg/hr]</p> required <code>temperature</code> <code>Union[ndarray, float]</code> <p>Array of temperatures [Kelvin], defaults to 273.15 [K]</p> <code>273.15</code> <code>pressure</code> <code>Union[ndarray, float]</code> <p>Array of pressures [kPa], defaults to 101.325 [kPa]</p> <code>101.325</code> <p>Returns:</p> Name Type Description <code>emission_m3s</code> <code>Union[ndarray, float]</code> <p>Array of emission rates [m^3/s]</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>def convert_emission_kghr_to_m3s(\n    self,\n    emission_kghr: Union[np.ndarray, float],\n    temperature: Union[np.ndarray, float] = 273.15,\n    pressure: Union[np.ndarray, float] = 101.325,\n) -&gt; np.ndarray:\n    \"\"\"Converting emission rates from  kg/hr to m^3/s given temperature and pressure.\n\n    If temperature and pressure are not provided we use Standard Temperature and Pressure (STP).\n\n    Args:\n        emission_kghr (np.ndarray): Array of emission rates in  [kg/hr]\n        temperature (Union[np.ndarray, float], optional): Array of temperatures [Kelvin],\n            defaults to 273.15 [K]\n        pressure (Union[np.ndarray, float], optional): Array of pressures [kPa],\n            defaults to 101.325 [kPa]\n\n    Returns:\n         emission_m3s (Union[np.ndarray, float]): Array of emission rates [m^3/s]\n\n    \"\"\"\n    density = self.gas_density(temperature=temperature, pressure=pressure)\n    emission_m3s = np.divide(emission_kghr, density) / 3600\n    return emission_m3s\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4","title":"<code>CH4</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of CH4.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass CH4(GasSpecies):\n    \"\"\"Defines the properties of CH4.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Methane\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 16.04246\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"CH4\"\n\n    global_background = 1.85\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CH4.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6","title":"<code>C2H6</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of C2H6.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass C2H6(GasSpecies):\n    \"\"\"Defines the properties of C2H6.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Ethane\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 30.06904\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"C2H6\"\n\n    global_background = 5e-4\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C2H6.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8","title":"<code>C3H8</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of C3H8.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass C3H8(GasSpecies):\n    \"\"\"Defines the properties of C3H8.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Propane\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 46.0055\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"C3H8\"\n\n    global_background = 5e-4\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.C3H8.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2","title":"<code>CO2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of CO2.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass CO2(GasSpecies):\n    \"\"\"Defines the properties of CO2.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Carbon Dioxide\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 44.0095\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"CO2\"\n\n    global_background = 400\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.CO2.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2","title":"<code>NO2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of NO2.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass NO2(GasSpecies):\n    \"\"\"Defines the properties of NO2.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Nitrogen Dioxide\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 46.0055\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"NO2\"\n\n    global_background = 0\n    half_life = 12\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.NO2.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.H2","title":"<code>H2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GasSpecies</code></p> <p>Defines the properties of H2.</p> Source code in <code>src/pyelq/gas_species.py</code> <pre><code>@dataclass\nclass H2(GasSpecies):\n    \"\"\"Defines the properties of H2.\"\"\"\n\n    @property\n    def name(self):\n        \"\"\"Str: Name of gas.\"\"\"\n        return \"Hydrogen\"\n\n    @property\n    def molar_mass(self):\n        \"\"\"Float: Molar Mass [g/mol].\"\"\"\n        return 2.01568\n\n    @property\n    def formula(self):\n        \"\"\"Str: Chemical formula of gas.\"\"\"\n        return \"H2\"\n\n    global_background = 0.5\n</code></pre>"},{"location":"pyelq/gas_species/#pyelq.gas_species.H2.name","title":"<code>name</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.H2.molar_mass","title":"<code>molar_mass</code>  <code>property</code>","text":""},{"location":"pyelq/gas_species/#pyelq.gas_species.H2.formula","title":"<code>formula</code>  <code>property</code>","text":""},{"location":"pyelq/meteorology/","title":"Meteorology","text":""},{"location":"pyelq/meteorology/#meteorology","title":"Meteorology","text":"<p>Meteorology module.</p> <p>The superclass for the meteorology classes</p>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology","title":"<code>Meteorology</code>  <code>dataclass</code>","text":"<p>Defines the properties and methods of the meteorology class.</p> <p>Sizes of all attributes should match.</p> <p>Attributes:</p> Name Type Description <code>wind_speed</code> <code>ndarray</code> <p>Wind speed [m/s]</p> <code>wind_direction</code> <code>ndarray</code> <p>Meteorological wind direction (from) [deg], see https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398</p> <code>u_component</code> <code>ndarray</code> <p>u component of wind [m/s] in the easterly direction</p> <code>v_component</code> <code>ndarray</code> <p>v component of wind [m/s] in the northerly direction</p> <code>w_component</code> <code>ndarray</code> <p>w component of wind [m/s] in the vertical direction</p> <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Parameter of the wind stability in horizontal direction [deg]</p> <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Parameter of the wind stability in vertical direction [deg]</p> <code>pressure</code> <code>ndarray</code> <p>Pressure [kPa]</p> <code>temperature</code> <code>ndarray</code> <p>Temperature [K]</p> <code>atmospheric_boundary_layer</code> <code>ndarray</code> <p>Atmospheric boundary layer [m]</p> <code>surface_albedo</code> <code>ndarray</code> <p>Surface reflectance parameter [unitless]</p> <code>time</code> <code>DatetimeArray</code> <p>Array containing time values associated with the meteorological observation</p> <code>location</code> <code>Coordinate</code> <p>(Coordinate, optional): Coordinate object specifying the meteorological observation locations</p> <code>label</code> <code>str</code> <p>String label for object</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>@dataclass\nclass Meteorology:\n    \"\"\"Defines the properties and methods of the meteorology class.\n\n    Sizes of all attributes should match.\n\n    Attributes:\n        wind_speed (np.ndarray, optional): Wind speed [m/s]\n        wind_direction (np.ndarray, optional): Meteorological wind direction (from) [deg], see\n            https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n        u_component (np.ndarray, optional): u component of wind [m/s] in the easterly direction\n        v_component (np.ndarray, optional): v component of wind [m/s] in the northerly direction\n        w_component (np.ndarray, optional): w component of wind [m/s] in the vertical direction\n        wind_turbulence_horizontal (np.ndarray, optional): Parameter of the wind stability in\n            horizontal direction [deg]\n        wind_turbulence_vertical (np.ndarray, optional): Parameter of the wind stability in\n            vertical direction [deg]\n        pressure (np.ndarray, optional): Pressure [kPa]\n        temperature (np.ndarray, optional): Temperature [K]\n        atmospheric_boundary_layer (np.ndarray, optional): Atmospheric boundary layer [m]\n        surface_albedo (np.ndarray, optional): Surface reflectance parameter [unitless]\n        time (pandas.arrays.DatetimeArray, optional): Array containing time values associated with the\n            meteorological observation\n        location: (Coordinate, optional): Coordinate object specifying the meteorological observation locations\n        label (str, optional): String label for object\n\n    \"\"\"\n\n    wind_speed: np.ndarray = field(init=False, default=None)\n    wind_direction: np.ndarray = field(init=False, default=None)\n    u_component: np.ndarray = field(init=False, default=None)\n    v_component: np.ndarray = field(init=False, default=None)\n    w_component: np.ndarray = field(init=False, default=None)\n    wind_turbulence_horizontal: np.ndarray = field(init=False, default=None)\n    wind_turbulence_vertical: np.ndarray = field(init=False, default=None)\n    pressure: np.ndarray = field(init=False, default=None)\n    temperature: np.ndarray = field(init=False, default=None)\n    atmospheric_boundary_layer: np.ndarray = field(init=False, default=None)\n    surface_albedo: np.ndarray = field(init=False, default=None)\n    time: DatetimeArray = field(init=False, default=None)\n    location: Coordinate = field(init=False, default=None)\n    label: str = field(init=False)\n\n    @property\n    def nof_observations(self) -&gt; int:\n        \"\"\"Number of observations.\"\"\"\n        if self.location is None:\n            return 0\n        return self.location.nof_observations\n\n    def calculate_wind_speed_from_uv(self) -&gt; None:\n        \"\"\"Calculate wind speed.\n\n        Calculate the wind speed from u and v components. Result gets stored in the wind_speed attribute\n\n        \"\"\"\n        self.wind_speed = np.sqrt(self.u_component**2 + self.v_component**2)\n\n    def calculate_wind_direction_from_uv(self) -&gt; None:\n        \"\"\"Calculate wind direction: meteorological convention 0 is wind from the North.\n\n        Calculate the wind direction from u and v components. Result gets stored in the wind_direction attribute\n        See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n        \"\"\"\n        self.wind_direction = (270 - 180 / np.pi * np.arctan2(self.v_component, self.u_component)) % 360\n\n    def calculate_uv_from_wind_speed_direction(self) -&gt; None:\n        \"\"\"Calculate u and v components from wind speed and direction.\n\n        Results get stored in the u_component and v_component attributes.\n        See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n        \"\"\"\n        self.u_component = -1 * self.wind_speed * np.sin(self.wind_direction * (np.pi / 180))\n        self.v_component = -1 * self.wind_speed * np.cos(self.wind_direction * (np.pi / 180))\n\n    def calculate_wind_turbulence_horizontal(self, window: str) -&gt; None:\n        \"\"\"Calculate the horizontal wind turbulence values from the wind direction attribute.\n\n        Wind turbulence values are calculated as the circular standard deviation of wind direction\n        (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circstd.html).\n        The implementation here is equivalent to using the circstd function from scipy.stats as an apply\n        function on a rolling window. However, using the rolling mean on sin and cos speeds up\n        the calculation by a factor of 100.\n\n        Outputted values are calculated at the center of the window and at least 3 observations are required in a\n        window for the calculation. If the window contains less values the result will be np.nan.\n        The result of the calculation will be stored as the wind_turbulence_horizontal attribute.\n\n        Args:\n            window (str): The size of the window in which values are aggregated specified as an offset alias:\n                https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n\n        \"\"\"\n        data_series = pd.Series(data=self.wind_direction, index=self.time)\n        sin_rolling = (np.sin(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n        cos_rolling = (np.cos(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n        aggregated_data = np.sqrt(-2 * np.log((sin_rolling**2 + cos_rolling**2) ** 0.5)) * 180 / np.pi\n        self.wind_turbulence_horizontal = aggregated_data.values\n\n    def plot_polar_hist(self, nof_sectors: int = 16, nof_divisions: int = 5, template: object = None) -&gt; go.Figure():\n        \"\"\"Plots a histogram of wind speed and wind direction in polar Coordinates.\n\n        Args:\n            nof_sectors (int, optional): The number of wind direction sectors into which the data is binned.\n            nof_divisions (int, optional): The number of wind speed divisions into which the data is binned.\n            template (object): A layout template which can be applied to the plot. Defaults to None.\n\n        Returns:\n            fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n        \"\"\"\n        sector_half_width = 0.5 * (360 / nof_sectors)\n        wind_direction_bin_edges = np.linspace(-sector_half_width, 360 - sector_half_width, nof_sectors + 1)\n        wind_speed_bin_edges = np.linspace(np.min(self.wind_speed), np.max(self.wind_speed), nof_divisions)\n\n        dataframe = pd.DataFrame()\n        dataframe[\"wind_direction\"] = [x - 360 if x &gt; (360 - sector_half_width) else x for x in self.wind_direction]\n        dataframe[\"wind_speed\"] = self.wind_speed\n\n        dataframe[\"sector\"] = pd.cut(dataframe[\"wind_direction\"], wind_direction_bin_edges, include_lowest=True)\n        if np.allclose(wind_speed_bin_edges[0], wind_speed_bin_edges):\n            dataframe[\"speed\"] = wind_speed_bin_edges[0]\n        else:\n            dataframe[\"speed\"] = pd.cut(dataframe[\"wind_speed\"], wind_speed_bin_edges, include_lowest=True)\n\n        dataframe = dataframe.groupby([\"sector\", \"speed\"], observed=False).count()\n        dataframe = dataframe.rename(columns={\"wind_speed\": \"count\"}).drop(columns=[\"wind_direction\"])\n        dataframe[\"%\"] = dataframe[\"count\"] / dataframe[\"count\"].sum()\n\n        dataframe = dataframe.reset_index()\n        dataframe[\"theta\"] = dataframe.apply(lambda x: x[\"sector\"].mid, axis=1)\n\n        fig = px.bar_polar(\n            dataframe,\n            r=\"%\",\n            theta=\"theta\",\n            color=\"speed\",\n            direction=\"clockwise\",\n            start_angle=90,\n            color_discrete_sequence=px.colors.sequential.Sunset_r,\n        )\n\n        ticktext = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n        polar_dict = {\n            \"radialaxis\": {\"tickangle\": 90},\n            \"radialaxis_angle\": 90,\n            \"angularaxis\": {\n                \"tickmode\": \"array\",\n                \"ticktext\": ticktext,\n                \"tickvals\": list(np.linspace(0, 360 - (360 / 8), 8)),\n            },\n        }\n        fig.add_annotation(\n            x=1,\n            y=1,\n            yref=\"paper\",\n            xref=\"paper\",\n            xanchor=\"right\",\n            yanchor=\"top\",\n            align=\"left\",\n            font={\"size\": 18, \"color\": \"#000000\"},\n            showarrow=False,\n            borderwidth=2,\n            borderpad=10,\n            bgcolor=\"#ffffff\",\n            bordercolor=\"#000000\",\n            opacity=0.8,\n            text=\"&lt;b&gt;Radial Axis:&lt;/b&gt; Proportion&lt;br&gt;of wind measurements&lt;br&gt;in a given direction.\",\n        )\n\n        fig.update_layout(polar=polar_dict)\n        fig.update_layout(template=template)\n        fig.update_layout(title=\"Distribution of Wind Speeds and Directions\")\n\n        return fig\n\n    def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n        \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar Coordinates.\n\n        This function implements the polar scatter functionality for a (single) Meteorology object. Assuming the all\n        Sensors in the SensorGroup are consistent with the Meteorology object.\n\n        Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n        in plot colors.\n\n        Args:\n            fig (go.Figure): A plotly figure onto which traces can be drawn.\n            sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n            template (object): A layout template which can be applied to the plot. Defaults to None.\n\n        Returns:\n            fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n        \"\"\"\n        max_concentration = 0\n\n        for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n            if sensor.concentration.shape != self.wind_direction.shape:\n                warnings.warn(\n                    f\"Concentration values for sensor {sensor_key} are of shape \"\n                    + f\"{sensor.concentration.shape}, but self.wind_direction has shape \"\n                    + f\"{self.wind_direction.shape}. It will not be plotted on the polar scatter plot.\"\n                )\n            else:\n                theta = self.wind_direction\n                color_idx = i % len(sensor_object.color_map)\n\n                fig.add_trace(\n                    go.Scatterpolar(\n                        r=sensor.concentration,\n                        theta=theta,\n                        mode=\"markers\",\n                        name=sensor_key,\n                        marker={\"color\": sensor_object.color_map[color_idx]},\n                    )\n                )\n                if sensor.concentration.size &gt; 0:\n                    max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n        fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n        return fig\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":"<p>Number of observations.</p>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.calculate_wind_speed_from_uv","title":"<code>calculate_wind_speed_from_uv()</code>","text":"<p>Calculate wind speed.</p> <p>Calculate the wind speed from u and v components. Result gets stored in the wind_speed attribute</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_wind_speed_from_uv(self) -&gt; None:\n    \"\"\"Calculate wind speed.\n\n    Calculate the wind speed from u and v components. Result gets stored in the wind_speed attribute\n\n    \"\"\"\n    self.wind_speed = np.sqrt(self.u_component**2 + self.v_component**2)\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.calculate_wind_direction_from_uv","title":"<code>calculate_wind_direction_from_uv()</code>","text":"<p>Calculate wind direction: meteorological convention 0 is wind from the North.</p> <p>Calculate the wind direction from u and v components. Result gets stored in the wind_direction attribute See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_wind_direction_from_uv(self) -&gt; None:\n    \"\"\"Calculate wind direction: meteorological convention 0 is wind from the North.\n\n    Calculate the wind direction from u and v components. Result gets stored in the wind_direction attribute\n    See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n    \"\"\"\n    self.wind_direction = (270 - 180 / np.pi * np.arctan2(self.v_component, self.u_component)) % 360\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.calculate_uv_from_wind_speed_direction","title":"<code>calculate_uv_from_wind_speed_direction()</code>","text":"<p>Calculate u and v components from wind speed and direction.</p> <p>Results get stored in the u_component and v_component attributes. See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_uv_from_wind_speed_direction(self) -&gt; None:\n    \"\"\"Calculate u and v components from wind speed and direction.\n\n    Results get stored in the u_component and v_component attributes.\n    See: https://confluence.ecmwf.int/pages/viewpage.action?pageId=133262398\n\n    \"\"\"\n    self.u_component = -1 * self.wind_speed * np.sin(self.wind_direction * (np.pi / 180))\n    self.v_component = -1 * self.wind_speed * np.cos(self.wind_direction * (np.pi / 180))\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.calculate_wind_turbulence_horizontal","title":"<code>calculate_wind_turbulence_horizontal(window)</code>","text":"<p>Calculate the horizontal wind turbulence values from the wind direction attribute.</p> <p>Wind turbulence values are calculated as the circular standard deviation of wind direction (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circstd.html). The implementation here is equivalent to using the circstd function from scipy.stats as an apply function on a rolling window. However, using the rolling mean on sin and cos speeds up the calculation by a factor of 100.</p> <p>Outputted values are calculated at the center of the window and at least 3 observations are required in a window for the calculation. If the window contains less values the result will be np.nan. The result of the calculation will be stored as the wind_turbulence_horizontal attribute.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>str</code> <p>The size of the window in which values are aggregated specified as an offset alias: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases</p> required Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_wind_turbulence_horizontal(self, window: str) -&gt; None:\n    \"\"\"Calculate the horizontal wind turbulence values from the wind direction attribute.\n\n    Wind turbulence values are calculated as the circular standard deviation of wind direction\n    (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circstd.html).\n    The implementation here is equivalent to using the circstd function from scipy.stats as an apply\n    function on a rolling window. However, using the rolling mean on sin and cos speeds up\n    the calculation by a factor of 100.\n\n    Outputted values are calculated at the center of the window and at least 3 observations are required in a\n    window for the calculation. If the window contains less values the result will be np.nan.\n    The result of the calculation will be stored as the wind_turbulence_horizontal attribute.\n\n    Args:\n        window (str): The size of the window in which values are aggregated specified as an offset alias:\n            https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n\n    \"\"\"\n    data_series = pd.Series(data=self.wind_direction, index=self.time)\n    sin_rolling = (np.sin(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n    cos_rolling = (np.cos(data_series * np.pi / 180)).rolling(window=window, center=True, min_periods=3).mean()\n    aggregated_data = np.sqrt(-2 * np.log((sin_rolling**2 + cos_rolling**2) ** 0.5)) * 180 / np.pi\n    self.wind_turbulence_horizontal = aggregated_data.values\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.plot_polar_hist","title":"<code>plot_polar_hist(nof_sectors=16, nof_divisions=5, template=None)</code>","text":"<p>Plots a histogram of wind speed and wind direction in polar Coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>nof_sectors</code> <code>int</code> <p>The number of wind direction sectors into which the data is binned.</p> <code>16</code> <code>nof_divisions</code> <code>int</code> <p>The number of wind speed divisions into which the data is binned.</p> <code>5</code> <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def plot_polar_hist(self, nof_sectors: int = 16, nof_divisions: int = 5, template: object = None) -&gt; go.Figure():\n    \"\"\"Plots a histogram of wind speed and wind direction in polar Coordinates.\n\n    Args:\n        nof_sectors (int, optional): The number of wind direction sectors into which the data is binned.\n        nof_divisions (int, optional): The number of wind speed divisions into which the data is binned.\n        template (object): A layout template which can be applied to the plot. Defaults to None.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    \"\"\"\n    sector_half_width = 0.5 * (360 / nof_sectors)\n    wind_direction_bin_edges = np.linspace(-sector_half_width, 360 - sector_half_width, nof_sectors + 1)\n    wind_speed_bin_edges = np.linspace(np.min(self.wind_speed), np.max(self.wind_speed), nof_divisions)\n\n    dataframe = pd.DataFrame()\n    dataframe[\"wind_direction\"] = [x - 360 if x &gt; (360 - sector_half_width) else x for x in self.wind_direction]\n    dataframe[\"wind_speed\"] = self.wind_speed\n\n    dataframe[\"sector\"] = pd.cut(dataframe[\"wind_direction\"], wind_direction_bin_edges, include_lowest=True)\n    if np.allclose(wind_speed_bin_edges[0], wind_speed_bin_edges):\n        dataframe[\"speed\"] = wind_speed_bin_edges[0]\n    else:\n        dataframe[\"speed\"] = pd.cut(dataframe[\"wind_speed\"], wind_speed_bin_edges, include_lowest=True)\n\n    dataframe = dataframe.groupby([\"sector\", \"speed\"], observed=False).count()\n    dataframe = dataframe.rename(columns={\"wind_speed\": \"count\"}).drop(columns=[\"wind_direction\"])\n    dataframe[\"%\"] = dataframe[\"count\"] / dataframe[\"count\"].sum()\n\n    dataframe = dataframe.reset_index()\n    dataframe[\"theta\"] = dataframe.apply(lambda x: x[\"sector\"].mid, axis=1)\n\n    fig = px.bar_polar(\n        dataframe,\n        r=\"%\",\n        theta=\"theta\",\n        color=\"speed\",\n        direction=\"clockwise\",\n        start_angle=90,\n        color_discrete_sequence=px.colors.sequential.Sunset_r,\n    )\n\n    ticktext = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n    polar_dict = {\n        \"radialaxis\": {\"tickangle\": 90},\n        \"radialaxis_angle\": 90,\n        \"angularaxis\": {\n            \"tickmode\": \"array\",\n            \"ticktext\": ticktext,\n            \"tickvals\": list(np.linspace(0, 360 - (360 / 8), 8)),\n        },\n    }\n    fig.add_annotation(\n        x=1,\n        y=1,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        align=\"left\",\n        font={\"size\": 18, \"color\": \"#000000\"},\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=\"&lt;b&gt;Radial Axis:&lt;/b&gt; Proportion&lt;br&gt;of wind measurements&lt;br&gt;in a given direction.\",\n    )\n\n    fig.update_layout(polar=polar_dict)\n    fig.update_layout(template=template)\n    fig.update_layout(title=\"Distribution of Wind Speeds and Directions\")\n\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.Meteorology.plot_polar_scatter","title":"<code>plot_polar_scatter(fig, sensor_object, template=None)</code>","text":"<p>Plots a scatter plot of concentration with respect to wind direction in polar Coordinates.</p> <p>This function implements the polar scatter functionality for a (single) Meteorology object. Assuming the all Sensors in the SensorGroup are consistent with the Meteorology object.</p> <p>Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency in plot colors.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>A plotly figure onto which traces can be drawn.</p> required <code>sensor_object</code> <code>SensorGroup</code> <p>SensorGroup object which contains the concentration information</p> required <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n    \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar Coordinates.\n\n    This function implements the polar scatter functionality for a (single) Meteorology object. Assuming the all\n    Sensors in the SensorGroup are consistent with the Meteorology object.\n\n    Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n    in plot colors.\n\n    Args:\n        fig (go.Figure): A plotly figure onto which traces can be drawn.\n        sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n        template (object): A layout template which can be applied to the plot. Defaults to None.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    \"\"\"\n    max_concentration = 0\n\n    for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n        if sensor.concentration.shape != self.wind_direction.shape:\n            warnings.warn(\n                f\"Concentration values for sensor {sensor_key} are of shape \"\n                + f\"{sensor.concentration.shape}, but self.wind_direction has shape \"\n                + f\"{self.wind_direction.shape}. It will not be plotted on the polar scatter plot.\"\n            )\n        else:\n            theta = self.wind_direction\n            color_idx = i % len(sensor_object.color_map)\n\n            fig.add_trace(\n                go.Scatterpolar(\n                    r=sensor.concentration,\n                    theta=theta,\n                    mode=\"markers\",\n                    name=sensor_key,\n                    marker={\"color\": sensor_object.color_map[color_idx]},\n                )\n            )\n            if sensor.concentration.size &gt; 0:\n                max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n    fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup","title":"<code>MeteorologyGroup</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict</code></p> <p>A dictionary containing multiple Meteorology objects.</p> <p>This class is used when we want to define/store a collection of meteorology objects consistent with an associated SensorGroup which can then be used in further processing, e.g. Gaussian plume coupling computation.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>@dataclass\nclass MeteorologyGroup(dict):\n    \"\"\"A dictionary containing multiple Meteorology objects.\n\n    This class is used when we want to define/store a collection of meteorology objects consistent with an associated\n    SensorGroup which can then be used in further processing, e.g. Gaussian plume coupling computation.\n\n    \"\"\"\n\n    @property\n    def nof_objects(self) -&gt; int:\n        \"\"\"Int: Number of meteorology objects contained in the MeteorologyGroup.\"\"\"\n        return len(self)\n\n    def add_object(self, met_object: Meteorology):\n        \"\"\"Add an object to the MeteorologyGroup.\"\"\"\n        self[met_object.label] = met_object\n\n    def calculate_uv_from_wind_speed_direction(self):\n        \"\"\"Calculate the u and v components for each member of the group.\"\"\"\n        for met in self.values():\n            met.calculate_uv_from_wind_speed_direction()\n\n    def calculate_wind_direction_from_uv(self):\n        \"\"\"Calculate wind direction from the u and v components for each member of the group.\"\"\"\n        for met in self.values():\n            met.calculate_wind_direction_from_uv()\n\n    def calculate_wind_speed_from_uv(self):\n        \"\"\"Calculate wind speed from the u and v components for each member of the group.\"\"\"\n        for met in self.values():\n            met.calculate_wind_speed_from_uv()\n\n    def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n        \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar coordinates.\n\n        This function implements the polar scatter functionality for a MeteorologyGroup object. It assumes each object\n        in the SensorGroup has an associated Meteorology object in the MeteorologyGroup.\n\n        Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n        in plot colors.\n\n        Args:\n            fig (go.Figure): A plotly figure onto which traces can be drawn.\n            sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n            template (object): A layout template which can be applied to the plot. Defaults to None.\n\n        Returns:\n            fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n        Raises\n            ValueError: When there is a sensor key which is not present in the MeteorologyGroup.\n\n        \"\"\"\n        max_concentration = 0\n\n        for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n            if sensor_key not in self.keys():\n                raise ValueError(f\"Key {sensor_key} not found in MeteorologyGroup.\")\n            temp_met_object = self[sensor_key]\n            if sensor.concentration.shape != temp_met_object.wind_direction.shape:\n                warnings.warn(\n                    f\"Concentration values for sensor {sensor_key} are of shape \"\n                    + f\"{sensor.concentration.shape}, but wind_direction values for meteorology object {sensor_key} \"\n                    f\"has shape {temp_met_object.wind_direction.shape}. It will not be plotted on the polar scatter \"\n                    f\"plot.\"\n                )\n            else:\n                theta = temp_met_object.wind_direction\n                color_idx = i % len(sensor_object.color_map)\n\n                fig.add_trace(\n                    go.Scatterpolar(\n                        r=sensor.concentration,\n                        theta=theta,\n                        mode=\"markers\",\n                        name=sensor_key,\n                        marker={\"color\": sensor_object.color_map[color_idx]},\n                    )\n                )\n\n                if sensor.concentration.size &gt; 0:\n                    max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n        fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n        return fig\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup.nof_objects","title":"<code>nof_objects</code>  <code>property</code>","text":""},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup.add_object","title":"<code>add_object(met_object)</code>","text":"<p>Add an object to the MeteorologyGroup.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def add_object(self, met_object: Meteorology):\n    \"\"\"Add an object to the MeteorologyGroup.\"\"\"\n    self[met_object.label] = met_object\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup.calculate_uv_from_wind_speed_direction","title":"<code>calculate_uv_from_wind_speed_direction()</code>","text":"<p>Calculate the u and v components for each member of the group.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_uv_from_wind_speed_direction(self):\n    \"\"\"Calculate the u and v components for each member of the group.\"\"\"\n    for met in self.values():\n        met.calculate_uv_from_wind_speed_direction()\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup.calculate_wind_direction_from_uv","title":"<code>calculate_wind_direction_from_uv()</code>","text":"<p>Calculate wind direction from the u and v components for each member of the group.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_wind_direction_from_uv(self):\n    \"\"\"Calculate wind direction from the u and v components for each member of the group.\"\"\"\n    for met in self.values():\n        met.calculate_wind_direction_from_uv()\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup.calculate_wind_speed_from_uv","title":"<code>calculate_wind_speed_from_uv()</code>","text":"<p>Calculate wind speed from the u and v components for each member of the group.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def calculate_wind_speed_from_uv(self):\n    \"\"\"Calculate wind speed from the u and v components for each member of the group.\"\"\"\n    for met in self.values():\n        met.calculate_wind_speed_from_uv()\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.MeteorologyGroup.plot_polar_scatter","title":"<code>plot_polar_scatter(fig, sensor_object, template=None)</code>","text":"<p>Plots a scatter plot of concentration with respect to wind direction in polar coordinates.</p> <p>This function implements the polar scatter functionality for a MeteorologyGroup object. It assumes each object in the SensorGroup has an associated Meteorology object in the MeteorologyGroup.</p> <p>Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency in plot colors.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>A plotly figure onto which traces can be drawn.</p> required <code>sensor_object</code> <code>SensorGroup</code> <p>SensorGroup object which contains the concentration information</p> required <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> <p>Raises     ValueError: When there is a sensor key which is not present in the MeteorologyGroup.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def plot_polar_scatter(self, fig: go.Figure, sensor_object: SensorGroup, template: object = None) -&gt; go.Figure():\n    \"\"\"Plots a scatter plot of concentration with respect to wind direction in polar coordinates.\n\n    This function implements the polar scatter functionality for a MeteorologyGroup object. It assumes each object\n    in the SensorGroup has an associated Meteorology object in the MeteorologyGroup.\n\n    Note we do plot the sensors which do not contain any values when present in the SensorGroup to keep consistency\n    in plot colors.\n\n    Args:\n        fig (go.Figure): A plotly figure onto which traces can be drawn.\n        sensor_object (SensorGroup): SensorGroup object which contains the concentration information\n        template (object): A layout template which can be applied to the plot. Defaults to None.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    Raises\n        ValueError: When there is a sensor key which is not present in the MeteorologyGroup.\n\n    \"\"\"\n    max_concentration = 0\n\n    for i, (sensor_key, sensor) in enumerate(sensor_object.items()):\n        if sensor_key not in self.keys():\n            raise ValueError(f\"Key {sensor_key} not found in MeteorologyGroup.\")\n        temp_met_object = self[sensor_key]\n        if sensor.concentration.shape != temp_met_object.wind_direction.shape:\n            warnings.warn(\n                f\"Concentration values for sensor {sensor_key} are of shape \"\n                + f\"{sensor.concentration.shape}, but wind_direction values for meteorology object {sensor_key} \"\n                f\"has shape {temp_met_object.wind_direction.shape}. It will not be plotted on the polar scatter \"\n                f\"plot.\"\n            )\n        else:\n            theta = temp_met_object.wind_direction\n            color_idx = i % len(sensor_object.color_map)\n\n            fig.add_trace(\n                go.Scatterpolar(\n                    r=sensor.concentration,\n                    theta=theta,\n                    mode=\"markers\",\n                    name=sensor_key,\n                    marker={\"color\": sensor_object.color_map[color_idx]},\n                )\n            )\n\n            if sensor.concentration.size &gt; 0:\n                max_concentration = np.maximum(np.nanmax(sensor.concentration), max_concentration)\n\n    fig = set_plot_polar_scatter_layout(max_concentration=max_concentration, fig=fig, template=template)\n\n    return fig\n</code></pre>"},{"location":"pyelq/meteorology/#pyelq.meteorology.set_plot_polar_scatter_layout","title":"<code>set_plot_polar_scatter_layout(max_concentration, fig, template)</code>","text":"<p>Helper function to set the layout of the polar scatter plot.</p> <p>Helps avoid code duplication.</p> <p>Parameters:</p> Name Type Description Default <code>max_concentration</code> <code>float</code> <p>The maximum concentration value used to update radial axis range.</p> required <code>fig</code> <code>Figure</code> <p>A plotly figure onto which traces can be drawn.</p> required <code>template</code> <code>object</code> <p>A layout template which can be applied to the plot.</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>A plotly go figure containing the trace of the rose plot.</p> Source code in <code>src/pyelq/meteorology.py</code> <pre><code>def set_plot_polar_scatter_layout(max_concentration: float, fig: go.Figure(), template: object) -&gt; go.Figure:\n    \"\"\"Helper function to set the layout of the polar scatter plot.\n\n    Helps avoid code duplication.\n\n    Args:\n        max_concentration (float): The maximum concentration value used to update radial axis range.\n        fig (go.Figure): A plotly figure onto which traces can be drawn.\n        template (object): A layout template which can be applied to the plot.\n\n    Returns:\n        fig (go.Figure): A plotly go figure containing the trace of the rose plot.\n\n    \"\"\"\n    ticktext = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n    polar_dict = {\n        \"radialaxis\": {\"tickangle\": 0, \"range\": [0.0, 1.01 * max_concentration]},\n        \"radialaxis_angle\": 0,\n        \"angularaxis\": {\n            \"tickmode\": \"array\",\n            \"ticktext\": ticktext,\n            \"direction\": \"clockwise\",\n            \"rotation\": 90,\n            \"tickvals\": list(np.linspace(0, 360 - (360 / 8), 8)),\n        },\n    }\n\n    fig.add_annotation(\n        x=1,\n        y=1,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        align=\"left\",\n        font={\"size\": 18, \"color\": \"#000000\"},\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=\"&lt;b&gt;Radial Axis:&lt;/b&gt; Wind&lt;br&gt;speed in m/s.\",\n    )\n\n    fig.update_layout(polar=polar_dict)\n    fig.update_layout(template=template)\n    fig.update_layout(title=\"Measured Concentration against Wind Direction.\")\n    return fig\n</code></pre>"},{"location":"pyelq/model/","title":"Model","text":""},{"location":"pyelq/model/#model","title":"Model","text":"<p>ELQModel module.</p> <p>This module provides a class definition for the main functionalities of the codebase, providing the interface with the openMCMC repo and defining some plotting wrappers.</p>"},{"location":"pyelq/model/#pyelq.model.ELQModel","title":"<code>ELQModel</code>  <code>dataclass</code>","text":"<p>Class for setting up, running, and post-processing the full ELQModel analysis.</p> <p>Attributes:</p> Name Type Description <code>form</code> <code>dict</code> <p>dictionary detailing the form of the predictor for the concentration data. For details of the required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.</p> <code>transform</code> <code>dict</code> <p>dictionary detailing transformations applied to the model components. For details of the required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.</p> <code>model</code> <code>Model</code> <p>full model specification for the analysis, constructed in self.to_mcmc().</p> <code>mcmc</code> <code>MCMC</code> <p>MCMC object containing model and sampler specification for the problem. Constructed from the other components in self.to_mcmc().</p> <code>n_iter</code> <code>int</code> <p>number of MCMC iterations to be run.</p> <code>n_thin</code> <code>int</code> <p>number of iterations to thin by.</p> <code>fitted_values</code> <code>ndarray</code> <p>samples of fitted values (i.e. model predictions for the data) generated during the MCMC sampler. Attached in self.from_mcmc().</p> Source code in <code>src/pyelq/model.py</code> <pre><code>@dataclass\nclass ELQModel:\n    \"\"\"Class for setting up, running, and post-processing the full ELQModel analysis.\n\n    Attributes:\n        form (dict): dictionary detailing the form of the predictor for the concentration data. For details of the\n            required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.\n        transform (dict): dictionary detailing transformations applied to the model components. For details of the\n            required specification, see parameter.LinearCombinationWithTransform() in the openMCMC repo.\n        model (Model): full model specification for the analysis, constructed in self.to_mcmc().\n        mcmc (MCMC): MCMC object containing model and sampler specification for the problem. Constructed from the\n            other components in self.to_mcmc().\n        n_iter (int): number of MCMC iterations to be run.\n        n_thin (int): number of iterations to thin by.\n        fitted_values (np.ndarray): samples of fitted values (i.e. model predictions for the data) generated during the\n            MCMC sampler. Attached in self.from_mcmc().\n\n    \"\"\"\n\n    form: dict = field(init=False)\n    transform: dict = field(init=False)\n    model: Model = field(init=False)\n    mcmc: MCMC = field(init=False)\n    n_iter: int = 1000\n    n_thin: int = 1\n    fitted_values: np.ndarray = field(init=False)\n\n    def __init__(\n        self,\n        sensor_object: SensorGroup,\n        meteorology: Union[Meteorology, MeteorologyGroup],\n        gas_species: GasSpecies,\n        background: Background = SpatioTemporalBackground(),\n        source_model: Union[list, SourceModel] = Normal(),\n        error_model: ErrorModel = BySensor(),\n        offset_model: PerSensor = None,\n    ):\n        \"\"\"Initialise the ELQModel model.\n\n        Model form is as follows:\n        y = A*s + b + d + e\n        where:\n        - y is the vector of observed concentration data (extracted from the sensor object).\n        - A*s is the source contribution (from the source model and dispersion model).\n        - b is from the background model.\n        - d is from the offset model.\n        - e is residual error term and var(e) comes from the error precision model.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (Union[Meteorology, MeteorologyGroup]): meteorology data.\n            gas_species (GasSpecies): gas species object.\n            background (Background): background model specification. Defaults to SpatioTemporalBackground().\n            source_model (Union[list, SourceModel]): source model specification. This can be a list of multiple\n            SourceModels or a single SourceModel. Defaults to Normal(). If a single SourceModel is used, it will\n            be converted to a list.\n            error_model (Precision): measurement precision model specification. Defaults to BySensor().\n            offset_model (PerSensor): offset model specification. Defaults to None.\n\n        \"\"\"\n        self.sensor_object = sensor_object\n        self.meteorology = meteorology\n        self.gas_species = gas_species\n        self.components = {\n            \"background\": background,\n            \"error_model\": error_model,\n            \"offset\": offset_model,\n        }\n\n        if source_model is not None:\n            if not isinstance(source_model, list):\n                source_model = [source_model]\n            for source in source_model:\n                if source.label_string is None:\n                    self.components[\"source\"] = source\n                else:\n                    self.components[\"source_\" + source.label_string] = source\n\n        if error_model is None:\n            self.components[\"error_model\"] = BySensor()\n            warnings.warn(\"None is not an allowed type for error_model: resetting to default BySensor model.\")\n        for key in list(self.components.keys()):\n            if self.components[key] is None:\n                self.components.pop(key)\n\n    def initialise(self):\n        \"\"\"Take data inputs and extract relevant properties.\"\"\"\n        self.form = {}\n        self.transform = {}\n        for key, component in self.components.items():\n\n            if \"background\" in key:\n                self.form[\"bg\"] = \"B_bg\"\n                self.transform[\"bg\"] = False\n            if re.match(\"source\", key):\n                source_component_map = component.map\n                self.transform[source_component_map[\"source\"]] = False\n                self.form[source_component_map[\"source\"]] = source_component_map[\"coupling_matrix\"]\n            if \"offset\" in key:\n                self.form[\"d\"] = \"B_d\"\n                self.transform[\"d\"] = False\n\n            self.components[key].initialise(self.sensor_object, self.meteorology, self.gas_species)\n\n    def to_mcmc(self):\n        \"\"\"Convert the ELQModel specification into an MCMC solver object that can be run.\n\n        Executing the following steps:\n            - Initialise the model object with the data likelihood (response distribution for y), and add all the\n                associated prior distributions, as specified by the model components.\n            - Initialise the state dictionary with the observed sensor data, and add parameters associated with all\n                the associated prior distributions, as specified by the model components.\n            - Initialise the MCMC sampler objects associated with each of the model components.\n            - Create the MCMC solver object, using all of the above information.\n\n        \"\"\"\n        response_precision = self.components[\"error_model\"].precision_parameter\n        model = [\n            location_scale.Normal(\n                \"y\",\n                mean=parameter.LinearCombinationWithTransform(self.form, self.transform),\n                precision=response_precision,\n            )\n        ]\n\n        initial_state = {\"y\": self.sensor_object.concentration}\n\n        for component in self.components.values():\n            model = component.make_model(model)\n            initial_state = component.make_state(initial_state)\n\n        self.model = Model(model, response={\"y\": \"mean\"})\n\n        sampler_list = []\n        for component in self.components.values():\n            sampler_list = component.make_sampler(self.model, sampler_list)\n\n        self.mcmc = MCMC(initial_state, sampler_list, self.model, n_burn=0, n_iter=self.n_iter, n_thin=self.n_thin)\n\n    def run_mcmc(self):\n        \"\"\"Run the mcmc function.\"\"\"\n        self.mcmc.run_mcmc()\n\n    def from_mcmc(self):\n        \"\"\"Extract information from MCMC solver class once its has run.\n\n        Performs two operations:\n            - For each of the components of the model: extracts the related sampled parameter values and attaches these\n                to the component class.\n            - For all keys in the mcmc.store dictionary: extracts the sampled parameter values from self.mcmc.store and\n                puts them into the equivalent fields in the state\n\n        \"\"\"\n        state = self.mcmc.state\n        for component in self.components.values():\n            component.from_mcmc(self.mcmc.store)\n        for key in self.mcmc.store:\n            state[key] = self.mcmc.store[key]\n\n        self.make_combined_source_model()\n\n    def make_combined_source_model(self):\n        \"\"\"Aggregate multiple individual source models into a single combined source model.\n\n        This function iterates through the existing source models stored in `self.components` and consolidates them\n        into a unified source model named `\"sources_combined\"`. This is particularly useful when multiple source\n        models are involved in an analysis, and a merged representation is required for visualization.\n\n        The combined source model is created as an instance of the `Normal` model, with the label string\n        \"sources_combined\" with the following attributes:\n        - emission_rate: concatenated across all source models.\n        - all_source_locations: concatenated across all source models.\n        - number_on_sources: derived by summing the individual source counts across all source models\n        - label_string: concatenated across all source models.\n        - individual_source_labels: concatenated across all source models.\n\n        Once combined, the `\"sources_combined\"` model is stored in the `self.components` dictionary for later use.\n\n        Raises:\n            ValueError: If the reference locations of the individual source models are inconsistent.\n            This is checked by comparing the reference latitude, longitude, and altitude of each source model.\n\n        \"\"\"\n        combined_model = Normal(label_string=\"sources_combined\")\n        emission_rate = np.empty((0, self.mcmc.n_iter))\n        all_source_locations_east = np.empty((0, self.mcmc.n_iter))\n        all_source_locations_north = np.empty((0, self.mcmc.n_iter))\n        all_source_locations_up = np.empty((0, self.mcmc.n_iter))\n        number_on_sources = np.empty((0, self.mcmc.n_iter))\n        label_string = []\n        individual_source_labels = []\n\n        ref_latitude = None\n        ref_longitude = None\n        ref_altitude = None\n        for key, component in self.components.items():\n            if re.match(\"source\", key):\n                comp_ref_latitude = component.all_source_locations.ref_latitude\n                comp_ref_longitude = component.all_source_locations.ref_longitude\n                comp_ref_altitude = component.all_source_locations.ref_altitude\n                if ref_latitude is None and ref_longitude is None and ref_altitude is None:\n                    ref_latitude = comp_ref_latitude\n                    ref_longitude = comp_ref_longitude\n                    ref_altitude = comp_ref_altitude\n                else:\n                    if (\n                        not np.isclose(ref_latitude, comp_ref_latitude)\n                        or not np.isclose(ref_longitude, comp_ref_longitude)\n                        or not np.isclose(ref_altitude, comp_ref_altitude)\n                    ):\n                        raise ValueError(\n                            f\"Inconsistent reference locations in component '{key}'. \"\n                            \"All source models must share the same reference location.\"\n                        )\n                emission_rate = np.concatenate((emission_rate, component.emission_rate))\n                number_on_sources = np.concatenate(\n                    (\n                        number_on_sources.reshape((-1, self.mcmc.n_iter)),\n                        component.number_on_sources.reshape(-1, self.mcmc.n_iter),\n                    ),\n                    axis=0,\n                )\n                label_string.append(component.label_string)\n                individual_source_labels.append(component.individual_source_labels)\n\n                all_source_locations_east = np.concatenate(\n                    (\n                        all_source_locations_east,\n                        component.all_source_locations.east.reshape((-1, self.mcmc.n_iter)),\n                    ),\n                    axis=0,\n                )\n                all_source_locations_north = np.concatenate(\n                    (\n                        all_source_locations_north,\n                        component.all_source_locations.north.reshape((-1, self.mcmc.n_iter)),\n                    ),\n                    axis=0,\n                )\n                all_source_locations_up = np.concatenate(\n                    (\n                        all_source_locations_up,\n                        component.all_source_locations.up.reshape((-1, self.mcmc.n_iter)),\n                    ),\n                    axis=0,\n                )\n\n        combined_model.all_source_locations = ENU(\n            ref_altitude=ref_altitude,\n            ref_latitude=ref_latitude,\n            ref_longitude=ref_longitude,\n            east=all_source_locations_east,\n            north=all_source_locations_north,\n            up=all_source_locations_up,\n        )\n\n        combined_model.emission_rate = emission_rate\n        combined_model.label_string = label_string\n        combined_model.number_on_sources = np.sum(number_on_sources, axis=0)\n        combined_model.individual_source_labels = [item for sublist in individual_source_labels for item in sublist]\n        self.components[\"sources_combined\"] = combined_model\n\n    def plot_log_posterior(self, burn_in_value: int, plot: Plot = Plot()) -&gt; Plot():\n        \"\"\"Plots the trace of the log posterior over the iterations of the MCMC.\n\n        Args:\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'log_posterior_plot'\n\n        \"\"\"\n        plot.plot_single_trace(object_to_plot=self.mcmc, burn_in=burn_in_value)\n        return plot\n\n    def plot_fitted_values(self, plot: Plot = Plot()) -&gt; Plot:\n        \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when possible.\n\n        Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n        measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n        Args:\n            plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with key 'fitted_values'\n\n        \"\"\"\n        plot.plot_fitted_values_per_sensor(\n            mcmc_object=self.mcmc, sensor_object=self.sensor_object, background_model=self.components[\"background\"]\n        )\n        return plot\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.__init__","title":"<code>__init__(sensor_object, meteorology, gas_species, background=SpatioTemporalBackground(), source_model=Normal(), error_model=BySensor(), offset_model=None)</code>","text":"<p>Initialise the ELQModel model.</p> <p>Model form is as follows: y = As + b + d + e where: - y is the vector of observed concentration data (extracted from the sensor object). - As is the source contribution (from the source model and dispersion model). - b is from the background model. - d is from the offset model. - e is residual error term and var(e) comes from the error precision model.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>Union[Meteorology, MeteorologyGroup]</code> <p>meteorology data.</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species object.</p> required <code>background</code> <code>Background</code> <p>background model specification. Defaults to SpatioTemporalBackground().</p> <code>SpatioTemporalBackground()</code> <code>source_model</code> <code>Union[list, SourceModel]</code> <p>source model specification. This can be a list of multiple</p> <code>Normal()</code> <code>error_model</code> <code>Precision</code> <p>measurement precision model specification. Defaults to BySensor().</p> <code>BySensor()</code> <code>offset_model</code> <code>PerSensor</code> <p>offset model specification. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/model.py</code> <pre><code>def __init__(\n    self,\n    sensor_object: SensorGroup,\n    meteorology: Union[Meteorology, MeteorologyGroup],\n    gas_species: GasSpecies,\n    background: Background = SpatioTemporalBackground(),\n    source_model: Union[list, SourceModel] = Normal(),\n    error_model: ErrorModel = BySensor(),\n    offset_model: PerSensor = None,\n):\n    \"\"\"Initialise the ELQModel model.\n\n    Model form is as follows:\n    y = A*s + b + d + e\n    where:\n    - y is the vector of observed concentration data (extracted from the sensor object).\n    - A*s is the source contribution (from the source model and dispersion model).\n    - b is from the background model.\n    - d is from the offset model.\n    - e is residual error term and var(e) comes from the error precision model.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (Union[Meteorology, MeteorologyGroup]): meteorology data.\n        gas_species (GasSpecies): gas species object.\n        background (Background): background model specification. Defaults to SpatioTemporalBackground().\n        source_model (Union[list, SourceModel]): source model specification. This can be a list of multiple\n        SourceModels or a single SourceModel. Defaults to Normal(). If a single SourceModel is used, it will\n        be converted to a list.\n        error_model (Precision): measurement precision model specification. Defaults to BySensor().\n        offset_model (PerSensor): offset model specification. Defaults to None.\n\n    \"\"\"\n    self.sensor_object = sensor_object\n    self.meteorology = meteorology\n    self.gas_species = gas_species\n    self.components = {\n        \"background\": background,\n        \"error_model\": error_model,\n        \"offset\": offset_model,\n    }\n\n    if source_model is not None:\n        if not isinstance(source_model, list):\n            source_model = [source_model]\n        for source in source_model:\n            if source.label_string is None:\n                self.components[\"source\"] = source\n            else:\n                self.components[\"source_\" + source.label_string] = source\n\n    if error_model is None:\n        self.components[\"error_model\"] = BySensor()\n        warnings.warn(\"None is not an allowed type for error_model: resetting to default BySensor model.\")\n    for key in list(self.components.keys()):\n        if self.components[key] is None:\n            self.components.pop(key)\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.initialise","title":"<code>initialise()</code>","text":"<p>Take data inputs and extract relevant properties.</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def initialise(self):\n    \"\"\"Take data inputs and extract relevant properties.\"\"\"\n    self.form = {}\n    self.transform = {}\n    for key, component in self.components.items():\n\n        if \"background\" in key:\n            self.form[\"bg\"] = \"B_bg\"\n            self.transform[\"bg\"] = False\n        if re.match(\"source\", key):\n            source_component_map = component.map\n            self.transform[source_component_map[\"source\"]] = False\n            self.form[source_component_map[\"source\"]] = source_component_map[\"coupling_matrix\"]\n        if \"offset\" in key:\n            self.form[\"d\"] = \"B_d\"\n            self.transform[\"d\"] = False\n\n        self.components[key].initialise(self.sensor_object, self.meteorology, self.gas_species)\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.to_mcmc","title":"<code>to_mcmc()</code>","text":"<p>Convert the ELQModel specification into an MCMC solver object that can be run.</p> Executing the following steps <ul> <li>Initialise the model object with the data likelihood (response distribution for y), and add all the     associated prior distributions, as specified by the model components.</li> <li>Initialise the state dictionary with the observed sensor data, and add parameters associated with all     the associated prior distributions, as specified by the model components.</li> <li>Initialise the MCMC sampler objects associated with each of the model components.</li> <li>Create the MCMC solver object, using all of the above information.</li> </ul> Source code in <code>src/pyelq/model.py</code> <pre><code>def to_mcmc(self):\n    \"\"\"Convert the ELQModel specification into an MCMC solver object that can be run.\n\n    Executing the following steps:\n        - Initialise the model object with the data likelihood (response distribution for y), and add all the\n            associated prior distributions, as specified by the model components.\n        - Initialise the state dictionary with the observed sensor data, and add parameters associated with all\n            the associated prior distributions, as specified by the model components.\n        - Initialise the MCMC sampler objects associated with each of the model components.\n        - Create the MCMC solver object, using all of the above information.\n\n    \"\"\"\n    response_precision = self.components[\"error_model\"].precision_parameter\n    model = [\n        location_scale.Normal(\n            \"y\",\n            mean=parameter.LinearCombinationWithTransform(self.form, self.transform),\n            precision=response_precision,\n        )\n    ]\n\n    initial_state = {\"y\": self.sensor_object.concentration}\n\n    for component in self.components.values():\n        model = component.make_model(model)\n        initial_state = component.make_state(initial_state)\n\n    self.model = Model(model, response={\"y\": \"mean\"})\n\n    sampler_list = []\n    for component in self.components.values():\n        sampler_list = component.make_sampler(self.model, sampler_list)\n\n    self.mcmc = MCMC(initial_state, sampler_list, self.model, n_burn=0, n_iter=self.n_iter, n_thin=self.n_thin)\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.run_mcmc","title":"<code>run_mcmc()</code>","text":"<p>Run the mcmc function.</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def run_mcmc(self):\n    \"\"\"Run the mcmc function.\"\"\"\n    self.mcmc.run_mcmc()\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.from_mcmc","title":"<code>from_mcmc()</code>","text":"<p>Extract information from MCMC solver class once its has run.</p> Performs two operations <ul> <li>For each of the components of the model: extracts the related sampled parameter values and attaches these     to the component class.</li> <li>For all keys in the mcmc.store dictionary: extracts the sampled parameter values from self.mcmc.store and     puts them into the equivalent fields in the state</li> </ul> Source code in <code>src/pyelq/model.py</code> <pre><code>def from_mcmc(self):\n    \"\"\"Extract information from MCMC solver class once its has run.\n\n    Performs two operations:\n        - For each of the components of the model: extracts the related sampled parameter values and attaches these\n            to the component class.\n        - For all keys in the mcmc.store dictionary: extracts the sampled parameter values from self.mcmc.store and\n            puts them into the equivalent fields in the state\n\n    \"\"\"\n    state = self.mcmc.state\n    for component in self.components.values():\n        component.from_mcmc(self.mcmc.store)\n    for key in self.mcmc.store:\n        state[key] = self.mcmc.store[key]\n\n    self.make_combined_source_model()\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.make_combined_source_model","title":"<code>make_combined_source_model()</code>","text":"<p>Aggregate multiple individual source models into a single combined source model.</p> <p>This function iterates through the existing source models stored in <code>self.components</code> and consolidates them into a unified source model named <code>\"sources_combined\"</code>. This is particularly useful when multiple source models are involved in an analysis, and a merged representation is required for visualization.</p> <p>The combined source model is created as an instance of the <code>Normal</code> model, with the label string \"sources_combined\" with the following attributes: - emission_rate: concatenated across all source models. - all_source_locations: concatenated across all source models. - number_on_sources: derived by summing the individual source counts across all source models - label_string: concatenated across all source models. - individual_source_labels: concatenated across all source models.</p> <p>Once combined, the <code>\"sources_combined\"</code> model is stored in the <code>self.components</code> dictionary for later use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reference locations of the individual source models are inconsistent.</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def make_combined_source_model(self):\n    \"\"\"Aggregate multiple individual source models into a single combined source model.\n\n    This function iterates through the existing source models stored in `self.components` and consolidates them\n    into a unified source model named `\"sources_combined\"`. This is particularly useful when multiple source\n    models are involved in an analysis, and a merged representation is required for visualization.\n\n    The combined source model is created as an instance of the `Normal` model, with the label string\n    \"sources_combined\" with the following attributes:\n    - emission_rate: concatenated across all source models.\n    - all_source_locations: concatenated across all source models.\n    - number_on_sources: derived by summing the individual source counts across all source models\n    - label_string: concatenated across all source models.\n    - individual_source_labels: concatenated across all source models.\n\n    Once combined, the `\"sources_combined\"` model is stored in the `self.components` dictionary for later use.\n\n    Raises:\n        ValueError: If the reference locations of the individual source models are inconsistent.\n        This is checked by comparing the reference latitude, longitude, and altitude of each source model.\n\n    \"\"\"\n    combined_model = Normal(label_string=\"sources_combined\")\n    emission_rate = np.empty((0, self.mcmc.n_iter))\n    all_source_locations_east = np.empty((0, self.mcmc.n_iter))\n    all_source_locations_north = np.empty((0, self.mcmc.n_iter))\n    all_source_locations_up = np.empty((0, self.mcmc.n_iter))\n    number_on_sources = np.empty((0, self.mcmc.n_iter))\n    label_string = []\n    individual_source_labels = []\n\n    ref_latitude = None\n    ref_longitude = None\n    ref_altitude = None\n    for key, component in self.components.items():\n        if re.match(\"source\", key):\n            comp_ref_latitude = component.all_source_locations.ref_latitude\n            comp_ref_longitude = component.all_source_locations.ref_longitude\n            comp_ref_altitude = component.all_source_locations.ref_altitude\n            if ref_latitude is None and ref_longitude is None and ref_altitude is None:\n                ref_latitude = comp_ref_latitude\n                ref_longitude = comp_ref_longitude\n                ref_altitude = comp_ref_altitude\n            else:\n                if (\n                    not np.isclose(ref_latitude, comp_ref_latitude)\n                    or not np.isclose(ref_longitude, comp_ref_longitude)\n                    or not np.isclose(ref_altitude, comp_ref_altitude)\n                ):\n                    raise ValueError(\n                        f\"Inconsistent reference locations in component '{key}'. \"\n                        \"All source models must share the same reference location.\"\n                    )\n            emission_rate = np.concatenate((emission_rate, component.emission_rate))\n            number_on_sources = np.concatenate(\n                (\n                    number_on_sources.reshape((-1, self.mcmc.n_iter)),\n                    component.number_on_sources.reshape(-1, self.mcmc.n_iter),\n                ),\n                axis=0,\n            )\n            label_string.append(component.label_string)\n            individual_source_labels.append(component.individual_source_labels)\n\n            all_source_locations_east = np.concatenate(\n                (\n                    all_source_locations_east,\n                    component.all_source_locations.east.reshape((-1, self.mcmc.n_iter)),\n                ),\n                axis=0,\n            )\n            all_source_locations_north = np.concatenate(\n                (\n                    all_source_locations_north,\n                    component.all_source_locations.north.reshape((-1, self.mcmc.n_iter)),\n                ),\n                axis=0,\n            )\n            all_source_locations_up = np.concatenate(\n                (\n                    all_source_locations_up,\n                    component.all_source_locations.up.reshape((-1, self.mcmc.n_iter)),\n                ),\n                axis=0,\n            )\n\n    combined_model.all_source_locations = ENU(\n        ref_altitude=ref_altitude,\n        ref_latitude=ref_latitude,\n        ref_longitude=ref_longitude,\n        east=all_source_locations_east,\n        north=all_source_locations_north,\n        up=all_source_locations_up,\n    )\n\n    combined_model.emission_rate = emission_rate\n    combined_model.label_string = label_string\n    combined_model.number_on_sources = np.sum(number_on_sources, axis=0)\n    combined_model.individual_source_labels = [item for sublist in individual_source_labels for item in sublist]\n    self.components[\"sources_combined\"] = combined_model\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.plot_log_posterior","title":"<code>plot_log_posterior(burn_in_value, plot=Plot())</code>","text":"<p>Plots the trace of the log posterior over the iterations of the MCMC.</p> <p>Parameters:</p> Name Type Description Default <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> <code>Plot()</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'log_posterior_plot'</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def plot_log_posterior(self, burn_in_value: int, plot: Plot = Plot()) -&gt; Plot():\n    \"\"\"Plots the trace of the log posterior over the iterations of the MCMC.\n\n    Args:\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'log_posterior_plot'\n\n    \"\"\"\n    plot.plot_single_trace(object_to_plot=self.mcmc, burn_in=burn_in_value)\n    return plot\n</code></pre>"},{"location":"pyelq/model/#pyelq.model.ELQModel.plot_fitted_values","title":"<code>plot_fitted_values(plot=Plot())</code>","text":"<p>Plot the fitted values from the mcmc object against time, also shows the estimated background when possible.</p> <p>Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> <code>Plot()</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'fitted_values'</p> Source code in <code>src/pyelq/model.py</code> <pre><code>def plot_fitted_values(self, plot: Plot = Plot()) -&gt; Plot:\n    \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when possible.\n\n    Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n    measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n    Args:\n        plot (Plot, optional): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with key 'fitted_values'\n\n    \"\"\"\n    plot.plot_fitted_values_per_sensor(\n        mcmc_object=self.mcmc, sensor_object=self.sensor_object, background_model=self.components[\"background\"]\n    )\n    return plot\n</code></pre>"},{"location":"pyelq/preprocessing/","title":"Pre-Processing","text":""},{"location":"pyelq/preprocessing/#pre-processing","title":"Pre-processing","text":"<p>Class for performing preprocessing on the loaded data.</p>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor","title":"<code>Preprocessor</code>  <code>dataclass</code>","text":"<p>Class which implements generic functionality for pre-processing of sensor and meteorology information.</p> <p>Attributes:</p> Name Type Description <code>time_bin_edges</code> <code>DatetimeArray</code> <p>edges of the time bins to be used for smoothing/interpolation.</p> <code>sensor_object</code> <code>SensorGroup</code> <p>sensor group object containing raw data.</p> <code>met_object</code> <code>Meteorology</code> <p>met object containing raw data.</p> <code>aggregate_function</code> <code>str</code> <p>function to be used for aggregation of data. Defaults to mean.</p> <code>sensor_fields</code> <code>list</code> <p>standard list of sensor attributes that we wish to regularize and/or filter.</p> <code>met_fields</code> <code>list</code> <p>standard list of meteorology attributes that we wish to regularize/filter.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>@dataclass\nclass Preprocessor:\n    \"\"\"Class which implements generic functionality for pre-processing of sensor and meteorology information.\n\n    Attributes:\n        time_bin_edges (pd.arrays.DatetimeArray): edges of the time bins to be used for smoothing/interpolation.\n        sensor_object (SensorGroup): sensor group object containing raw data.\n        met_object (Meteorology): met object containing raw data.\n        aggregate_function (str): function to be used for aggregation of data. Defaults to mean.\n        sensor_fields (list): standard list of sensor attributes that we wish to regularize and/or filter.\n        met_fields (list): standard list of meteorology attributes that we wish to regularize/filter.\n\n    \"\"\"\n\n    time_bin_edges: pd.arrays.DatetimeArray\n    sensor_object: SensorGroup\n    met_object: Union[Meteorology, MeteorologyGroup]\n    aggregate_function: str = \"mean\"\n    sensor_fields = [\"time\", \"concentration\", \"source_on\"]\n    met_fields = [\n        \"time\",\n        \"wind_direction\",\n        \"wind_speed\",\n        \"pressure\",\n        \"temperature\",\n        \"u_component\",\n        \"v_component\",\n        \"w_component\",\n        \"wind_turbulence_horizontal\",\n        \"wind_turbulence_vertical\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialise the class.\n\n        Attaching the sensor and meteorology objects as attributes, and running initial regularization and NaN filtering\n        steps.\n\n        Before running the regularization &amp; NaN filtering, the function ensures that u_component and v_component are\n        present as fields on met_object. The post-smoothing wind speed and direction are then calculated from the\n        smoothed u and v components, to eliminate the need to take means of directions when binning.\n\n        The sensor and meteorology group objects attached to the class will have identical numbers of data points per\n        device, identical time stamps, and be free of NaNs.\n\n        \"\"\"\n        self.met_object.calculate_uv_from_wind_speed_direction()\n\n        self.regularize_data()\n        self.met_object.calculate_wind_direction_from_uv()\n        self.met_object.calculate_wind_speed_from_uv()\n        self.filter_nans()\n\n    def regularize_data(self) -&gt; None:\n        \"\"\"Smoothing or interpolation of data onto a common set of time points.\n\n        Function which takes in sensor and meteorology objects containing raw data (on original time points), and\n        smooths or interpolates these onto a common set of time points.\n\n        When a SensorGroup object is supplied, the function will return a SensorGroup object with the same number of\n        sensors. When a MeteorologyGroup object is supplied, the function will return a MeteorologyGroup object with the\n        same number of objects. When a Meteorology object is supplied, the function will return a MeteorologyGroup\n        object with the same number of objects as there is sensors in the SensorGroup object. The individual Meteorology\n        objects will be identical.\n\n        Assumes that sensor_object and met_object attributes contain the RAW data, on the original time stamps, as\n        loaded from file/API using the relevant data access class.\n\n        After the function has been run, the sensor and meteorology group objects attached to the class as attributes\n        will have identical time stamps, but may still contain NaNs.\n\n        \"\"\"\n        sensor_out = deepcopy(self.sensor_object)\n        for sns_new, sns_old in zip(sensor_out.values(), self.sensor_object.values()):\n            for field in self.sensor_fields:\n                if (field != \"time\") and (getattr(sns_old, field) is not None):\n                    time_out, resampled_values = temporal_resampling(\n                        sns_old.time, getattr(sns_old, field), self.time_bin_edges, self.aggregate_function\n                    )\n                    setattr(sns_new, field, resampled_values)\n            sns_new.time = time_out\n\n        met_out = MeteorologyGroup()\n        if isinstance(self.met_object, Meteorology):\n            single_met_object = self.interpolate_single_met_object(met_in_object=self.met_object)\n            for key in sensor_out.keys():\n                met_out[key] = single_met_object\n        else:\n            for key, temp_met_object in self.met_object.items():\n                met_out[key] = self.interpolate_single_met_object(met_in_object=temp_met_object)\n\n        self.sensor_object = sensor_out\n        self.met_object = met_out\n\n    def filter_nans(self) -&gt; None:\n        \"\"\"Filter out data points where any of the specified sensor or meteorology fields has a NaN value.\n\n        Assumes that sensor_object and met_object attributes have first been passed through the regularize_data\n        function, and thus have fields on aligned time grids.\n\n        Function first works through all sensor and meteorology fields and finds indices of all times where there is a\n        NaN value in any field. Then, it uses the resulting index to filter all fields.\n\n        The result of this function is that the sensor_object and met_object attributes of the class are updated, any\n        NaN values having been removed.\n\n        \"\"\"\n        for sns_key, met_key in zip(self.sensor_object, self.met_object):\n            sns_in = self.sensor_object[sns_key]\n            met_in = self.met_object[met_key]\n            filter_index = np.ones(sns_in.nof_observations, dtype=bool)\n            for field in self.sensor_fields:\n                if (field != \"time\") and (getattr(sns_in, field) is not None):\n                    filter_index = np.logical_and(filter_index, np.logical_not(np.isnan(getattr(sns_in, field))))\n            for field in self.met_fields:\n                if (field != \"time\") and (getattr(met_in, field) is not None):\n                    filter_index = np.logical_and(filter_index, np.logical_not(np.isnan(getattr(met_in, field))))\n\n            self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, filter_index)\n            self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, filter_index)\n\n    def filter_on_met(self, filter_variable: list, lower_limit: list = None, upper_limit: list = None) -&gt; None:\n        \"\"\"Filter the supplied data on given properties of the meteorological data.\n\n        Assumes that the SensorGroup and MeteorologyGroup objects attached as attributes have corresponding values (one\n        per sensor device), and have attributes that have been pre-smoothed/interpolated onto a common time grid per\n        device.\n\n        The result of this function is that the sensor_object and met_object attributes are updated with the filtered\n        versions.\n\n        Args:\n            filter_variable (list of str): list of meteorology variables that we wish to use for filtering.\n            lower_limit (list of float): list of lower limits associated with the variables in filter_variables.\n                Defaults to None.\n            upper_limit (list of float): list of upper limits associated with the variables in filter_variables.\n                Defaults to None.\n\n        \"\"\"\n        if lower_limit is None:\n            lower_limit = [-np.inf] * len(filter_variable)\n        if upper_limit is None:\n            upper_limit = [np.inf] * len(filter_variable)\n\n        for vrb, low, high in zip(filter_variable, lower_limit, upper_limit):\n            for sns_key, met_key in zip(self.sensor_object, self.met_object):\n                sns_in = self.sensor_object[sns_key]\n                met_in = self.met_object[met_key]\n                index_keep = np.logical_and(getattr(met_in, vrb) &gt;= low, getattr(met_in, vrb) &lt;= high)\n                self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, index_keep)\n                self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, index_keep)\n\n    def block_data(\n        self, time_edges: pd.arrays.DatetimeArray, data_object: Union[SensorGroup, MeteorologyGroup]\n    ) -&gt; list:\n        \"\"\"Break the supplied data group objects into time-blocked chunks.\n\n        Returning a list of sensor and meteorology group objects per time chunk.\n\n        If there is no data for a given device in a particular period, then that device is simply dropped from the group\n        object in that block.\n\n        Either a SensorGroup or a MeteorologyGroup object can be supplied, and the list of blocked objects returned will\n        be of the same type.\n\n        Args:\n            time_edges (pd.Arrays.DatetimeArray): [(n_period + 1) x 1] array of edges of the time bins to be used for\n                dividing the data into blocks.\n            data_object (SensorGroup or MeteorologyGroup): data object containing either or meteorological data, to be\n                divided into blocks.\n\n        Returns:\n            data_list (list): list of [n_period x 1] data objects, each list element being either a SensorGroup or\n                MeteorologyGroup object (depending on the input) containing the data for the corresponding period.\n\n        \"\"\"\n        data_list = []\n        nof_periods = len(time_edges) - 1\n        if isinstance(data_object, SensorGroup):\n            field_list = self.sensor_fields\n        elif isinstance(data_object, MeteorologyGroup):\n            field_list = self.met_fields\n        else:\n            raise TypeError(\"Data input must be either a SensorGroup or MeteorologyGroup.\")\n\n        for k in range(nof_periods):\n            data_list.append(type(data_object)())\n            for key, dat in data_object.items():\n                idx_time = (dat.time &gt;= time_edges[k]) &amp; (dat.time &lt;= time_edges[k + 1])\n                if np.any(idx_time):\n                    data_list[-1][key] = deepcopy(dat)\n                    data_list[-1][key] = self.filter_object_fields(data_list[-1][key], field_list, idx_time)\n        return data_list\n\n    @staticmethod\n    def filter_object_fields(\n        data_object: Union[Sensor, Meteorology], fields: list, index: np.ndarray\n    ) -&gt; Union[Sensor, Meteorology]:\n        \"\"\"Apply a filter index to all the fields in a given data object.\n\n        Can be used for either a Sensor or Meteorology object.\n\n        Args:\n            data_object (Union[Sensor, Meteorology]): sensor or meteorology object (corresponding to a single device)\n                for which fields are to be filtered.\n            fields (list): list of field names to be filtered.\n            index (np.ndarray): filter index.\n\n        Returns:\n            Union[Sensor, Meteorology]: filtered data object.\n\n        \"\"\"\n        return_object = deepcopy(data_object)\n        for field in fields:\n            if getattr(return_object, field) is not None:\n                setattr(return_object, field, getattr(return_object, field)[index])\n        return return_object\n\n    def interpolate_single_met_object(self, met_in_object: Meteorology) -&gt; Meteorology:\n        \"\"\"Interpolate a single Meteorology object onto the time grid of the class.\n\n        Args:\n            met_in_object (Meteorology): Meteorology object to be interpolated onto the time grid of the class.\n\n        Returns:\n            met_out_object (Meteorology): interpolated Meteorology object.\n\n        \"\"\"\n        met_out_object = Meteorology()\n        time_out = None\n        for field in self.met_fields:\n            if (field != \"time\") and (getattr(met_in_object, field) is not None):\n                time_out, resampled_values = temporal_resampling(\n                    met_in_object.time,\n                    getattr(met_in_object, field),\n                    self.time_bin_edges,\n                    self.aggregate_function,\n                )\n                setattr(met_out_object, field, resampled_values)\n\n        if time_out is not None:\n            met_out_object.time = time_out\n\n        return met_out_object\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialise the class.</p> <p>Attaching the sensor and meteorology objects as attributes, and running initial regularization and NaN filtering steps.</p> <p>Before running the regularization &amp; NaN filtering, the function ensures that u_component and v_component are present as fields on met_object. The post-smoothing wind speed and direction are then calculated from the smoothed u and v components, to eliminate the need to take means of directions when binning.</p> <p>The sensor and meteorology group objects attached to the class will have identical numbers of data points per device, identical time stamps, and be free of NaNs.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialise the class.\n\n    Attaching the sensor and meteorology objects as attributes, and running initial regularization and NaN filtering\n    steps.\n\n    Before running the regularization &amp; NaN filtering, the function ensures that u_component and v_component are\n    present as fields on met_object. The post-smoothing wind speed and direction are then calculated from the\n    smoothed u and v components, to eliminate the need to take means of directions when binning.\n\n    The sensor and meteorology group objects attached to the class will have identical numbers of data points per\n    device, identical time stamps, and be free of NaNs.\n\n    \"\"\"\n    self.met_object.calculate_uv_from_wind_speed_direction()\n\n    self.regularize_data()\n    self.met_object.calculate_wind_direction_from_uv()\n    self.met_object.calculate_wind_speed_from_uv()\n    self.filter_nans()\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.regularize_data","title":"<code>regularize_data()</code>","text":"<p>Smoothing or interpolation of data onto a common set of time points.</p> <p>Function which takes in sensor and meteorology objects containing raw data (on original time points), and smooths or interpolates these onto a common set of time points.</p> <p>When a SensorGroup object is supplied, the function will return a SensorGroup object with the same number of sensors. When a MeteorologyGroup object is supplied, the function will return a MeteorologyGroup object with the same number of objects. When a Meteorology object is supplied, the function will return a MeteorologyGroup object with the same number of objects as there is sensors in the SensorGroup object. The individual Meteorology objects will be identical.</p> <p>Assumes that sensor_object and met_object attributes contain the RAW data, on the original time stamps, as loaded from file/API using the relevant data access class.</p> <p>After the function has been run, the sensor and meteorology group objects attached to the class as attributes will have identical time stamps, but may still contain NaNs.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def regularize_data(self) -&gt; None:\n    \"\"\"Smoothing or interpolation of data onto a common set of time points.\n\n    Function which takes in sensor and meteorology objects containing raw data (on original time points), and\n    smooths or interpolates these onto a common set of time points.\n\n    When a SensorGroup object is supplied, the function will return a SensorGroup object with the same number of\n    sensors. When a MeteorologyGroup object is supplied, the function will return a MeteorologyGroup object with the\n    same number of objects. When a Meteorology object is supplied, the function will return a MeteorologyGroup\n    object with the same number of objects as there is sensors in the SensorGroup object. The individual Meteorology\n    objects will be identical.\n\n    Assumes that sensor_object and met_object attributes contain the RAW data, on the original time stamps, as\n    loaded from file/API using the relevant data access class.\n\n    After the function has been run, the sensor and meteorology group objects attached to the class as attributes\n    will have identical time stamps, but may still contain NaNs.\n\n    \"\"\"\n    sensor_out = deepcopy(self.sensor_object)\n    for sns_new, sns_old in zip(sensor_out.values(), self.sensor_object.values()):\n        for field in self.sensor_fields:\n            if (field != \"time\") and (getattr(sns_old, field) is not None):\n                time_out, resampled_values = temporal_resampling(\n                    sns_old.time, getattr(sns_old, field), self.time_bin_edges, self.aggregate_function\n                )\n                setattr(sns_new, field, resampled_values)\n        sns_new.time = time_out\n\n    met_out = MeteorologyGroup()\n    if isinstance(self.met_object, Meteorology):\n        single_met_object = self.interpolate_single_met_object(met_in_object=self.met_object)\n        for key in sensor_out.keys():\n            met_out[key] = single_met_object\n    else:\n        for key, temp_met_object in self.met_object.items():\n            met_out[key] = self.interpolate_single_met_object(met_in_object=temp_met_object)\n\n    self.sensor_object = sensor_out\n    self.met_object = met_out\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_nans","title":"<code>filter_nans()</code>","text":"<p>Filter out data points where any of the specified sensor or meteorology fields has a NaN value.</p> <p>Assumes that sensor_object and met_object attributes have first been passed through the regularize_data function, and thus have fields on aligned time grids.</p> <p>Function first works through all sensor and meteorology fields and finds indices of all times where there is a NaN value in any field. Then, it uses the resulting index to filter all fields.</p> <p>The result of this function is that the sensor_object and met_object attributes of the class are updated, any NaN values having been removed.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def filter_nans(self) -&gt; None:\n    \"\"\"Filter out data points where any of the specified sensor or meteorology fields has a NaN value.\n\n    Assumes that sensor_object and met_object attributes have first been passed through the regularize_data\n    function, and thus have fields on aligned time grids.\n\n    Function first works through all sensor and meteorology fields and finds indices of all times where there is a\n    NaN value in any field. Then, it uses the resulting index to filter all fields.\n\n    The result of this function is that the sensor_object and met_object attributes of the class are updated, any\n    NaN values having been removed.\n\n    \"\"\"\n    for sns_key, met_key in zip(self.sensor_object, self.met_object):\n        sns_in = self.sensor_object[sns_key]\n        met_in = self.met_object[met_key]\n        filter_index = np.ones(sns_in.nof_observations, dtype=bool)\n        for field in self.sensor_fields:\n            if (field != \"time\") and (getattr(sns_in, field) is not None):\n                filter_index = np.logical_and(filter_index, np.logical_not(np.isnan(getattr(sns_in, field))))\n        for field in self.met_fields:\n            if (field != \"time\") and (getattr(met_in, field) is not None):\n                filter_index = np.logical_and(filter_index, np.logical_not(np.isnan(getattr(met_in, field))))\n\n        self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, filter_index)\n        self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, filter_index)\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_on_met","title":"<code>filter_on_met(filter_variable, lower_limit=None, upper_limit=None)</code>","text":"<p>Filter the supplied data on given properties of the meteorological data.</p> <p>Assumes that the SensorGroup and MeteorologyGroup objects attached as attributes have corresponding values (one per sensor device), and have attributes that have been pre-smoothed/interpolated onto a common time grid per device.</p> <p>The result of this function is that the sensor_object and met_object attributes are updated with the filtered versions.</p> <p>Parameters:</p> Name Type Description Default <code>filter_variable</code> <code>list of str</code> <p>list of meteorology variables that we wish to use for filtering.</p> required <code>lower_limit</code> <code>list of float</code> <p>list of lower limits associated with the variables in filter_variables. Defaults to None.</p> <code>None</code> <code>upper_limit</code> <code>list of float</code> <p>list of upper limits associated with the variables in filter_variables. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def filter_on_met(self, filter_variable: list, lower_limit: list = None, upper_limit: list = None) -&gt; None:\n    \"\"\"Filter the supplied data on given properties of the meteorological data.\n\n    Assumes that the SensorGroup and MeteorologyGroup objects attached as attributes have corresponding values (one\n    per sensor device), and have attributes that have been pre-smoothed/interpolated onto a common time grid per\n    device.\n\n    The result of this function is that the sensor_object and met_object attributes are updated with the filtered\n    versions.\n\n    Args:\n        filter_variable (list of str): list of meteorology variables that we wish to use for filtering.\n        lower_limit (list of float): list of lower limits associated with the variables in filter_variables.\n            Defaults to None.\n        upper_limit (list of float): list of upper limits associated with the variables in filter_variables.\n            Defaults to None.\n\n    \"\"\"\n    if lower_limit is None:\n        lower_limit = [-np.inf] * len(filter_variable)\n    if upper_limit is None:\n        upper_limit = [np.inf] * len(filter_variable)\n\n    for vrb, low, high in zip(filter_variable, lower_limit, upper_limit):\n        for sns_key, met_key in zip(self.sensor_object, self.met_object):\n            sns_in = self.sensor_object[sns_key]\n            met_in = self.met_object[met_key]\n            index_keep = np.logical_and(getattr(met_in, vrb) &gt;= low, getattr(met_in, vrb) &lt;= high)\n            self.sensor_object[sns_key] = self.filter_object_fields(sns_in, self.sensor_fields, index_keep)\n            self.met_object[met_key] = self.filter_object_fields(met_in, self.met_fields, index_keep)\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.block_data","title":"<code>block_data(time_edges, data_object)</code>","text":"<p>Break the supplied data group objects into time-blocked chunks.</p> <p>Returning a list of sensor and meteorology group objects per time chunk.</p> <p>If there is no data for a given device in a particular period, then that device is simply dropped from the group object in that block.</p> <p>Either a SensorGroup or a MeteorologyGroup object can be supplied, and the list of blocked objects returned will be of the same type.</p> <p>Parameters:</p> Name Type Description Default <code>time_edges</code> <code>DatetimeArray</code> <p>[(n_period + 1) x 1] array of edges of the time bins to be used for dividing the data into blocks.</p> required <code>data_object</code> <code>SensorGroup or MeteorologyGroup</code> <p>data object containing either or meteorological data, to be divided into blocks.</p> required <p>Returns:</p> Name Type Description <code>data_list</code> <code>list</code> <p>list of [n_period x 1] data objects, each list element being either a SensorGroup or MeteorologyGroup object (depending on the input) containing the data for the corresponding period.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def block_data(\n    self, time_edges: pd.arrays.DatetimeArray, data_object: Union[SensorGroup, MeteorologyGroup]\n) -&gt; list:\n    \"\"\"Break the supplied data group objects into time-blocked chunks.\n\n    Returning a list of sensor and meteorology group objects per time chunk.\n\n    If there is no data for a given device in a particular period, then that device is simply dropped from the group\n    object in that block.\n\n    Either a SensorGroup or a MeteorologyGroup object can be supplied, and the list of blocked objects returned will\n    be of the same type.\n\n    Args:\n        time_edges (pd.Arrays.DatetimeArray): [(n_period + 1) x 1] array of edges of the time bins to be used for\n            dividing the data into blocks.\n        data_object (SensorGroup or MeteorologyGroup): data object containing either or meteorological data, to be\n            divided into blocks.\n\n    Returns:\n        data_list (list): list of [n_period x 1] data objects, each list element being either a SensorGroup or\n            MeteorologyGroup object (depending on the input) containing the data for the corresponding period.\n\n    \"\"\"\n    data_list = []\n    nof_periods = len(time_edges) - 1\n    if isinstance(data_object, SensorGroup):\n        field_list = self.sensor_fields\n    elif isinstance(data_object, MeteorologyGroup):\n        field_list = self.met_fields\n    else:\n        raise TypeError(\"Data input must be either a SensorGroup or MeteorologyGroup.\")\n\n    for k in range(nof_periods):\n        data_list.append(type(data_object)())\n        for key, dat in data_object.items():\n            idx_time = (dat.time &gt;= time_edges[k]) &amp; (dat.time &lt;= time_edges[k + 1])\n            if np.any(idx_time):\n                data_list[-1][key] = deepcopy(dat)\n                data_list[-1][key] = self.filter_object_fields(data_list[-1][key], field_list, idx_time)\n    return data_list\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.filter_object_fields","title":"<code>filter_object_fields(data_object, fields, index)</code>  <code>staticmethod</code>","text":"<p>Apply a filter index to all the fields in a given data object.</p> <p>Can be used for either a Sensor or Meteorology object.</p> <p>Parameters:</p> Name Type Description Default <code>data_object</code> <code>Union[Sensor, Meteorology]</code> <p>sensor or meteorology object (corresponding to a single device) for which fields are to be filtered.</p> required <code>fields</code> <code>list</code> <p>list of field names to be filtered.</p> required <code>index</code> <code>ndarray</code> <p>filter index.</p> required <p>Returns:</p> Type Description <code>Union[Sensor, Meteorology]</code> <p>Union[Sensor, Meteorology]: filtered data object.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>@staticmethod\ndef filter_object_fields(\n    data_object: Union[Sensor, Meteorology], fields: list, index: np.ndarray\n) -&gt; Union[Sensor, Meteorology]:\n    \"\"\"Apply a filter index to all the fields in a given data object.\n\n    Can be used for either a Sensor or Meteorology object.\n\n    Args:\n        data_object (Union[Sensor, Meteorology]): sensor or meteorology object (corresponding to a single device)\n            for which fields are to be filtered.\n        fields (list): list of field names to be filtered.\n        index (np.ndarray): filter index.\n\n    Returns:\n        Union[Sensor, Meteorology]: filtered data object.\n\n    \"\"\"\n    return_object = deepcopy(data_object)\n    for field in fields:\n        if getattr(return_object, field) is not None:\n            setattr(return_object, field, getattr(return_object, field)[index])\n    return return_object\n</code></pre>"},{"location":"pyelq/preprocessing/#pyelq.preprocessing.Preprocessor.interpolate_single_met_object","title":"<code>interpolate_single_met_object(met_in_object)</code>","text":"<p>Interpolate a single Meteorology object onto the time grid of the class.</p> <p>Parameters:</p> Name Type Description Default <code>met_in_object</code> <code>Meteorology</code> <p>Meteorology object to be interpolated onto the time grid of the class.</p> required <p>Returns:</p> Name Type Description <code>met_out_object</code> <code>Meteorology</code> <p>interpolated Meteorology object.</p> Source code in <code>src/pyelq/preprocessing.py</code> <pre><code>def interpolate_single_met_object(self, met_in_object: Meteorology) -&gt; Meteorology:\n    \"\"\"Interpolate a single Meteorology object onto the time grid of the class.\n\n    Args:\n        met_in_object (Meteorology): Meteorology object to be interpolated onto the time grid of the class.\n\n    Returns:\n        met_out_object (Meteorology): interpolated Meteorology object.\n\n    \"\"\"\n    met_out_object = Meteorology()\n    time_out = None\n    for field in self.met_fields:\n        if (field != \"time\") and (getattr(met_in_object, field) is not None):\n            time_out, resampled_values = temporal_resampling(\n                met_in_object.time,\n                getattr(met_in_object, field),\n                self.time_bin_edges,\n                self.aggregate_function,\n            )\n            setattr(met_out_object, field, resampled_values)\n\n    if time_out is not None:\n        met_out_object.time = time_out\n\n    return met_out_object\n</code></pre>"},{"location":"pyelq/source_map/","title":"Source Map","text":""},{"location":"pyelq/source_map/#source-map","title":"Source Map","text":"<p>SourceMap module.</p> <p>The class for the source maps used in pyELQ</p>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap","title":"<code>SourceMap</code>  <code>dataclass</code>","text":"<p>Defines SourceMap class.</p> <p>Attributes:</p> Name Type Description <code>location</code> <code>Coordinate</code> <p>Coordinate object specifying the potential source locations</p> <code>prior_value</code> <code>ndarray</code> <p>Array with prior values for each source</p> <code>inclusion_idx</code> <code>ndarray</code> <p>Array of lists containing indices of the observations of a corresponding sensor_object which are within the inclusion_radius of that particular source</p> <code>inclusion_n_obs</code> <code>list</code> <p>Array containing number of observations of a sensor_object within radius for each source</p> Source code in <code>src/pyelq/source_map.py</code> <pre><code>@dataclass\nclass SourceMap:\n    \"\"\"Defines SourceMap class.\n\n    Attributes:\n        location (Coordinate, optional): Coordinate object specifying the potential source locations\n        prior_value (np.ndarray, optional): Array with prior values for each source\n        inclusion_idx (np.ndarray, optional): Array of lists containing indices of the observations of a\n            corresponding sensor_object which are within the inclusion_radius of that particular source\n        inclusion_n_obs (list, optional): Array containing number of observations of a sensor_object within\n            radius for each source\n\n    \"\"\"\n\n    location: Coordinate = field(init=False, default=None)\n    prior_value: np.ndarray = None\n    inclusion_idx: np.ndarray = field(init=False, default=None)\n    inclusion_n_obs: np.ndarray = field(init=False, default=None)\n\n    @property\n    def nof_sources(self) -&gt; int:\n        \"\"\"Number of sources.\"\"\"\n        if self.location is None:\n            return 0\n        return self.location.nof_observations\n\n    def calculate_inclusion_idx(self, sensor_object: Sensor, inclusion_radius: Union[int, np.ndarray]) -&gt; None:\n        \"\"\"Find observation indices which are within specified radius of each source location.\n\n        This method takes the sensor object and for each source in the source_map object it calculates which\n        observations are within the specified radius.\n        When sensor_object location and sourcemap_object location are not of the same type, simply convert both to ECEF\n        and calculate inclusion indices accordingly.\n        The result is an array of lists which are the indices of the observations in sensor_object which are within the\n        specified radius. Result is stored in the corresponding attribute.\n        Also calculating number of observations in radius per source and storing result as a list in inclusion_n_obs\n        attribute\n        When a location attribute is in LLA we convert to ECEF for the inclusion radius to make sense\n\n        Args:\n            sensor_object (Sensor): Sensor object containing location information on the observations under\n                consideration\n            inclusion_radius (Union[float, np.ndarray], optional): Inclusion radius in [m] radius from source\n                for which we take observations into account\n\n        \"\"\"\n        sensor_kd_tree = sensor_object.location.to_ecef().create_tree()\n        source_points = self.location.to_ecef().to_array()\n\n        inclusion_idx = sensor_kd_tree.query_ball_point(source_points, inclusion_radius)\n        idx_array = np.array(inclusion_idx, dtype=object)\n        self.inclusion_idx = idx_array\n        self.inclusion_n_obs = np.array([len(value) for value in self.inclusion_idx])\n\n    def generate_sources(\n        self,\n        coordinate_object: Coordinate,\n        sourcemap_limits: np.ndarray,\n        sourcemap_type: str = \"central\",\n        nof_sources: int = 5,\n        grid_shape: Union[tuple, np.ndarray] = (5, 5, 1),\n    ) -&gt; None:\n        \"\"\"Generates source locations based on specified inputs.\n\n        The result gets stored in the location attribute\n\n        In grid_sphere we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad\n        and temp_lon_rad\n\n        Args:\n            coordinate_object (Coordinate): Empty coordinate object which specifies the coordinate class to populate\n                location with\n            sourcemap_limits (np.ndarray): Limits of the sourcemap on which to generate the sources of size [dim x 2]\n                if dim == 2 we assume the third dimension will be zeros. Assuming the units of the limits are defined in\n                the desired coordinate system\n            sourcemap_type (str, optional): Type of sourcemap to generate: central == 1 central source,\n                hypercube == nof_sources through a Latin Hypercube design, grid == grid of shape grid_shape\n                filled with sources, grid_sphere == grid of shape grid_shape taking into account a spherical spacing\n            nof_sources (int, optional): Number of sources to generate (used in 'hypercube' case)\n            grid_shape: (tuple, optional): Number of sources to generate in each dimension, total number of\n                sources will be the product of the entries of this tuple (used in 'grid' and 'grid_sphere' case)\n\n        \"\"\"\n        sourcemap_dimension = sourcemap_limits.shape[0]\n        if sourcemap_type == \"central\":\n            array = sourcemap_limits.mean(axis=1).reshape(1, sourcemap_dimension)\n        elif sourcemap_type == \"hypercube\":\n            array = make_latin_hypercube(bounds=sourcemap_limits, nof_samples=nof_sources)\n        elif sourcemap_type == \"grid\":\n            array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"rectangular\", shape=grid_shape)\n        elif sourcemap_type == \"grid_sphere\":\n            array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"spherical\", shape=grid_shape)\n        else:\n            raise NotImplementedError(\"Please provide a valid sourcemap type\")\n        coordinate_object.from_array(array=array)\n        self.location = coordinate_object\n</code></pre>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap.nof_sources","title":"<code>nof_sources</code>  <code>property</code>","text":"<p>Number of sources.</p>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap.calculate_inclusion_idx","title":"<code>calculate_inclusion_idx(sensor_object, inclusion_radius)</code>","text":"<p>Find observation indices which are within specified radius of each source location.</p> <p>This method takes the sensor object and for each source in the source_map object it calculates which observations are within the specified radius. When sensor_object location and sourcemap_object location are not of the same type, simply convert both to ECEF and calculate inclusion indices accordingly. The result is an array of lists which are the indices of the observations in sensor_object which are within the specified radius. Result is stored in the corresponding attribute. Also calculating number of observations in radius per source and storing result as a list in inclusion_n_obs attribute When a location attribute is in LLA we convert to ECEF for the inclusion radius to make sense</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Sensor object containing location information on the observations under consideration</p> required <code>inclusion_radius</code> <code>Union[float, ndarray]</code> <p>Inclusion radius in [m] radius from source for which we take observations into account</p> required Source code in <code>src/pyelq/source_map.py</code> <pre><code>def calculate_inclusion_idx(self, sensor_object: Sensor, inclusion_radius: Union[int, np.ndarray]) -&gt; None:\n    \"\"\"Find observation indices which are within specified radius of each source location.\n\n    This method takes the sensor object and for each source in the source_map object it calculates which\n    observations are within the specified radius.\n    When sensor_object location and sourcemap_object location are not of the same type, simply convert both to ECEF\n    and calculate inclusion indices accordingly.\n    The result is an array of lists which are the indices of the observations in sensor_object which are within the\n    specified radius. Result is stored in the corresponding attribute.\n    Also calculating number of observations in radius per source and storing result as a list in inclusion_n_obs\n    attribute\n    When a location attribute is in LLA we convert to ECEF for the inclusion radius to make sense\n\n    Args:\n        sensor_object (Sensor): Sensor object containing location information on the observations under\n            consideration\n        inclusion_radius (Union[float, np.ndarray], optional): Inclusion radius in [m] radius from source\n            for which we take observations into account\n\n    \"\"\"\n    sensor_kd_tree = sensor_object.location.to_ecef().create_tree()\n    source_points = self.location.to_ecef().to_array()\n\n    inclusion_idx = sensor_kd_tree.query_ball_point(source_points, inclusion_radius)\n    idx_array = np.array(inclusion_idx, dtype=object)\n    self.inclusion_idx = idx_array\n    self.inclusion_n_obs = np.array([len(value) for value in self.inclusion_idx])\n</code></pre>"},{"location":"pyelq/source_map/#pyelq.source_map.SourceMap.generate_sources","title":"<code>generate_sources(coordinate_object, sourcemap_limits, sourcemap_type='central', nof_sources=5, grid_shape=(5, 5, 1))</code>","text":"<p>Generates source locations based on specified inputs.</p> <p>The result gets stored in the location attribute</p> <p>In grid_sphere we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad and temp_lon_rad</p> <p>Parameters:</p> Name Type Description Default <code>coordinate_object</code> <code>Coordinate</code> <p>Empty coordinate object which specifies the coordinate class to populate location with</p> required <code>sourcemap_limits</code> <code>ndarray</code> <p>Limits of the sourcemap on which to generate the sources of size [dim x 2] if dim == 2 we assume the third dimension will be zeros. Assuming the units of the limits are defined in the desired coordinate system</p> required <code>sourcemap_type</code> <code>str</code> <p>Type of sourcemap to generate: central == 1 central source, hypercube == nof_sources through a Latin Hypercube design, grid == grid of shape grid_shape filled with sources, grid_sphere == grid of shape grid_shape taking into account a spherical spacing</p> <code>'central'</code> <code>nof_sources</code> <code>int</code> <p>Number of sources to generate (used in 'hypercube' case)</p> <code>5</code> <code>grid_shape</code> <code>Union[tuple, ndarray]</code> <p>(tuple, optional): Number of sources to generate in each dimension, total number of sources will be the product of the entries of this tuple (used in 'grid' and 'grid_sphere' case)</p> <code>(5, 5, 1)</code> Source code in <code>src/pyelq/source_map.py</code> <pre><code>def generate_sources(\n    self,\n    coordinate_object: Coordinate,\n    sourcemap_limits: np.ndarray,\n    sourcemap_type: str = \"central\",\n    nof_sources: int = 5,\n    grid_shape: Union[tuple, np.ndarray] = (5, 5, 1),\n) -&gt; None:\n    \"\"\"Generates source locations based on specified inputs.\n\n    The result gets stored in the location attribute\n\n    In grid_sphere we scale the latitude and longitude from -90/90 and -180/180 to 0/1 for the use in temp_lat_rad\n    and temp_lon_rad\n\n    Args:\n        coordinate_object (Coordinate): Empty coordinate object which specifies the coordinate class to populate\n            location with\n        sourcemap_limits (np.ndarray): Limits of the sourcemap on which to generate the sources of size [dim x 2]\n            if dim == 2 we assume the third dimension will be zeros. Assuming the units of the limits are defined in\n            the desired coordinate system\n        sourcemap_type (str, optional): Type of sourcemap to generate: central == 1 central source,\n            hypercube == nof_sources through a Latin Hypercube design, grid == grid of shape grid_shape\n            filled with sources, grid_sphere == grid of shape grid_shape taking into account a spherical spacing\n        nof_sources (int, optional): Number of sources to generate (used in 'hypercube' case)\n        grid_shape: (tuple, optional): Number of sources to generate in each dimension, total number of\n            sources will be the product of the entries of this tuple (used in 'grid' and 'grid_sphere' case)\n\n    \"\"\"\n    sourcemap_dimension = sourcemap_limits.shape[0]\n    if sourcemap_type == \"central\":\n        array = sourcemap_limits.mean(axis=1).reshape(1, sourcemap_dimension)\n    elif sourcemap_type == \"hypercube\":\n        array = make_latin_hypercube(bounds=sourcemap_limits, nof_samples=nof_sources)\n    elif sourcemap_type == \"grid\":\n        array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"rectangular\", shape=grid_shape)\n    elif sourcemap_type == \"grid_sphere\":\n        array = coordinate_object.make_grid(bounds=sourcemap_limits, grid_type=\"spherical\", shape=grid_shape)\n    else:\n        raise NotImplementedError(\"Please provide a valid sourcemap type\")\n    coordinate_object.from_array(array=array)\n    self.location = coordinate_object\n</code></pre>"},{"location":"pyelq/component/background/","title":"Background","text":""},{"location":"pyelq/component/background/#background","title":"Background","text":"<p>Model components for background modelling.</p>"},{"location":"pyelq/component/background/#pyelq.component.background.Background","title":"<code>Background</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code></p> <p>Superclass for background models.</p> <p>Attributes:</p> Name Type Description <code>n_obs</code> <code>int</code> <p>total number of observations in the background model (across all sensors).</p> <code>n_parameter</code> <code>int</code> <p>number of parameters in the background model</p> <code>bg</code> <code>ndarray</code> <p>array of sampled background values, populated in self.from_mcmc() after the MCMC run is completed.</p> <code>precision_scalar</code> <code>ndarray</code> <p>array of sampled background precision values, populated in self.from_mcmc() after the MCMC run is completed. Only populated if update_precision is True.</p> <code>precision_matrix</code> <code>Union[ndarray, csr_array]</code> <p>un-scaled precision matrix for the background parameter vector.</p> <code>mean_bg</code> <code>float</code> <p>global mean background value. Should be populated from the value specified in the GasSpecies object.</p> <code>update_precision</code> <code>bool</code> <p>logical determining whether the background (scalar) precision parameter should be updated as part of the MCMC. Defaults to False.</p> <code>prior_precision_shape</code> <code>float</code> <p>shape parameter for the prior gamma distribution for the scalar precision parameter(s).</p> <code>prior_precision_rate</code> <code>float</code> <p>rate parameter for the prior gamma distribution for the scalar precision parameter(s).</p> <code>initial_precision</code> <code>float</code> <p>initial value for the scalar precision parameter.</p> <code>basis_matrix</code> <code>csr_array</code> <p>[n_obs x n_time] matrix mapping the background model parameters on to the observations.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>@dataclass\nclass Background(Component):\n    \"\"\"Superclass for background models.\n\n    Attributes:\n        n_obs (int): total number of observations in the background model (across all sensors).\n        n_parameter (int): number of parameters in the background model\n        bg (np.ndarray): array of sampled background values, populated in self.from_mcmc() after the MCMC run is\n            completed.\n        precision_scalar (np.ndarray): array of sampled background precision values, populated in self.from_mcmc() after\n            the MCMC run is completed. Only populated if update_precision is True.\n        precision_matrix (Union[np.ndarray, sparse.csr_array]): un-scaled precision matrix for the background parameter\n            vector.\n        mean_bg (float): global mean background value. Should be populated from the value specified in the GasSpecies\n            object.\n        update_precision (bool): logical determining whether the background (scalar) precision parameter should be\n            updated as part of the MCMC. Defaults to False.\n        prior_precision_shape (float): shape parameter for the prior gamma distribution for the scalar precision\n            parameter(s).\n        prior_precision_rate (float): rate parameter for the prior gamma distribution for the scalar precision\n            parameter(s).\n        initial_precision (float): initial value for the scalar precision parameter.\n        basis_matrix (sparse.csr_array): [n_obs x n_time] matrix mapping the background model parameters on to the\n            observations.\n\n    \"\"\"\n\n    n_obs: int = field(init=False)\n    n_parameter: int = field(init=False)\n    bg: np.ndarray = field(init=False)\n    precision_scalar: np.ndarray = field(init=False)\n    precision_matrix: Union[np.ndarray, sparse.csc_matrix] = field(init=False)\n    mean_bg: Union[float, None] = None\n    update_precision: bool = False\n    prior_precision_shape: float = 1e-3\n    prior_precision_rate: float = 1e-3\n    initial_precision: float = 1.0\n    basis_matrix: sparse.csr_array = field(init=False)\n\n    @abstractmethod\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n\n    def make_model(self, model: list = None) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to None.\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n        bg_precision_predictor = parameter.ScaledMatrix(matrix=\"P_bg\", scalar=\"lambda_bg\")\n        model.append(Normal(\"bg\", mean=\"mu_bg\", precision=bg_precision_predictor))\n        if self.update_precision:\n            model.append(Gamma(\"lambda_bg\", shape=\"a_lam_bg\", rate=\"b_lam_bg\"))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalNormal(\"bg\", model))\n        if self.update_precision:\n            sampler_list.append(NormalGamma(\"lambda_bg\", model))\n        return sampler_list\n\n    def make_state(self, state: dict = None) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to None.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n        state[\"mu_bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n        state[\"B_bg\"] = self.basis_matrix\n        state[\"bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n        state[\"P_bg\"] = self.precision_matrix\n        state[\"lambda_bg\"] = self.initial_precision\n        if self.update_precision:\n            state[\"a_lam_bg\"] = self.prior_precision_shape\n            state[\"b_lam_bg\"] = self.prior_precision_rate\n        return state\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.bg = store[\"bg\"]\n        if self.update_precision:\n            self.precision_scalar = store[\"lambda_bg\"]\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>  <code>abstractmethod</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>@abstractmethod\ndef initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.make_model","title":"<code>make_model(model=None)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_model(self, model: list = None) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to None.\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n    bg_precision_predictor = parameter.ScaledMatrix(matrix=\"P_bg\", scalar=\"lambda_bg\")\n    model.append(Normal(\"bg\", mean=\"mu_bg\", precision=bg_precision_predictor))\n    if self.update_precision:\n        model.append(Gamma(\"lambda_bg\", shape=\"a_lam_bg\", rate=\"b_lam_bg\"))\n    return model\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.make_sampler","title":"<code>make_sampler(model, sampler_list=None)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalNormal(\"bg\", model))\n    if self.update_precision:\n        sampler_list.append(NormalGamma(\"lambda_bg\", model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.make_state","title":"<code>make_state(state=None)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_state(self, state: dict = None) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to None.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n    state[\"mu_bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n    state[\"B_bg\"] = self.basis_matrix\n    state[\"bg\"] = np.ones((self.n_parameter, 1)) * self.mean_bg\n    state[\"P_bg\"] = self.precision_matrix\n    state[\"lambda_bg\"] = self.initial_precision\n    if self.update_precision:\n        state[\"a_lam_bg\"] = self.prior_precision_shape\n        state[\"b_lam_bg\"] = self.prior_precision_rate\n    return state\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.Background.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.bg = store[\"bg\"]\n    if self.update_precision:\n        self.precision_scalar = store[\"lambda_bg\"]\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.TemporalBackground","title":"<code>TemporalBackground</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Background</code></p> <p>Model which imposes only temporal correlation on the background parameters.</p> <p>Assumes that the prior mean concentration of the background at every location/time point is the global average background concentration as defined in the input GasSpecies object.</p> <p>Generates the (un-scaled) prior background precision matrix using the function gmrf.precision_temporal: this precision matrix imposes first-oder Markov structure for the temporal dependence.</p> <p>By default, the times used for the model definition are the set of unique times in the observation set.</p> <p>This background model only requires the initialise function, and does not require the implementation of any further methods.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>Union[ndarray, DatetimeArray]</code> <p>vector of times used in defining the model.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>@dataclass\nclass TemporalBackground(Background):\n    \"\"\"Model which imposes only temporal correlation on the background parameters.\n\n    Assumes that the prior mean concentration of the background at every location/time point is the global average\n    background concentration as defined in the input GasSpecies object.\n\n    Generates the (un-scaled) prior background precision matrix using the function gmrf.precision_temporal: this\n    precision matrix imposes first-oder Markov structure for the temporal dependence.\n\n    By default, the times used for the model definition are the set of unique times in the observation set.\n\n    This background model only requires the initialise function, and does not require the implementation of any further\n    methods.\n\n    Attributes:\n        time (Union[np.ndarray, pd.arrays.DatetimeArray]): vector of times used in defining the model.\n\n    \"\"\"\n\n    time: Union[np.ndarray, pd.arrays.DatetimeArray] = field(init=False)\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Create temporal background model from sensor, meteorology and gas species inputs.\n\n        Args:\n            sensor_object (SensorGroup): sensor data object.\n            meteorology (MeteorologyGroup): meteorology data object.\n            gas_species (GasSpecies): gas species data object.\n\n        \"\"\"\n        self.n_obs = sensor_object.nof_observations\n        self.time, unique_inverse = np.unique(sensor_object.time, return_inverse=True)\n        self.time = pd.array(self.time, dtype=\"datetime64[ns]\")\n        self.n_parameter = len(self.time)\n        self.basis_matrix = sparse.csr_array((np.ones(self.n_obs), (np.array(range(self.n_obs)), unique_inverse)))\n        self.precision_matrix = gmrf.precision_temporal(time=self.time)\n        if self.mean_bg is None:\n            self.mean_bg = gas_species.global_background\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.TemporalBackground.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Create temporal background model from sensor, meteorology and gas species inputs.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data object.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data object.</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species data object.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Create temporal background model from sensor, meteorology and gas species inputs.\n\n    Args:\n        sensor_object (SensorGroup): sensor data object.\n        meteorology (MeteorologyGroup): meteorology data object.\n        gas_species (GasSpecies): gas species data object.\n\n    \"\"\"\n    self.n_obs = sensor_object.nof_observations\n    self.time, unique_inverse = np.unique(sensor_object.time, return_inverse=True)\n    self.time = pd.array(self.time, dtype=\"datetime64[ns]\")\n    self.n_parameter = len(self.time)\n    self.basis_matrix = sparse.csr_array((np.ones(self.n_obs), (np.array(range(self.n_obs)), unique_inverse)))\n    self.precision_matrix = gmrf.precision_temporal(time=self.time)\n    if self.mean_bg is None:\n        self.mean_bg = gas_species.global_background\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground","title":"<code>SpatioTemporalBackground</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Background</code></p> <p>Model which imposes both spatial and temporal correlation on the background parameters.</p> <p>Defines a grid in time, and assumes a correlated time-series per sensor using the defined time grid.</p> <p>The background parameter is an [n_location * n_time x 1] (if self.spatial_dependence is True) or an [n_time x 1] vector (if self.spatial_dependence is False). In the spatio-temporal case, the background vector is assumed to unwrap over space and time as follows: bg = [b_1(t_1), b_2(t_1),..., b_nlct(t_1),...,b_1(t_k),..., b_nlct(t_k),...].T where nlct is the number of sensor locations. This unwrapping mechanism is chosen as it greatly speeds up the sparse matrix operations in the solver (vs. the alternative).</p> <p>self.basis_matrix is set up to map the elements of the full background vector onto the observations, on the basis of spatial location and nearest time knot.</p> <p>The temporal background correlation is computed using gmrf.precision_temporal, and the spatial correlation is computed using a squared exponential correlation function, parametrized by self.spatial_correlation_param (spatial correlation, measured in metres). The full precision matrix is simply a Kronecker product between the two component precision matrices.</p> <p>Attributes:</p> Name Type Description <code>n_time</code> <code>int</code> <p>number of time knots for which the model is defined. Note that this does not need to be the same as the number of concentration observations in the analysis.</p> <code>n_location</code> <code>int</code> <p>number of spatial knots in the model.</p> <code>time</code> <code>DatetimeArray</code> <p>vector of times used in defining the model.</p> <code>spatial_dependence</code> <code>bool</code> <p>flag indicating whether the background parameters should be spatially correlated. If True, the model assumes a separate background time-series per sensor location, and assumes these time-series to be spatially correlated. If False (default), the background parameters are assumed to be common between sensors (only temporally correlated).</p> <code>spatial_correlation_param</code> <code>float</code> <p>correlation length parameter, determining the degree of spatial correlation imposed on the background time-series. Units are metres. Assumes equal correlation in all spatial directions. Defaults to 1.0.</p> <code>location</code> <code>ndarray</code> <p>[n_location x 3] array of sensor locations, used for calculating the spatial correlation between the sensor background values. If self.spatial_dependence is False, this attribute is simply set to be the location of the first sensor in the sensor object.</p> <code>temporal_precision_matrix</code> <code>Union[ndarray, csc_matrix]</code> <p>temporal component of the precision matrix. The full model precision matrix is the Kronecker product of this matrix with self.spatial_precision_matrix.</p> <code>spatial_precision_matrix</code> <code>ndarray</code> <p>spatial component of the precision matrix. The full model precision matrix is the Kronecker product of this matrix with the self.temporal_precision_matrix. Simply set to 1 if self.spatial_dependence is False.</p> <code>precision_time_0</code> <code>float</code> <p>precision relating to the first time stamp in the model. Defaults to 0.01.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>@dataclass\nclass SpatioTemporalBackground(Background):\n    \"\"\"Model which imposes both spatial and temporal correlation on the background parameters.\n\n    Defines a grid in time, and assumes a correlated time-series per sensor using the defined time grid.\n\n    The background parameter is an [n_location * n_time x 1] (if self.spatial_dependence is True) or an [n_time x 1]\n    vector (if self.spatial_dependence is False). In the spatio-temporal case, the background vector is assumed to\n    unwrap over space and time as follows:\n    bg = [b_1(t_1), b_2(t_1),..., b_nlct(t_1),...,b_1(t_k),..., b_nlct(t_k),...].T\n    where nlct is the number of sensor locations.\n    This unwrapping mechanism is chosen as it greatly speeds up the sparse matrix operations in the solver (vs. the\n    alternative).\n\n    self.basis_matrix is set up to map the elements of the full background vector onto the observations, on the basis\n    of spatial location and nearest time knot.\n\n    The temporal background correlation is computed using gmrf.precision_temporal, and the spatial correlation is\n    computed using a squared exponential correlation function, parametrized by self.spatial_correlation_param (spatial\n    correlation, measured in metres). The full precision matrix is simply a Kronecker product between the two\n    component precision matrices.\n\n    Attributes:\n        n_time (int): number of time knots for which the model is defined. Note that this does not need to be the same\n            as the number of concentration observations in the analysis.\n        n_location (int): number of spatial knots in the model.\n        time (pd.arrays.DatetimeArray): vector of times used in defining the model.\n        spatial_dependence (bool): flag indicating whether the background parameters should be spatially correlated. If\n            True, the model assumes a separate background time-series per sensor location, and assumes these\n            time-series to be spatially correlated. If False (default), the background parameters are assumed to be\n            common between sensors (only temporally correlated).\n        spatial_correlation_param (float): correlation length parameter, determining the degree of spatial correlation\n            imposed on the background time-series. Units are metres. Assumes equal correlation in all spatial\n            directions. Defaults to 1.0.\n        location (np.ndarray): [n_location x 3] array of sensor locations, used for calculating the spatial correlation\n            between the sensor background values. If self.spatial_dependence is False, this attribute is simply set to\n            be the location of the first sensor in the sensor object.\n        temporal_precision_matrix (Union[np.ndarray, sparse.csc_matrix]): temporal component of the precision matrix.\n            The full model precision matrix is the Kronecker product of this matrix with self.spatial_precision_matrix.\n        spatial_precision_matrix (np.ndarray): spatial component of the precision matrix. The full model precision\n            matrix is the Kronecker product of this matrix with the self.temporal_precision_matrix. Simply set to 1 if\n            self.spatial_dependence is False.\n        precision_time_0 (float): precision relating to the first time stamp in the model. Defaults to 0.01.\n\n    \"\"\"\n\n    n_time: Union[int, None] = None\n    n_location: int = field(init=False)\n    time: pd.arrays.DatetimeArray = field(init=False)\n    spatial_dependence: bool = False\n    spatial_correlation_param: float = field(init=False, default=1.0)\n    location: Coordinate = field(init=False)\n    temporal_precision_matrix: Union[np.ndarray, sparse.csc_matrix] = field(init=False)\n    spatial_precision_matrix: np.ndarray = field(init=False)\n    precision_time_0: float = field(init=False, default=0.01)\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data wind data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n        self.make_temporal_knots(sensor_object)\n        self.make_spatial_knots(sensor_object)\n        self.n_parameter = self.n_time * self.n_location\n        self.n_obs = sensor_object.nof_observations\n\n        self.make_precision_matrix()\n        self.make_parameter_mapping(sensor_object)\n\n        if self.mean_bg is None:\n            self.mean_bg = gas_species.global_background\n\n    def make_parameter_mapping(self, sensor_object: SensorGroup):\n        \"\"\"Create the mapping of parameters onto observations, through creation of the associated basis matrix.\n\n        The background vector unwraps first over the spatial (sensor) location dimension, then over the temporal\n        dimension. For more detail, see the main class docstring.\n\n        The data vector in the solver state is assumed to consist of the individual sensor data vectors stacked\n        consecutively.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        nn_object = NearestNeighbors(n_neighbors=1, algorithm=\"kd_tree\").fit(self.time.to_numpy().reshape(-1, 1))\n        for k, sensor in enumerate(sensor_object.values()):\n            _, time_index = nn_object.kneighbors(sensor.time.to_numpy().reshape(-1, 1))\n            basis_matrix = sparse.csr_array(\n                (np.ones(sensor.nof_observations), (np.array(range(sensor.nof_observations)), time_index.flatten())),\n                shape=(sensor.nof_observations, self.n_time),\n            )\n            if self.spatial_dependence:\n                basis_matrix = sparse.kron(basis_matrix, np.eye(N=self.n_location, M=1, k=-k).T)\n\n            if k == 0:\n                self.basis_matrix = basis_matrix\n            else:\n                self.basis_matrix = sparse.vstack([self.basis_matrix, basis_matrix])\n\n    def make_temporal_knots(self, sensor_object: SensorGroup):\n        \"\"\"Create the temporal grid for the model.\n\n        If self.n_time is not specified, then the model will use the unique set of times from the sensor data.\n\n        If self.n_time is specified, then the model will define a time grid with self.n_time elements.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        if self.n_time is None:\n            self.time = pd.array(np.unique(sensor_object.time), dtype=\"datetime64[ns]\")\n            self.n_time = len(self.time)\n        else:\n            self.time = pd.array(\n                pd.date_range(start=np.min(sensor_object.time), end=np.max(sensor_object.time), periods=self.n_time),\n                dtype=\"datetime64[ns]\",\n            )\n\n    def make_spatial_knots(self, sensor_object: SensorGroup):\n        \"\"\"Create the spatial grid for the model.\n\n        If self.spatial_dependence is False, the code assumes that only a single (arbitrary) location is used, thereby\n        eliminating any spatial dependence.\n\n        If self.spatial_dependence is True, a separate but correlated time-series of background parameters is assumed\n        for each sensor location.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        if self.spatial_dependence:\n            self.n_location = sensor_object.nof_sensors\n            self.get_locations_from_sensors(sensor_object)\n        else:\n            self.n_location = 1\n            self.location = sensor_object[list(sensor_object.keys())[0]].location\n\n    def make_precision_matrix(self):\n        \"\"\"Create the full precision matrix for the background parameters.\n\n        Defined as the Kronecker product of the temporal precision matrix and the spatial precision matrix.\n\n        \"\"\"\n        self.temporal_precision_matrix = gmrf.precision_temporal(time=self.time)\n        lam = self.temporal_precision_matrix[0, 0]\n        self.temporal_precision_matrix[0, 0] = lam * (2.0 - lam / (self.precision_time_0 + lam))\n\n        if self.spatial_dependence:\n            self.make_spatial_precision_matrix()\n            self.precision_matrix = sparse.kron(self.temporal_precision_matrix, self.spatial_precision_matrix)\n        else:\n            self.precision_matrix = self.temporal_precision_matrix\n        if (self.n_parameter == 1) and sparse.issparse(self.precision_matrix):\n            self.precision_matrix = self.precision_matrix.toarray()\n\n    def make_spatial_precision_matrix(self):\n        \"\"\"Create the spatial precision matrix for the model.\n\n        The spatial precision matrix is simply calculated as the inverse of a squared exponential covariance matrix\n        calculated using the sensor locations.\n\n        \"\"\"\n        location_array = self.location.to_array()\n        spatial_covariance_matrix = np.exp(\n            -(1 / (2 * np.power(self.spatial_correlation_param, 2)))\n            * (\n                np.power(location_array[:, [0]] - location_array[:, [0]].T, 2)\n                + np.power(location_array[:, [1]] - location_array[:, [1]].T, 2)\n                + np.power(location_array[:, [2]] - location_array[:, [2]].T, 2)\n            )\n        )\n        self.spatial_precision_matrix = np.linalg.inv(\n            spatial_covariance_matrix + (1e-6) * np.eye(spatial_covariance_matrix.shape[0])\n        )\n\n    def get_locations_from_sensors(self, sensor_object: SensorGroup):\n        \"\"\"Extract the location information from the sensor object.\n\n        Attaches a Coordinate.ENU object as the self.location attribute, with all the sensor locations stored on the\n        same object.\n\n        Args:\n            sensor_object (SensorGroup): group of sensor objects.\n\n        \"\"\"\n        self.location = deepcopy(sensor_object[list(sensor_object.keys())[0]].location.to_enu())\n        self.location.east = np.full(shape=(self.n_location,), fill_value=np.nan)\n        self.location.north = np.full(shape=(self.n_location,), fill_value=np.nan)\n        self.location.up = np.full(shape=(self.n_location,), fill_value=np.nan)\n        for k, sensor in enumerate(sensor_object.values()):\n            if isinstance(sensor, Beam):\n                self.location.east[k] = np.mean(sensor.location.to_enu().east, axis=0)\n                self.location.north[k] = np.mean(sensor.location.to_enu().north, axis=0)\n                self.location.up[k] = np.mean(sensor.location.to_enu().up, axis=0)\n            else:\n                self.location.east[k] = sensor.location.to_enu().east\n                self.location.north[k] = sensor.location.to_enu().north\n                self.location.up[k] = sensor.location.to_enu().up\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data wind data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data wind data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n    self.make_temporal_knots(sensor_object)\n    self.make_spatial_knots(sensor_object)\n    self.n_parameter = self.n_time * self.n_location\n    self.n_obs = sensor_object.nof_observations\n\n    self.make_precision_matrix()\n    self.make_parameter_mapping(sensor_object)\n\n    if self.mean_bg is None:\n        self.mean_bg = gas_species.global_background\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_parameter_mapping","title":"<code>make_parameter_mapping(sensor_object)</code>","text":"<p>Create the mapping of parameters onto observations, through creation of the associated basis matrix.</p> <p>The background vector unwraps first over the spatial (sensor) location dimension, then over the temporal dimension. For more detail, see the main class docstring.</p> <p>The data vector in the solver state is assumed to consist of the individual sensor data vectors stacked consecutively.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_parameter_mapping(self, sensor_object: SensorGroup):\n    \"\"\"Create the mapping of parameters onto observations, through creation of the associated basis matrix.\n\n    The background vector unwraps first over the spatial (sensor) location dimension, then over the temporal\n    dimension. For more detail, see the main class docstring.\n\n    The data vector in the solver state is assumed to consist of the individual sensor data vectors stacked\n    consecutively.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    nn_object = NearestNeighbors(n_neighbors=1, algorithm=\"kd_tree\").fit(self.time.to_numpy().reshape(-1, 1))\n    for k, sensor in enumerate(sensor_object.values()):\n        _, time_index = nn_object.kneighbors(sensor.time.to_numpy().reshape(-1, 1))\n        basis_matrix = sparse.csr_array(\n            (np.ones(sensor.nof_observations), (np.array(range(sensor.nof_observations)), time_index.flatten())),\n            shape=(sensor.nof_observations, self.n_time),\n        )\n        if self.spatial_dependence:\n            basis_matrix = sparse.kron(basis_matrix, np.eye(N=self.n_location, M=1, k=-k).T)\n\n        if k == 0:\n            self.basis_matrix = basis_matrix\n        else:\n            self.basis_matrix = sparse.vstack([self.basis_matrix, basis_matrix])\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_temporal_knots","title":"<code>make_temporal_knots(sensor_object)</code>","text":"<p>Create the temporal grid for the model.</p> <p>If self.n_time is not specified, then the model will use the unique set of times from the sensor data.</p> <p>If self.n_time is specified, then the model will define a time grid with self.n_time elements.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_temporal_knots(self, sensor_object: SensorGroup):\n    \"\"\"Create the temporal grid for the model.\n\n    If self.n_time is not specified, then the model will use the unique set of times from the sensor data.\n\n    If self.n_time is specified, then the model will define a time grid with self.n_time elements.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    if self.n_time is None:\n        self.time = pd.array(np.unique(sensor_object.time), dtype=\"datetime64[ns]\")\n        self.n_time = len(self.time)\n    else:\n        self.time = pd.array(\n            pd.date_range(start=np.min(sensor_object.time), end=np.max(sensor_object.time), periods=self.n_time),\n            dtype=\"datetime64[ns]\",\n        )\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_spatial_knots","title":"<code>make_spatial_knots(sensor_object)</code>","text":"<p>Create the spatial grid for the model.</p> <p>If self.spatial_dependence is False, the code assumes that only a single (arbitrary) location is used, thereby eliminating any spatial dependence.</p> <p>If self.spatial_dependence is True, a separate but correlated time-series of background parameters is assumed for each sensor location.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_spatial_knots(self, sensor_object: SensorGroup):\n    \"\"\"Create the spatial grid for the model.\n\n    If self.spatial_dependence is False, the code assumes that only a single (arbitrary) location is used, thereby\n    eliminating any spatial dependence.\n\n    If self.spatial_dependence is True, a separate but correlated time-series of background parameters is assumed\n    for each sensor location.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    if self.spatial_dependence:\n        self.n_location = sensor_object.nof_sensors\n        self.get_locations_from_sensors(sensor_object)\n    else:\n        self.n_location = 1\n        self.location = sensor_object[list(sensor_object.keys())[0]].location\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_precision_matrix","title":"<code>make_precision_matrix()</code>","text":"<p>Create the full precision matrix for the background parameters.</p> <p>Defined as the Kronecker product of the temporal precision matrix and the spatial precision matrix.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_precision_matrix(self):\n    \"\"\"Create the full precision matrix for the background parameters.\n\n    Defined as the Kronecker product of the temporal precision matrix and the spatial precision matrix.\n\n    \"\"\"\n    self.temporal_precision_matrix = gmrf.precision_temporal(time=self.time)\n    lam = self.temporal_precision_matrix[0, 0]\n    self.temporal_precision_matrix[0, 0] = lam * (2.0 - lam / (self.precision_time_0 + lam))\n\n    if self.spatial_dependence:\n        self.make_spatial_precision_matrix()\n        self.precision_matrix = sparse.kron(self.temporal_precision_matrix, self.spatial_precision_matrix)\n    else:\n        self.precision_matrix = self.temporal_precision_matrix\n    if (self.n_parameter == 1) and sparse.issparse(self.precision_matrix):\n        self.precision_matrix = self.precision_matrix.toarray()\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.make_spatial_precision_matrix","title":"<code>make_spatial_precision_matrix()</code>","text":"<p>Create the spatial precision matrix for the model.</p> <p>The spatial precision matrix is simply calculated as the inverse of a squared exponential covariance matrix calculated using the sensor locations.</p> Source code in <code>src/pyelq/component/background.py</code> <pre><code>def make_spatial_precision_matrix(self):\n    \"\"\"Create the spatial precision matrix for the model.\n\n    The spatial precision matrix is simply calculated as the inverse of a squared exponential covariance matrix\n    calculated using the sensor locations.\n\n    \"\"\"\n    location_array = self.location.to_array()\n    spatial_covariance_matrix = np.exp(\n        -(1 / (2 * np.power(self.spatial_correlation_param, 2)))\n        * (\n            np.power(location_array[:, [0]] - location_array[:, [0]].T, 2)\n            + np.power(location_array[:, [1]] - location_array[:, [1]].T, 2)\n            + np.power(location_array[:, [2]] - location_array[:, [2]].T, 2)\n        )\n    )\n    self.spatial_precision_matrix = np.linalg.inv(\n        spatial_covariance_matrix + (1e-6) * np.eye(spatial_covariance_matrix.shape[0])\n    )\n</code></pre>"},{"location":"pyelq/component/background/#pyelq.component.background.SpatioTemporalBackground.get_locations_from_sensors","title":"<code>get_locations_from_sensors(sensor_object)</code>","text":"<p>Extract the location information from the sensor object.</p> <p>Attaches a Coordinate.ENU object as the self.location attribute, with all the sensor locations stored on the same object.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>group of sensor objects.</p> required Source code in <code>src/pyelq/component/background.py</code> <pre><code>def get_locations_from_sensors(self, sensor_object: SensorGroup):\n    \"\"\"Extract the location information from the sensor object.\n\n    Attaches a Coordinate.ENU object as the self.location attribute, with all the sensor locations stored on the\n    same object.\n\n    Args:\n        sensor_object (SensorGroup): group of sensor objects.\n\n    \"\"\"\n    self.location = deepcopy(sensor_object[list(sensor_object.keys())[0]].location.to_enu())\n    self.location.east = np.full(shape=(self.n_location,), fill_value=np.nan)\n    self.location.north = np.full(shape=(self.n_location,), fill_value=np.nan)\n    self.location.up = np.full(shape=(self.n_location,), fill_value=np.nan)\n    for k, sensor in enumerate(sensor_object.values()):\n        if isinstance(sensor, Beam):\n            self.location.east[k] = np.mean(sensor.location.to_enu().east, axis=0)\n            self.location.north[k] = np.mean(sensor.location.to_enu().north, axis=0)\n            self.location.up[k] = np.mean(sensor.location.to_enu().up, axis=0)\n        else:\n            self.location.east[k] = sensor.location.to_enu().east\n            self.location.north[k] = sensor.location.to_enu().north\n            self.location.up[k] = sensor.location.to_enu().up\n</code></pre>"},{"location":"pyelq/component/component/","title":"Overview","text":""},{"location":"pyelq/component/component/#component-classes","title":"Component classes","text":"<p>An overview of the component classes:</p> <ul> <li> <p>Background Model</p> </li> <li> <p>Error Model</p> </li> <li> <p>Offset</p> </li> <li> <p>Source Model</p> </li> </ul>"},{"location":"pyelq/component/component/#component-superclass","title":"Component superclass","text":"<p>Superclass for model components.</p>"},{"location":"pyelq/component/component/#pyelq.component.component.Component","title":"<code>Component</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class defining methods and rules for model elements.</p> <p>The bulk of attributes will be defined in the subclasses inheriting from this superclass.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@dataclass\nclass Component(ABC):\n    \"\"\"Abstract class defining methods and rules for model elements.\n\n    The bulk of attributes will be defined in the subclasses inheriting from this superclass.\n\n    \"\"\"\n\n    @abstractmethod\n    def initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n\n    @abstractmethod\n    def make_model(self, model: list) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to [].\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_state(self, state: dict) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to {}.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n\n    @abstractmethod\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>  <code>abstractmethod</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef initialise(self, sensor_object: SensorGroup, meteorology: MeteorologyGroup, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.make_model","title":"<code>make_model(model)</code>  <code>abstractmethod</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to [].</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef make_model(self, model: list) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to [].\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.make_sampler","title":"<code>make_sampler(model, sampler_list)</code>  <code>abstractmethod</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to [].</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.make_state","title":"<code>make_state(state)</code>  <code>abstractmethod</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to {}.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef make_state(self, state: dict) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to {}.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/component/#pyelq.component.component.Component.from_mcmc","title":"<code>from_mcmc(store)</code>  <code>abstractmethod</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/component.py</code> <pre><code>@abstractmethod\ndef from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/error_model/","title":"Error Model","text":""},{"location":"pyelq/component/error_model/#error-model","title":"Error Model","text":"<p>Error model module.</p>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel","title":"<code>ErrorModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code></p> <p>Measurement precision model component for the model.</p> <p>Attributes:</p> Name Type Description <code>n_sensor</code> <code>int</code> <p>number of sensors in the sensor object used for analysis.</p> <code>precision_index</code> <code>ndarray</code> <p>index mapping precision parameters onto observations. Will be set up differently for different model types.</p> <code>precision_parameter</code> <code>Parameter</code> <p>parameter object which constructs the full measurement error precision matrix from the components stored in state. Will be passed to the distribution for the observed when the full model is constructed.</p> <code>prior_precision_shape</code> <code>Union[ndarray, float]</code> <p>prior shape parameters for the precision model. Set up differently per model type.</p> <code>prior_precision_rate</code> <code>Union[ndarray, float]</code> <p>prior rate parameters for the precision model. Set up differently per model type.</p> <code>initial_precision</code> <code>Union[ndarray, float]</code> <p>initial value for the precision to be passed to the analysis routine. Set up differently per model type.</p> <code>precision</code> <code>ndarray</code> <p>array of sampled measurement error precision values, populated in self.from_mcmc() after the MCMC run is completed.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>@dataclass\nclass ErrorModel(Component):\n    \"\"\"Measurement precision model component for the model.\n\n    Attributes:\n        n_sensor (int): number of sensors in the sensor object used for analysis.\n        precision_index (np.ndarray): index mapping precision parameters onto observations. Will be set up differently\n            for different model types.\n        precision_parameter (parameter.Parameter): parameter object which constructs the full measurement error\n            precision matrix from the components stored in state. Will be passed to the distribution for the observed\n            when the full model is constructed.\n        prior_precision_shape (Union[np.ndarray, float]): prior shape parameters for the precision model. Set up\n            differently per model type.\n        prior_precision_rate (Union[np.ndarray, float]): prior rate parameters for the precision model. Set up\n            differently per model type.\n        initial_precision (Union[np.ndarray, float]): initial value for the precision to be passed to the analysis\n            routine. Set up differently per model type.\n        precision (np.ndarray): array of sampled measurement error precision values, populated in self.from_mcmc() after\n            the MCMC run is completed.\n\n    \"\"\"\n\n    n_sensor: int = field(init=False)\n    precision_index: np.ndarray = field(init=False)\n    precision_parameter: parameter.Parameter = field(init=False)\n    prior_precision_shape: Union[np.ndarray, float] = field(init=False)\n    prior_precision_rate: Union[np.ndarray, float] = field(init=False)\n    initial_precision: Union[np.ndarray, float] = field(init=False)\n    precision: np.ndarray = field(init=False)\n\n    def initialise(\n        self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n    ):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n            gas_species (GasSpecies): gas species information. Defaults to None.\n\n        \"\"\"\n        self.n_sensor = sensor_object.nof_sensors\n\n    def make_model(self, model: list = None) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to None.\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n        if model is None:\n            model = []\n        model.append(Gamma(\"tau\", shape=\"a_tau\", rate=\"b_tau\"))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalGamma(\"tau\", model))\n        return sampler_list\n\n    def make_state(self, state: dict = None) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to None.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n        if state is None:\n            state = {}\n        state[\"a_tau\"] = self.prior_precision_shape.flatten()\n        state[\"b_tau\"] = self.prior_precision_rate.flatten()\n        state[\"precision_index\"] = self.precision_index\n        state[\"tau\"] = self.initial_precision.flatten()\n        return state\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.precision = store[\"tau\"]\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.initialise","title":"<code>initialise(sensor_object, meteorology=None, gas_species=None)</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data. Defaults to None.</p> <code>None</code> <code>gas_species</code> <code>GasSpecies</code> <p>gas species information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def initialise(\n    self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n        gas_species (GasSpecies): gas species information. Defaults to None.\n\n    \"\"\"\n    self.n_sensor = sensor_object.nof_sensors\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.make_model","title":"<code>make_model(model=None)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def make_model(self, model: list = None) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to None.\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n    if model is None:\n        model = []\n    model.append(Gamma(\"tau\", shape=\"a_tau\", rate=\"b_tau\"))\n    return model\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.make_sampler","title":"<code>make_sampler(model, sampler_list=None)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to None.\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalGamma(\"tau\", model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.make_state","title":"<code>make_state(state=None)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def make_state(self, state: dict = None) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to None.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n    if state is None:\n        state = {}\n    state[\"a_tau\"] = self.prior_precision_shape.flatten()\n    state[\"b_tau\"] = self.prior_precision_rate.flatten()\n    state[\"precision_index\"] = self.precision_index\n    state[\"tau\"] = self.initial_precision.flatten()\n    return state\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ErrorModel.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.precision = store[\"tau\"]\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor","title":"<code>BySensor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ErrorModel</code></p> <p>Version of measurement precision where each sensor object has a different precision.</p> <p>Attributes:</p> Name Type Description <code>prior_precision_shape</code> <code>Union[ndarray, float]</code> <p>prior shape parameters for the precision model, can be specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same parameter value for each sensor. Defaults to 1e-3.</p> <code>prior_precision_rate</code> <code>Union[ndarray, float]</code> <p>prior rate parameters for the precision model, can be specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same parameter value for each sensor. Defaults to 1e-3.</p> <code>initial_precision</code> <code>Union[ndarray, float]</code> <p>initial value for the precision parameters, can be specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same parameter value for each sensor. Defaults to 1.</p> <code>precision_index</code> <code>ndarray</code> <p>index mapping precision parameters onto observations. Parameters 1:n_sensor are mapped as the measurement error precisions of the corresponding sensors.</p> <code>precision_parameter</code> <code>MixtureParameterMatrix</code> <p>parameter specification for this model, maps the current value of the parameter in the state dict onto the concentration data precisions.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>@dataclass\nclass BySensor(ErrorModel):\n    \"\"\"Version of measurement precision where each sensor object has a different precision.\n\n    Attributes:\n        prior_precision_shape (Union[np.ndarray, float]): prior shape parameters for the precision model, can be\n            specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in\n            the same parameter value for each sensor. Defaults to 1e-3.\n        prior_precision_rate (Union[np.ndarray, float]): prior rate parameters for the precision model, can be\n            specified either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in\n            the same parameter value for each sensor. Defaults to 1e-3.\n        initial_precision (Union[np.ndarray, float]): initial value for the precision parameters, can be specified\n            either as a float or as a (nof_sensors, ) np.ndarray: a float specification will result in the same\n            parameter value for each sensor. Defaults to 1.\n        precision_index (np.ndarray): index mapping precision parameters onto observations. Parameters 1:n_sensor are\n            mapped as the measurement error precisions of the corresponding sensors.\n        precision_parameter (Parameter.MixtureParameterMatrix): parameter specification for this model, maps the\n            current value of the parameter in the state dict onto the concentration data precisions.\n\n    \"\"\"\n\n    prior_precision_shape: Union[np.ndarray, float] = 1e-3\n    prior_precision_rate: Union[np.ndarray, float] = 1e-3\n    initial_precision: Union[np.ndarray, float] = 1.0\n\n    def initialise(\n        self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n    ):\n        \"\"\"Set up the error model using sensor properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n            gas_species (GasSpecies): gas species information. Defaults to None.\n\n        \"\"\"\n        super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n        self.prior_precision_shape = self.prior_precision_shape * np.ones((self.n_sensor,))\n        self.prior_precision_rate = self.prior_precision_rate * np.ones((self.n_sensor,))\n        self.initial_precision = self.initial_precision * np.ones((self.n_sensor,))\n        self.precision_index = sensor_object.sensor_index\n        self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n\n    def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the error model values for every sensor with respect to the MCMC iterations.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_iterations'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n        )\n\n        return plot\n\n    def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the distribution of the error model values after the burn in for every sensor.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_distributions'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n        )\n\n        return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor.initialise","title":"<code>initialise(sensor_object, meteorology=None, gas_species=None)</code>","text":"<p>Set up the error model using sensor properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data. Defaults to None.</p> <code>None</code> <code>gas_species</code> <code>GasSpecies</code> <p>gas species information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def initialise(\n    self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n):\n    \"\"\"Set up the error model using sensor properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n        gas_species (GasSpecies): gas species information. Defaults to None.\n\n    \"\"\"\n    super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n    self.prior_precision_shape = self.prior_precision_shape * np.ones((self.n_sensor,))\n    self.prior_precision_rate = self.prior_precision_rate * np.ones((self.n_sensor,))\n    self.initial_precision = self.initial_precision * np.ones((self.n_sensor,))\n    self.precision_index = sensor_object.sensor_index\n    self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor.plot_iterations","title":"<code>plot_iterations(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the error model values for every sensor with respect to the MCMC iterations.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_iterations'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the error model values for every sensor with respect to the MCMC iterations.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_iterations'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.BySensor.plot_distributions","title":"<code>plot_distributions(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the distribution of the error model values after the burn in for every sensor.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_distributions'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the distribution of the error model values after the burn in for every sensor.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_distributions'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease","title":"<code>ByRelease</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ErrorModel</code></p> <p>ByRelease error model, special case of the measurement precision model.</p> <p>Version of the measurement precision model where each sensor object has a different precision, and there are different precisions for periods inside and outside controlled release periods. For all parameters: the first element corresponds to the case where the sources are OFF; the second element corresponds to the case where the sources are ON.</p> <p>Attributes:</p> Name Type Description <code>prior_precision_shape</code> <code>ndarray</code> <p>prior shape parameters for the precision model, can be specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification will result in the same prior specification for the off/on precisions for each sensor. Defaults to np.array([1e-3, 1e-3]).</p> <code>prior_precision_rate</code> <code>ndarray</code> <p>prior rate parameters for the precision model, can be specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification will result in the same prior specification for the off/on precisions for each sensor. Defaults to np.array([1e-3, 1e-3]).</p> <code>initial_precision</code> <code>ndarray</code> <p>initial value for the precision parameters, can be specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification will result in the same prior specification for the off/on precisions for each sensor. Defaults to np.array([1.0, 1.0]).</p> <code>precision_index</code> <code>ndarray</code> <p>index mapping precision parameters onto observations. Parameters 1:n_sensor are mapped onto each sensor for the periods where the sources are OFF; parameters (n_sensor + 1):(2 * n_sensor) are mapped onto each sensor for the periods where the sources are ON.</p> <code>precision_parameter</code> <code>MixtureParameterMatrix</code> <p>parameter specification for this model, maps the current value of the parameter in the state dict onto the concentration data precisions.</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>@dataclass\nclass ByRelease(ErrorModel):\n    \"\"\"ByRelease error model, special case of the measurement precision model.\n\n    Version of the measurement precision model where each sensor object has a different precision, and there are\n    different precisions for periods inside and outside controlled release periods. For all parameters: the first\n    element corresponds to the case where the sources are OFF; the second element corresponds to the case where the\n    sources are ON.\n\n    Attributes:\n        prior_precision_shape (np.ndarray): prior shape parameters for the precision model, can be\n            specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification\n            will result in the same prior specification for the off/on precisions for each sensor. Defaults to\n            np.array([1e-3, 1e-3]).\n        prior_precision_rate (np.ndarray): prior rate parameters for the precision model, can be\n            specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification\n            will result in the same prior specification for the off/on precisions for each sensor. Defaults to\n            np.array([1e-3, 1e-3]).\n        initial_precision (np.ndarray): initial value for the precision parameters, can be\n            specified either as a (2, 1) np.ndarray or as a (2, nof_sensors) np.ndarray: the former specification\n            will result in the same prior specification for the off/on precisions for each sensor. Defaults to\n            np.array([1.0, 1.0]).\n        precision_index (np.ndarray): index mapping precision parameters onto observations. Parameters 1:n_sensor are\n            mapped onto each sensor for the periods where the sources are OFF; parameters (n_sensor + 1):(2 * n_sensor)\n            are mapped onto each sensor for the periods where the sources are ON.\n        precision_parameter (Parameter.MixtureParameterMatrix): parameter specification for this model, maps the\n            current value of the parameter in the state dict onto the concentration data precisions.\n\n    \"\"\"\n\n    prior_precision_shape: np.ndarray = field(default_factory=lambda: np.array([1e-3, 1e-3], ndmin=2).T)\n    prior_precision_rate: np.ndarray = field(default_factory=lambda: np.array([1e-3, 1e-3], ndmin=2).T)\n    initial_precision: np.ndarray = field(default_factory=lambda: np.array([1.0, 1.0], ndmin=2).T)\n\n    def initialise(\n        self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n    ):\n        \"\"\"Set up the error model using sensor properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data.\n            meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n            gas_species (GasSpecies): gas species information. Defaults to None.\n\n        \"\"\"\n        super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n        self.prior_precision_shape = self.prior_precision_shape * np.ones((2, self.n_sensor))\n        self.prior_precision_rate = self.prior_precision_rate * np.ones((2, self.n_sensor))\n        self.initial_precision = self.initial_precision * np.ones((2, self.n_sensor))\n        self.precision_index = sensor_object.sensor_index + sensor_object.source_on * self.n_sensor\n        self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n\n    def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plot the estimated error model parameters against iterations of the MCMC chain.\n\n        Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n        sources are on/off). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n        call to its plot function.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_iterations'\n\n        \"\"\"\n        figure_keys = [\"error_model_off_iterations\", \"error_model_on_iterations\"]\n        figure_titles = [\n            \"Estimated error parameter values: sources off\",\n            \"Estimated error parameter values: sources on\",\n        ]\n        precision_arrays = [\n            self.precision[: sensor_object.nof_sensors, :],\n            self.precision[sensor_object.nof_sensors :, :],\n        ]\n        for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n            error_model = BySensor()\n            error_model.precision = array\n            plot = error_model.plot_iterations(plot, sensor_object, burn_in_value)\n            plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_iterations\")\n            plot.figure_dict[key].update_layout(title=title)\n        return plot\n\n    def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plot the estimated distributions of error model parameters.\n\n        Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n        sources are off/on). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n        call to its plot function.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'error_model_distributions'\n\n        \"\"\"\n        figure_keys = [\"error_model_off_distributions\", \"error_model_on_distributions\"]\n        figure_titles = [\n            \"Estimated error parameter distribution: sources off\",\n            \"Estimated error parameter distribution: sources on\",\n        ]\n        precision_arrays = [\n            self.precision[: sensor_object.nof_sensors, :],\n            self.precision[sensor_object.nof_sensors :, :],\n        ]\n        for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n            error_model = BySensor()\n            error_model.precision = array\n            plot = error_model.plot_distributions(plot, sensor_object, burn_in_value)\n            plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_distributions\")\n            plot.figure_dict[key].update_layout(title=title)\n        return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease.initialise","title":"<code>initialise(sensor_object, meteorology=None, gas_species=None)</code>","text":"<p>Set up the error model using sensor properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data. Defaults to None.</p> <code>None</code> <code>gas_species</code> <code>GasSpecies</code> <p>gas species information. Defaults to None.</p> <code>None</code> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def initialise(\n    self, sensor_object: SensorGroup, meteorology: MeteorologyGroup = None, gas_species: GasSpecies = None\n):\n    \"\"\"Set up the error model using sensor properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data.\n        meteorology (MeteorologyGroup): meteorology data. Defaults to None.\n        gas_species (GasSpecies): gas species information. Defaults to None.\n\n    \"\"\"\n    super().initialise(sensor_object=sensor_object, meteorology=meteorology, gas_species=gas_species)\n    self.prior_precision_shape = self.prior_precision_shape * np.ones((2, self.n_sensor))\n    self.prior_precision_rate = self.prior_precision_rate * np.ones((2, self.n_sensor))\n    self.initial_precision = self.initial_precision * np.ones((2, self.n_sensor))\n    self.precision_index = sensor_object.sensor_index + sensor_object.source_on * self.n_sensor\n    self.precision_parameter = parameter.MixtureParameterMatrix(param=\"tau\", allocation=\"precision_index\")\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease.plot_iterations","title":"<code>plot_iterations(plot, sensor_object, burn_in_value)</code>","text":"<p>Plot the estimated error model parameters against iterations of the MCMC chain.</p> <p>Works by simply creating a separate plot for each of the two categories of precision parameter (when the sources are on/off). Creates a BySensor() object for each of the off/on precision cases, and then makes a call to its plot function.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_iterations'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plot the estimated error model parameters against iterations of the MCMC chain.\n\n    Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n    sources are on/off). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n    call to its plot function.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_iterations'\n\n    \"\"\"\n    figure_keys = [\"error_model_off_iterations\", \"error_model_on_iterations\"]\n    figure_titles = [\n        \"Estimated error parameter values: sources off\",\n        \"Estimated error parameter values: sources on\",\n    ]\n    precision_arrays = [\n        self.precision[: sensor_object.nof_sensors, :],\n        self.precision[sensor_object.nof_sensors :, :],\n    ]\n    for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n        error_model = BySensor()\n        error_model.precision = array\n        plot = error_model.plot_iterations(plot, sensor_object, burn_in_value)\n        plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_iterations\")\n        plot.figure_dict[key].update_layout(title=title)\n    return plot\n</code></pre>"},{"location":"pyelq/component/error_model/#pyelq.component.error_model.ByRelease.plot_distributions","title":"<code>plot_distributions(plot, sensor_object, burn_in_value)</code>","text":"<p>Plot the estimated distributions of error model parameters.</p> <p>Works by simply creating a separate plot for each of the two categories of precision parameter (when the sources are off/on). Creates a BySensor() object for each of the off/on precision cases, and then makes a call to its plot function.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the error_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'error_model_distributions'</p> Source code in <code>src/pyelq/component/error_model.py</code> <pre><code>def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plot the estimated distributions of error model parameters.\n\n    Works by simply creating a separate plot for each of the two categories of precision parameter (when the\n    sources are off/on). Creates a BySensor() object for each of the off/on precision cases, and then makes a\n    call to its plot function.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the error_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'error_model_distributions'\n\n    \"\"\"\n    figure_keys = [\"error_model_off_distributions\", \"error_model_on_distributions\"]\n    figure_titles = [\n        \"Estimated error parameter distribution: sources off\",\n        \"Estimated error parameter distribution: sources on\",\n    ]\n    precision_arrays = [\n        self.precision[: sensor_object.nof_sensors, :],\n        self.precision[sensor_object.nof_sensors :, :],\n    ]\n    for key, title, array in zip(figure_keys, figure_titles, precision_arrays):\n        error_model = BySensor()\n        error_model.precision = array\n        plot = error_model.plot_distributions(plot, sensor_object, burn_in_value)\n        plot.figure_dict[key] = plot.figure_dict.pop(\"error_model_distributions\")\n        plot.figure_dict[key].update_layout(title=title)\n    return plot\n</code></pre>"},{"location":"pyelq/component/offset/","title":"Offset","text":""},{"location":"pyelq/component/offset/#offset","title":"Offset","text":"<p>Offset module.</p>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor","title":"<code>PerSensor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code></p> <p>Offset implementation which assumes an additive offset between sensors.</p> <p>The offset is which is constant in space and time and accounts for calibration differences between sensors. To maintain parameter identifiability, the offset for the first sensor (with index 0) is assumed to be 0, and other sensor offsets are defined relative to this beam.</p> <p>Attributes:</p> Name Type Description <code>n_sensor</code> <code>int</code> <p>number of sensors in the sensor object used for analysis.</p> <code>offset</code> <code>ndarray</code> <p>array of sampled offset values, populated in self.from_mcmc() after the MCMC run is completed.</p> <code>precision_scalar</code> <code>ndarray</code> <p>array of sampled offset precision values, populated in self.from_mcmc() after the MCMC run is completed. Only populated if update_precision is True.</p> <code>indicator_basis</code> <code>csc_matrix</code> <p>[nof_observations x (nof_sensors - 1)] sparse matrix which assigns the offset parameters to the correct observations.</p> <code>update_precision</code> <code>bool</code> <p>logical indicating whether the offset prior precision parameter should be updated as part of the analysis.</p> <code>mean_offset</code> <code>float</code> <p>prior mean parameter for the offsets, assumed to be the same for each beam. Default is 0.</p> <code>prior_precision_shape</code> <code>float</code> <p>shape parameter for the prior gamma distribution for the scalar precision parameter. Default is 1e-3.</p> <code>prior_precision_rate</code> <code>float</code> <p>rate parameter for the prior gamma distribution for the scalar precision parameter(s). Default is 1e-3.</p> <code>initial_precision</code> <code>float</code> <p>initial value for the scalar precision parameter. Default is 1.0.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>@dataclass\nclass PerSensor(Component):\n    \"\"\"Offset implementation which assumes an additive offset between sensors.\n\n    The offset is which is constant in space and time and accounts for calibration differences between sensors.\n    To maintain parameter identifiability, the offset for the first sensor (with index 0) is assumed to be 0, and other\n    sensor offsets are defined relative to this beam.\n\n    Attributes:\n        n_sensor (int): number of sensors in the sensor object used for analysis.\n        offset (np.ndarray): array of sampled offset values, populated in self.from_mcmc() after the MCMC run is\n            completed.\n        precision_scalar (np.ndarray): array of sampled offset precision values, populated in self.from_mcmc() after\n            the MCMC run is completed. Only populated if update_precision is True.\n        indicator_basis (sparse.csc_matrix): [nof_observations x (nof_sensors - 1)] sparse matrix which assigns the\n            offset parameters to the correct observations.\n        update_precision (bool): logical indicating whether the offset prior precision parameter should be updated as\n            part of the analysis.\n        mean_offset (float): prior mean parameter for the offsets, assumed to be the same for each beam. Default is 0.\n        prior_precision_shape (float): shape parameter for the prior gamma distribution for the scalar precision\n            parameter. Default is 1e-3.\n        prior_precision_rate (float): rate parameter for the prior gamma distribution for the scalar precision\n            parameter(s). Default is 1e-3.\n        initial_precision (float): initial value for the scalar precision parameter. Default is 1.0.\n\n    \"\"\"\n\n    n_sensor: int = field(init=False)\n    offset: np.ndarray = field(init=False)\n    precision_scalar: np.ndarray = field(init=False)\n    indicator_basis: sparse.csc_matrix = field(init=False)\n    update_precision: bool = False\n    mean_offset: float = 0.0\n    prior_precision_shape: float = 1e-3\n    prior_precision_rate: float = 1e-3\n    initial_precision: float = 1.0\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n        \"\"\"Take data inputs and extract relevant properties.\n\n        Args:\n            sensor_object (SensorGroup): sensor data\n            meteorology (MeteorologyGroup): meteorology data wind data\n            gas_species (GasSpecies): gas species information\n\n        \"\"\"\n        self.n_sensor = len(sensor_object)\n        self.indicator_basis = sparse.csc_matrix(\n            np.equal(sensor_object.sensor_index[:, np.newaxis], np.array(range(1, self.n_sensor)))\n        )\n\n    def make_model(self, model: list = None) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list, optional): Current list of model elements. Defaults to [].\n\n        Returns:\n            list: model output list.\n\n        \"\"\"\n        if model is None:\n            model = []\n        off_precision_predictor = parameter.ScaledMatrix(matrix=\"P_d\", scalar=\"lambda_d\")\n        model.append(Normal(\"d\", mean=\"mu_d\", precision=off_precision_predictor))\n        if self.update_precision:\n            model.append(Gamma(\"lambda_d\", shape=\"a_lam_d\", rate=\"b_lam_d\"))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n        Returns:\n            list: sampler output list.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalNormal(\"d\", model))\n        if self.update_precision:\n            sampler_list.append(NormalGamma(\"lambda_d\", model))\n        return sampler_list\n\n    def make_state(self, state: dict = None) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict, optional): current state vector. Defaults to {}.\n\n        Returns:\n            dict: current state vector with components added.\n\n        \"\"\"\n        if state is None:\n            state = {}\n        state[\"mu_d\"] = np.ones((self.n_sensor - 1, 1)) * self.mean_offset\n        state[\"d\"] = np.zeros((self.n_sensor - 1, 1))\n        state[\"B_d\"] = self.indicator_basis\n        state[\"P_d\"] = sparse.eye(self.n_sensor - 1, format=\"csc\")\n        state[\"lambda_d\"] = self.initial_precision\n        if self.update_precision:\n            state[\"a_lam_d\"] = self.prior_precision_shape\n            state[\"b_lam_d\"] = self.prior_precision_rate\n        return state\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.offset = store[\"d\"]\n        if self.update_precision:\n            self.precision_scalar = store[\"lambda_d\"]\n\n    def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the offset values for every sensor with respect to the MCMC iterations.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n            burn_in_value (int): Burn in value to show in plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'offset_iterations'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n        )\n\n        return plot\n\n    def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n        \"\"\"Plots the distribution of the offset values after the burn in for every sensor.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n            burn_in_value (int): Burn in value to use for plot.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n        Returns:\n            plot (Plot): Plot object to which this figure is added in the figure dictionary with\n                key 'offset_distributions'\n\n        \"\"\"\n        plot.plot_trace_per_sensor(\n            object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n        )\n\n        return plot\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Take data inputs and extract relevant properties.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>sensor data</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>meteorology data wind data</p> required <code>gas_species</code> <code>GasSpecies</code> <p>gas species information</p> required Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n    \"\"\"Take data inputs and extract relevant properties.\n\n    Args:\n        sensor_object (SensorGroup): sensor data\n        meteorology (MeteorologyGroup): meteorology data wind data\n        gas_species (GasSpecies): gas species information\n\n    \"\"\"\n    self.n_sensor = len(sensor_object)\n    self.indicator_basis = sparse.csc_matrix(\n        np.equal(sensor_object.sensor_index[:, np.newaxis], np.array(range(1, self.n_sensor)))\n    )\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.make_model","title":"<code>make_model(model=None)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements. Defaults to [].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model output list.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def make_model(self, model: list = None) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list, optional): Current list of model elements. Defaults to [].\n\n    Returns:\n        list: model output list.\n\n    \"\"\"\n    if model is None:\n        model = []\n    off_precision_predictor = parameter.ScaledMatrix(matrix=\"P_d\", scalar=\"lambda_d\")\n    model.append(Normal(\"d\", mean=\"mu_d\", precision=off_precision_predictor))\n    if self.update_precision:\n        model.append(Gamma(\"lambda_d\", shape=\"a_lam_d\", rate=\"b_lam_d\"))\n    return model\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.make_sampler","title":"<code>make_sampler(model, sampler_list=None)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers. Defaults to [].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler output list.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list, optional): Current list of samplers. Defaults to [].\n\n    Returns:\n        list: sampler output list.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalNormal(\"d\", model))\n    if self.update_precision:\n        sampler_list.append(NormalGamma(\"lambda_d\", model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.make_state","title":"<code>make_state(state=None)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector. Defaults to {}.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with components added.</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def make_state(self, state: dict = None) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict, optional): current state vector. Defaults to {}.\n\n    Returns:\n        dict: current state vector with components added.\n\n    \"\"\"\n    if state is None:\n        state = {}\n    state[\"mu_d\"] = np.ones((self.n_sensor - 1, 1)) * self.mean_offset\n    state[\"d\"] = np.zeros((self.n_sensor - 1, 1))\n    state[\"B_d\"] = self.indicator_basis\n    state[\"P_d\"] = sparse.eye(self.n_sensor - 1, format=\"csc\")\n    state[\"lambda_d\"] = self.initial_precision\n    if self.update_precision:\n        state[\"a_lam_d\"] = self.prior_precision_shape\n        state[\"b_lam_d\"] = self.prior_precision_rate\n    return state\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.offset = store[\"d\"]\n    if self.update_precision:\n        self.precision_scalar = store[\"lambda_d\"]\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.plot_iterations","title":"<code>plot_iterations(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the offset values for every sensor with respect to the MCMC iterations.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the offset_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'offset_iterations'</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the offset values for every sensor with respect to the MCMC iterations.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n        burn_in_value (int): Burn in value to show in plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'offset_iterations'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"line\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/offset/#pyelq.component.offset.PerSensor.plot_distributions","title":"<code>plot_distributions(plot, sensor_object, burn_in_value)</code>","text":"<p>Plots the distribution of the offset values after the burn in for every sensor.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the offset_model</p> required <code>burn_in_value</code> <code>int</code> <p>Burn in value to use for plot.</p> required <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which this figure is added in the figure dictionary with key 'offset_distributions'</p> Source code in <code>src/pyelq/component/offset.py</code> <pre><code>def plot_distributions(self, plot: \"Plot\", sensor_object: Union[SensorGroup, Sensor], burn_in_value: int) -&gt; \"Plot\":\n    \"\"\"Plots the distribution of the offset values after the burn in for every sensor.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the offset_model\n        burn_in_value (int): Burn in value to use for plot.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary\n\n    Returns:\n        plot (Plot): Plot object to which this figure is added in the figure dictionary with\n            key 'offset_distributions'\n\n    \"\"\"\n    plot.plot_trace_per_sensor(\n        object_to_plot=self, sensor_object=sensor_object, plot_type=\"box\", burn_in=burn_in_value\n    )\n\n    return plot\n</code></pre>"},{"location":"pyelq/component/source_model/","title":"Source Model","text":""},{"location":"pyelq/component/source_model/#source-model","title":"Source Model","text":"<p>Component Class and subclasses for source model.</p> A SourceModel instance inherits from 3 super-classes <ul> <li>Component: this is the general superclass for ELQModel components, which prototypes generic methods.</li> <li>A type of SourceGrouping: this class type implements an allocation of sources to different categories (e.g. slab     or spike), and sets up a sampler for estimating the classification of each source within the source map.     Inheriting from the NullGrouping class ensures that the allocation of all sources is fixed during the inversion,     and is not updated.</li> <li>A type of SourceDistribution: this class type implements a particular type of response distribution (mostly     Normal, but also allows for cases where we have e.g. exp(log_s) or a non-Gaussian prior).</li> </ul>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.ParameterMapping","title":"<code>ParameterMapping</code>  <code>dataclass</code>","text":"<p>Class for defining mapping variable/parameter labels needed for creating an analysis.</p> <p>In instances where we want to include multiple source_model instances in an MCMC analysis, we can apply a suffix to all of the parameter names in the mapping dictionary. This allows us to create separate variables for different source map types, so that these can be associated with different sampler types in the MCMC analysis.</p> <p>Attributes:</p> Name Type Description <code>map</code> <code>dict</code> <p>dictionary containing mapping between variable types and MCMC parameters.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass ParameterMapping:\n    \"\"\"Class for defining mapping variable/parameter labels needed for creating an analysis.\n\n    In instances where we want to include multiple source_model instances in an MCMC analysis, we can apply a suffix to\n    all of the parameter names in the mapping dictionary. This allows us to create separate variables for different\n    source map types, so that these can be associated with different sampler types in the MCMC analysis.\n\n    Attributes:\n        map (dict): dictionary containing mapping between variable types and MCMC parameters.\n\n    \"\"\"\n\n    map: dict = field(\n        default_factory=lambda: {\n            \"source\": \"s\",\n            \"coupling_matrix\": \"A\",\n            \"emission_rate_mean\": \"mu_s\",\n            \"emission_rate_precision\": \"lambda_s\",\n            \"allocation\": \"alloc_s\",\n            \"source_prob\": \"s_prob\",\n            \"precision_prior_shape\": \"a_lam_s\",\n            \"precision_prior_rate\": \"b_lam_s\",\n            \"source_location\": \"z_src\",\n            \"number_sources\": \"n_src\",\n            \"number_source_rate\": \"rho\",\n        }\n    )\n\n    def append_string(self, string: str = None):\n        \"\"\"Apply the supplied string as a suffix to all of the values in the mapping dictionary.\n\n        For example: {'source': 's'} would become {'source': 's_fixed'} when string = 'fixed' is passed as the argument.\n        If string is None, nothing is appended.\n\n        Args:\n            string (str): string to append to the variable names.\n\n        \"\"\"\n        for key, value in self.map.items():\n            self.map[key] = value + \"_\" + string\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.ParameterMapping.append_string","title":"<code>append_string(string=None)</code>","text":"<p>Apply the supplied string as a suffix to all of the values in the mapping dictionary.</p> <p>For example: {'source': 's'} would become {'source': 's_fixed'} when string = 'fixed' is passed as the argument. If string is None, nothing is appended.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>string to append to the variable names.</p> <code>None</code> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def append_string(self, string: str = None):\n    \"\"\"Apply the supplied string as a suffix to all of the values in the mapping dictionary.\n\n    For example: {'source': 's'} would become {'source': 's_fixed'} when string = 'fixed' is passed as the argument.\n    If string is None, nothing is appended.\n\n    Args:\n        string (str): string to append to the variable names.\n\n    \"\"\"\n    for key, value in self.map.items():\n        self.map[key] = value + \"_\" + string\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping","title":"<code>SourceGrouping</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParameterMapping</code></p> <p>Superclass for source grouping approach.</p> <p>Source grouping method determines the group allocation of each source in the model, e.g: slab and spike distribution makes an on/off allocation for each source.</p> <p>Attributes:</p> Name Type Description <code>nof_sources</code> <code>int</code> <p>number of sources in the model.</p> <code>emission_rate_mean</code> <code>Union[float, ndarray]</code> <p>prior mean parameter for the emission rate distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SourceGrouping(ParameterMapping):\n    \"\"\"Superclass for source grouping approach.\n\n    Source grouping method determines the group allocation of each source in the model, e.g: slab and spike\n    distribution makes an on/off allocation for each source.\n\n    Attributes:\n        nof_sources (int): number of sources in the model.\n        emission_rate_mean (Union[float, np.ndarray]): prior mean parameter for the emission rate distribution.\n\n    \"\"\"\n\n    nof_sources: int = field(init=False)\n    emission_rate_mean: Union[float, np.ndarray] = field(init=False)\n\n    @abstractmethod\n    def make_allocation_model(self, model: list) -&gt; list:\n        \"\"\"Initialise the source allocation part of the model, and the parameters of the source response distribution.\n\n        Args:\n            model (list): overall model, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with allocation distribution.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the allocation part of the sampler.\n\n        Args:\n            model (Model): overall model, consisting of list of distributions.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the source allocation.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_allocation_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the allocation part of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source grouping.\n\n        \"\"\"\n\n    @abstractmethod\n    def from_mcmc_group(self, store: dict):\n        \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.make_allocation_model","title":"<code>make_allocation_model(model)</code>  <code>abstractmethod</code>","text":"<p>Initialise the source allocation part of the model, and the parameters of the source response distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>overall model, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with allocation distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_allocation_model(self, model: list) -&gt; list:\n    \"\"\"Initialise the source allocation part of the model, and the parameters of the source response distribution.\n\n    Args:\n        model (list): overall model, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with allocation distribution.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.make_allocation_sampler","title":"<code>make_allocation_sampler(model, sampler_list)</code>  <code>abstractmethod</code>","text":"<p>Initialise the allocation part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model, consisting of list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the source allocation.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the allocation part of the sampler.\n\n    Args:\n        model (Model): overall model, consisting of list of distributions.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the source allocation.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.make_allocation_state","title":"<code>make_allocation_state(state)</code>  <code>abstractmethod</code>","text":"<p>Initialise the allocation part of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source grouping.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_allocation_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the allocation part of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source grouping.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceGrouping.from_mcmc_group","title":"<code>from_mcmc_group(store)</code>  <code>abstractmethod</code>","text":"<p>Extract posterior allocation samples from the MCMC sampler, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef from_mcmc_group(self, store: dict):\n    \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping","title":"<code>NullGrouping</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceGrouping</code></p> <p>Null grouping: the grouping of the sources will not change during the course of the inversion.</p> Note that this is intended to support two distinct cases <p>1) The case where the source map is fixed, and a given prior mean and prior precision value are assigned to     each source (can be a common value for all sources, or can be a distinct allocation to each element of the     source map). 2) The case where the dimensionality of the source map is changing during the inversion, and a common prior     mean and precision term are used for all sources.</p> <p>number_on_sources (np.ndarray): number of sources switched on in the solution, per iteration. Extracted as a     property from the MCMC samples in self.from_mcmc_group().</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass NullGrouping(SourceGrouping):\n    \"\"\"Null grouping: the grouping of the sources will not change during the course of the inversion.\n\n    Note that this is intended to support two distinct cases:\n        1) The case where the source map is fixed, and a given prior mean and prior precision value are assigned to\n            each source (can be a common value for all sources, or can be a distinct allocation to each element of the\n            source map).\n        2) The case where the dimensionality of the source map is changing during the inversion, and a common prior\n            mean and precision term are used for all sources.\n\n    Attributes:\n    number_on_sources (np.ndarray): number of sources switched on in the solution, per iteration. Extracted as a\n        property from the MCMC samples in self.from_mcmc_group().\n\n    \"\"\"\n\n    number_on_sources: np.ndarray = field(init=False)\n\n    def make_allocation_model(self, model: list) -&gt; list:\n        \"\"\"Initialise the source allocation part of the model.\n\n        In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n        returns the existing model un-modified).\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with allocation distribution.\n\n        \"\"\"\n        return model\n\n    def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the allocation part of the sampler.\n\n        In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n        returns the existing sampler list un-modified).\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the source allocation.\n\n        \"\"\"\n        return sampler_list\n\n    def make_allocation_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the allocation part of the state.\n\n        The prior mean parameter and the fixed source allocation are added to the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source grouping.\n\n        \"\"\"\n        state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n        state[self.map[\"allocation\"]] = np.zeros((self.nof_sources, 1), dtype=\"int\")\n        return state\n\n    def from_mcmc_group(self, store: dict):\n        \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n        Gets the number of sources present in each iteration of the MCMC sampler, and attaches this as a class property.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n        \"\"\"\n        self.number_on_sources = np.count_nonzero(np.logical_not(np.isnan(store[self.map[\"source\"]])), axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.make_allocation_model","title":"<code>make_allocation_model(model)</code>","text":"<p>Initialise the source allocation part of the model.</p> <p>In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply returns the existing model un-modified).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with allocation distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_model(self, model: list) -&gt; list:\n    \"\"\"Initialise the source allocation part of the model.\n\n    In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n    returns the existing model un-modified).\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with allocation distribution.\n\n    \"\"\"\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.make_allocation_sampler","title":"<code>make_allocation_sampler(model, sampler_list)</code>","text":"<p>Initialise the allocation part of the sampler.</p> <p>In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply returns the existing sampler list un-modified).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the source allocation.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the allocation part of the sampler.\n\n    In the NullGrouping case, the source allocation is fixed throughout, so this function does nothing (simply\n    returns the existing sampler list un-modified).\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the source allocation.\n\n    \"\"\"\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.make_allocation_state","title":"<code>make_allocation_state(state)</code>","text":"<p>Initialise the allocation part of the state.</p> <p>The prior mean parameter and the fixed source allocation are added to the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source grouping.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the allocation part of the state.\n\n    The prior mean parameter and the fixed source allocation are added to the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source grouping.\n\n    \"\"\"\n    state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n    state[self.map[\"allocation\"]] = np.zeros((self.nof_sources, 1), dtype=\"int\")\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NullGrouping.from_mcmc_group","title":"<code>from_mcmc_group(store)</code>","text":"<p>Extract posterior allocation samples from the MCMC sampler, attach them to the class.</p> <p>Gets the number of sources present in each iteration of the MCMC sampler, and attaches this as a class property.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc_group(self, store: dict):\n    \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n    Gets the number of sources present in each iteration of the MCMC sampler, and attaches this as a class property.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n    \"\"\"\n    self.number_on_sources = np.count_nonzero(np.logical_not(np.isnan(store[self.map[\"source\"]])), axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike","title":"<code>SlabAndSpike</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceGrouping</code></p> <p>Slab and spike source model, special case for the source grouping.</p> <p>Slab and spike: the prior for the emission rates is a two-component mixture, and the allocation is to be estimated as part of the inversion.</p> <p>Attributes:</p> Name Type Description <code>slab_probability</code> <code>float</code> <p>prior probability of allocation to the slab component. Defaults to 0.05.</p> <code>allocation</code> <code>ndarray</code> <p>set of allocation samples, with shape=(n_sources, n_iterations). Attached to the class by self.from_mcmc_group().</p> <code>number_on_sources</code> <code>ndarray</code> <p>number of sources switched on in the solution, per iteration.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SlabAndSpike(SourceGrouping):\n    \"\"\"Slab and spike source model, special case for the source grouping.\n\n    Slab and spike: the prior for the emission rates is a two-component mixture, and the allocation is to be\n    estimated as part of the inversion.\n\n    Attributes:\n        slab_probability (float): prior probability of allocation to the slab component. Defaults to 0.05.\n        allocation (np.ndarray): set of allocation samples, with shape=(n_sources, n_iterations). Attached to\n            the class by self.from_mcmc_group().\n        number_on_sources (np.ndarray): number of sources switched on in the solution, per iteration.\n\n    \"\"\"\n\n    slab_probability: float = 0.05\n    allocation: np.ndarray = field(init=False)\n    number_on_sources: np.ndarray = field(init=False)\n\n    def make_allocation_model(self, model: list) -&gt; list:\n        \"\"\"Initialise the source allocation part of the model.\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with allocation distribution.\n\n        \"\"\"\n        model.append(Categorical(self.map[\"allocation\"], prob=self.map[\"source_prob\"]))\n        return model\n\n    def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the allocation part of the sampler.\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the source allocation.\n\n        \"\"\"\n        sampler_list.append(\n            MixtureAllocation(param=self.map[\"allocation\"], model=model, response_param=self.map[\"source\"])\n        )\n        return sampler_list\n\n    def make_allocation_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the allocation part of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source grouping.\n\n        \"\"\"\n        state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n        state[self.map[\"source_prob\"]] = np.tile(\n            np.array([self.slab_probability, 1 - self.slab_probability]), (self.nof_sources, 1)\n        )\n        state[self.map[\"allocation\"]] = np.ones((self.nof_sources, 1), dtype=\"int\")\n        return state\n\n    def from_mcmc_group(self, store: dict):\n        \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n        self.allocation = store[self.map[\"allocation\"]]\n        self.number_on_sources = self.allocation.shape[0] - np.sum(self.allocation, axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.make_allocation_model","title":"<code>make_allocation_model(model)</code>","text":"<p>Initialise the source allocation part of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with allocation distribution.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_model(self, model: list) -&gt; list:\n    \"\"\"Initialise the source allocation part of the model.\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with allocation distribution.\n\n    \"\"\"\n    model.append(Categorical(self.map[\"allocation\"], prob=self.map[\"source_prob\"]))\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.make_allocation_sampler","title":"<code>make_allocation_sampler(model, sampler_list)</code>","text":"<p>Initialise the allocation part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the source allocation.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the allocation part of the sampler.\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the source allocation.\n\n    \"\"\"\n    sampler_list.append(\n        MixtureAllocation(param=self.map[\"allocation\"], model=model, response_param=self.map[\"source\"])\n    )\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.make_allocation_state","title":"<code>make_allocation_state(state)</code>","text":"<p>Initialise the allocation part of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source grouping.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_allocation_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the allocation part of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source grouping.\n\n    \"\"\"\n    state[self.map[\"emission_rate_mean\"]] = np.array(self.emission_rate_mean, ndmin=1)\n    state[self.map[\"source_prob\"]] = np.tile(\n        np.array([self.slab_probability, 1 - self.slab_probability]), (self.nof_sources, 1)\n    )\n    state[self.map[\"allocation\"]] = np.ones((self.nof_sources, 1), dtype=\"int\")\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SlabAndSpike.from_mcmc_group","title":"<code>from_mcmc_group(store)</code>","text":"<p>Extract posterior allocation samples from the MCMC sampler, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc_group(self, store: dict):\n    \"\"\"Extract posterior allocation samples from the MCMC sampler, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n    self.allocation = store[self.map[\"allocation\"]]\n    self.number_on_sources = self.allocation.shape[0] - np.sum(self.allocation, axis=0)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution","title":"<code>SourceDistribution</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParameterMapping</code></p> <p>Superclass for source emission rate distribution.</p> <p>Source distribution determines the type of prior to be used for the source emission rates, and the transformation linking the source parameters and the data.</p> <p>Elements related to transformation of source parameters are also specified at the model level.</p> <p>Attributes:</p> Name Type Description <code>nof_sources</code> <code>int</code> <p>number of sources in the model.</p> <code>emission_rate</code> <code>ndarray</code> <p>set of emission rate samples, with shape=(n_sources, n_iterations). Attached to the class by self.from_mcmc_dist().</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SourceDistribution(ParameterMapping):\n    \"\"\"Superclass for source emission rate distribution.\n\n    Source distribution determines the type of prior to be used for the source emission rates, and the transformation\n    linking the source parameters and the data.\n\n    Elements related to transformation of source parameters are also specified at the model level.\n\n    Attributes:\n        nof_sources (int): number of sources in the model.\n        emission_rate (np.ndarray): set of emission rate samples, with shape=(n_sources, n_iterations). Attached to\n            the class by self.from_mcmc_dist().\n\n    \"\"\"\n\n    nof_sources: int = field(init=False)\n    emission_rate: np.ndarray = field(init=False)\n\n    @abstractmethod\n    def make_source_model(self, model: list) -&gt; list:\n        \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: overall model list, updated with distributions related to source prior.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_source_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Initialise the source prior distribution part of the sampler.\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the emission rate parameters.\n\n        \"\"\"\n\n    @abstractmethod\n    def make_source_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the emission rate parts of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with parameters related to the source emission rates.\n\n        \"\"\"\n\n    @abstractmethod\n    def from_mcmc_dist(self, store: dict):\n        \"\"\"Extract posterior emission rate samples from the MCMC, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.make_source_model","title":"<code>make_source_model(model)</code>  <code>abstractmethod</code>","text":"<p>Add distributional component to the overall model corresponding to the source emission rate distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>overall model list, updated with distributions related to source prior.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_source_model(self, model: list) -&gt; list:\n    \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: overall model list, updated with distributions related to source prior.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.make_source_sampler","title":"<code>make_source_sampler(model, sampler_list)</code>  <code>abstractmethod</code>","text":"<p>Initialise the source prior distribution part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the emission rate parameters.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_source_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Initialise the source prior distribution part of the sampler.\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the emission rate parameters.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.make_source_state","title":"<code>make_source_state(state)</code>  <code>abstractmethod</code>","text":"<p>Initialise the emission rate parts of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with parameters related to the source emission rates.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef make_source_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the emission rate parts of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with parameters related to the source emission rates.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceDistribution.from_mcmc_dist","title":"<code>from_mcmc_dist(store)</code>  <code>abstractmethod</code>","text":"<p>Extract posterior emission rate samples from the MCMC, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@abstractmethod\ndef from_mcmc_dist(self, store: dict):\n    \"\"\"Extract posterior emission rate samples from the MCMC, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse","title":"<code>NormalResponse</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceDistribution</code></p> <p>(Truncated) Gaussian prior for sources.</p> <p>No transformation applied to parameters, i.e.: - Prior distribution: s ~ N(mu, 1/precision) - Likelihood contribution: y = A*s + b + ...</p> <p>Attributes:</p> Name Type Description <code>truncation</code> <code>bool</code> <p>indication of whether the emission rate prior should be truncated at 0. Defaults to True.</p> <code>emission_rate_lb</code> <code>Union[float, ndarray]</code> <p>lower bound for the source emission rates. Defaults to 0.</p> <code>emission_rate_mean</code> <code>Union[float, ndarray]</code> <p>prior mean for the emission rate distribution. Defaults to 0.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass NormalResponse(SourceDistribution):\n    \"\"\"(Truncated) Gaussian prior for sources.\n\n    No transformation applied to parameters, i.e.:\n    - Prior distribution: s ~ N(mu, 1/precision)\n    - Likelihood contribution: y = A*s + b + ...\n\n    Attributes:\n        truncation (bool): indication of whether the emission rate prior should be truncated at 0. Defaults to True.\n        emission_rate_lb (Union[float, np.ndarray]): lower bound for the source emission rates. Defaults to 0.\n        emission_rate_mean (Union[float, np.ndarray]): prior mean for the emission rate distribution. Defaults to 0.\n\n    \"\"\"\n\n    truncation: bool = True\n    emission_rate_lb: Union[float, np.ndarray] = 0\n    emission_rate_mean: Union[float, np.ndarray] = 0\n\n    def make_source_model(self, model: list) -&gt; list:\n        \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n        Args:\n            model (list): model as constructed so far, consisting of list of distributions.\n\n        Returns:\n            list: model, updated with distributions related to source prior.\n\n        \"\"\"\n        domain_response_lower = None\n        if self.truncation:\n            domain_response_lower = self.emission_rate_lb\n\n        model.append(\n            mcmcNormal(\n                self.map[\"source\"],\n                mean=parameter.MixtureParameterVector(\n                    param=self.map[\"emission_rate_mean\"], allocation=self.map[\"allocation\"]\n                ),\n                precision=parameter.MixtureParameterMatrix(\n                    param=self.map[\"emission_rate_precision\"], allocation=self.map[\"allocation\"]\n                ),\n                domain_response_lower=domain_response_lower,\n            )\n        )\n        return model\n\n    def make_source_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n        \"\"\"Initialise the source prior distribution part of the sampler.\n\n        Args:\n            model (Model): overall model set for the problem.\n            sampler_list (list): list of samplers for individual parameters.\n\n        Returns:\n            list: sampler_list updated with sampler for the emission rate parameters.\n\n        \"\"\"\n        if sampler_list is None:\n            sampler_list = []\n        sampler_list.append(NormalNormal(self.map[\"source\"], model))\n        return sampler_list\n\n    def make_source_state(self, state: dict) -&gt; dict:\n        \"\"\"Initialise the emission rate part of the state.\n\n        Args:\n            state (dict): dictionary containing current state information.\n\n        Returns:\n            dict: state updated with initial emission rate vector.\n\n        \"\"\"\n        state[self.map[\"source\"]] = np.zeros((self.nof_sources, 1))\n        return state\n\n    def from_mcmc_dist(self, store: dict):\n        \"\"\"Extract posterior emission rate samples from the MCMC sampler, attach them to the class.\n\n        Args:\n            store (dict): dictionary containing samples from the MCMC.\n\n        \"\"\"\n        self.emission_rate = store[self.map[\"source\"]]\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.make_source_model","title":"<code>make_source_model(model)</code>","text":"<p>Add distributional component to the overall model corresponding to the source emission rate distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>model as constructed so far, consisting of list of distributions.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model, updated with distributions related to source prior.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_source_model(self, model: list) -&gt; list:\n    \"\"\"Add distributional component to the overall model corresponding to the source emission rate distribution.\n\n    Args:\n        model (list): model as constructed so far, consisting of list of distributions.\n\n    Returns:\n        list: model, updated with distributions related to source prior.\n\n    \"\"\"\n    domain_response_lower = None\n    if self.truncation:\n        domain_response_lower = self.emission_rate_lb\n\n    model.append(\n        mcmcNormal(\n            self.map[\"source\"],\n            mean=parameter.MixtureParameterVector(\n                param=self.map[\"emission_rate_mean\"], allocation=self.map[\"allocation\"]\n            ),\n            precision=parameter.MixtureParameterMatrix(\n                param=self.map[\"emission_rate_precision\"], allocation=self.map[\"allocation\"]\n            ),\n            domain_response_lower=domain_response_lower,\n        )\n    )\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.make_source_sampler","title":"<code>make_source_sampler(model, sampler_list=None)</code>","text":"<p>Initialise the source prior distribution part of the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>overall model set for the problem.</p> required <code>sampler_list</code> <code>list</code> <p>list of samplers for individual parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler_list updated with sampler for the emission rate parameters.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_source_sampler(self, model: Model, sampler_list: list = None) -&gt; list:\n    \"\"\"Initialise the source prior distribution part of the sampler.\n\n    Args:\n        model (Model): overall model set for the problem.\n        sampler_list (list): list of samplers for individual parameters.\n\n    Returns:\n        list: sampler_list updated with sampler for the emission rate parameters.\n\n    \"\"\"\n    if sampler_list is None:\n        sampler_list = []\n    sampler_list.append(NormalNormal(self.map[\"source\"], model))\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.make_source_state","title":"<code>make_source_state(state)</code>","text":"<p>Initialise the emission rate part of the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing current state information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>state updated with initial emission rate vector.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_source_state(self, state: dict) -&gt; dict:\n    \"\"\"Initialise the emission rate part of the state.\n\n    Args:\n        state (dict): dictionary containing current state information.\n\n    Returns:\n        dict: state updated with initial emission rate vector.\n\n    \"\"\"\n    state[self.map[\"source\"]] = np.zeros((self.nof_sources, 1))\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalResponse.from_mcmc_dist","title":"<code>from_mcmc_dist(store)</code>","text":"<p>Extract posterior emission rate samples from the MCMC sampler, attach them to the class.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>dictionary containing samples from the MCMC.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc_dist(self, store: dict):\n    \"\"\"Extract posterior emission rate samples from the MCMC sampler, attach them to the class.\n\n    Args:\n        store (dict): dictionary containing samples from the MCMC.\n\n    \"\"\"\n    self.emission_rate = store[self.map[\"source\"]]\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel","title":"<code>SourceModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Component</code>, <code>SourceGrouping</code>, <code>SourceDistribution</code></p> <p>Superclass for the specification of the source model in an inversion run.</p> <p>Various different types of model. A SourceModel is an optional component of a model, and thus inherits from Component.</p> A subclass instance of SourceModel must inherit from <ul> <li>an INSTANCE of SourceDistribution, which specifies a prior emission rate distribution for all sources in the     source map.</li> <li>an INSTANCE of SourceGrouping, which specifies a type of mixture prior specification for the sources (for     which the allocation is to be estimated as part of the inversion).</li> </ul> <p>If the flag reversible_jump == True, then the number of sources and their locations are also estimated as part of the inversion, in addition to the emission rates. If this flag is set to true, the sensor_object, meteorology and gas_species objects are all attached to the class, as they will be required in the repeated computation of updates to the coupling matrix during the inversion.</p> <p>Attributes:</p> Name Type Description <code>dispersion_model</code> <code>GaussianPlume</code> <p>dispersion model used to generate the couplings between source locations and sensor observations.</p> <code>coupling</code> <code>ndarray</code> <p>coupling matrix generated using dispersion_model.</p> <code>sensor_object</code> <code>SensorGroup</code> <p>stores sensor information for reversible jump coupling updates.</p> <code>meteorology</code> <code>MeteorologyGroup</code> <p>stores meteorology information for reversible jump coupling updates.</p> <code>gas_species</code> <code>GasSpecies</code> <p>stores gas species information for reversible jump coupling updates.</p> <code>reversible_jump</code> <code>bool</code> <p>logical indicating whether the reversible jump algorithm for estimation of the number of sources and their locations should be run. Defaults to False.</p> <code>random_walk_step_size</code> <code>ndarray</code> <p>(3 x 1) array specifying the standard deviations of the distributions from which the random walk sampler draws new source locations. Defaults to np.array([1.0, 1.0, 0.1]).</p> <code>site_limits</code> <code>ndarray</code> <p>(3 x 2) array specifying the lower (column 0) and upper (column 1) limits of the analysis site. Only relevant for cases where reversible_jump == True (where sources are free to move in the solution).</p> <code>rate_num_sources</code> <code>int</code> <p>specification for the parameter for the Poisson prior distribution for the total number of sources. Only relevant for cases where reversible_jump == True (where the number of sources in the solution can change).</p> <code>n_sources_max</code> <code>int</code> <p>maximum number of sources that can feature in the solution. Only relevant for cases where reversible_jump == True (where the number of sources in the solution can change).</p> <code>emission_proposal_std</code> <code>float</code> <p>standard deviation of the truncated Gaussian distribution used to propose the new source emission rate in case of a birth move.</p> <code>update_precision</code> <code>bool</code> <p>logical indicating whether the prior precision parameter for emission rates should be updated as part of the inversion. Defaults to false.</p> <code>prior_precision_shape</code> <code>Union[float, ndarray]</code> <p>shape parameters for the prior Gamma distribution for the source precision parameter.</p> <code>prior_precision_rate</code> <code>Union[float, ndarray]</code> <p>rate parameters for the prior Gamma distribution for the source precision parameter.</p> <code>initial_precision</code> <code>Union[float, ndarray]</code> <p>initial value for the source emission rate precision parameter.</p> <code>precision_scalar</code> <code>ndarray</code> <p>precision values generated by MCMC inversion.</p> <code>all_source_locations</code> <code>ENU</code> <p>ENU object containing the locations of after the mcmc has been run, therefore in the situation where the reversible_jump == True, this will be the final locations of the sources in the solution over all iterations. For the case where reversible_jump == False, this will be the locations of the sources in the source map and will not change during the course of the inversion.</p> <code>individual_source_labels</code> <code>list</code> <p>list of labels for each source in the source map, defaults to None.</p> <code>coverage_detection</code> <code>float</code> <p>sensor detection threshold (in ppm) to be used for coverage calculations.</p> <code>coverage_test_source</code> <code>float</code> <p>test source (in kg/hr) which we wish to be able to see in coverage calculation.</p> <code>threshold_function</code> <code>Callable</code> <p>Callable function which returns a single value that defines the threshold for the coupling in a lambda function form. Examples: lambda x: np.quantile(x, 0.95, axis=0), lambda x: np.max(x, axis=0), lambda x: np.mean(x, axis=0). Defaults to np.quantile.</p> <code>label_string</code> <code>str</code> <p>string to append to the parameter mapping, e.g. for fixed sources, defaults to None.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass SourceModel(Component, SourceGrouping, SourceDistribution):\n    \"\"\"Superclass for the specification of the source model in an inversion run.\n\n    Various different types of model. A SourceModel is an optional component of a model, and thus inherits\n    from Component.\n\n    A subclass instance of SourceModel must inherit from:\n        - an INSTANCE of SourceDistribution, which specifies a prior emission rate distribution for all sources in the\n            source map.\n        - an INSTANCE of SourceGrouping, which specifies a type of mixture prior specification for the sources (for\n            which the allocation is to be estimated as part of the inversion).\n\n    If the flag reversible_jump == True, then the number of sources and their locations are also estimated as part of\n    the inversion, in addition to the emission rates. If this flag is set to true, the sensor_object, meteorology and\n    gas_species objects are all attached to the class, as they will be required in the repeated computation of updates\n    to the coupling matrix during the inversion.\n\n    Attributes:\n        dispersion_model (GaussianPlume): dispersion model used to generate the couplings between source locations and\n            sensor observations.\n        coupling (np.ndarray): coupling matrix generated using dispersion_model.\n\n        sensor_object (SensorGroup): stores sensor information for reversible jump coupling updates.\n        meteorology (MeteorologyGroup): stores meteorology information for reversible jump coupling updates.\n        gas_species (GasSpecies): stores gas species information for reversible jump coupling updates.\n\n        reversible_jump (bool): logical indicating whether the reversible jump algorithm for estimation of the number\n            of sources and their locations should be run. Defaults to False.\n        random_walk_step_size (np.ndarray): (3 x 1) array specifying the standard deviations of the distributions\n            from which the random walk sampler draws new source locations. Defaults to np.array([1.0, 1.0, 0.1]).\n        site_limits (np.ndarray): (3 x 2) array specifying the lower (column 0) and upper (column 1) limits of the\n            analysis site. Only relevant for cases where reversible_jump == True (where sources are free to move in\n            the solution).\n        rate_num_sources (int): specification for the parameter for the Poisson prior distribution for the total number\n            of sources. Only relevant for cases where reversible_jump == True (where the number of sources in the\n            solution can change).\n        n_sources_max (int): maximum number of sources that can feature in the solution. Only relevant for cases where\n            reversible_jump == True (where the number of sources in the solution can change).\n        emission_proposal_std (float): standard deviation of the truncated Gaussian distribution used to propose the\n            new source emission rate in case of a birth move.\n\n        update_precision (bool): logical indicating whether the prior precision parameter for emission rates should be\n            updated as part of the inversion. Defaults to false.\n        prior_precision_shape (Union[float, np.ndarray]): shape parameters for the prior Gamma distribution for the\n            source precision parameter.\n        prior_precision_rate (Union[float, np.ndarray]): rate parameters for the prior Gamma distribution for the\n            source precision parameter.\n        initial_precision (Union[float, np.ndarray]): initial value for the source emission rate precision parameter.\n        precision_scalar (np.ndarray): precision values generated by MCMC inversion.\n\n        all_source_locations (ENU): ENU object containing the locations of after the mcmc has been run, therefore in\n            the situation where the reversible_jump == True, this will be the final locations of the sources in the\n            solution over all iterations. For the case where reversible_jump == False, this will be the locations of\n            the sources in the source map and will not change during the course of the inversion.\n        individual_source_labels (list, optional): list of labels for each source in the source map, defaults to None.\n\n        coverage_detection (float): sensor detection threshold (in ppm) to be used for coverage calculations.\n        coverage_test_source (float): test source (in kg/hr) which we wish to be able to see in coverage calculation.\n\n        threshold_function (Callable): Callable function which returns a single value that defines the threshold\n            for the coupling in a lambda function form. Examples: lambda x: np.quantile(x, 0.95, axis=0),\n            lambda x: np.max(x, axis=0), lambda x: np.mean(x, axis=0). Defaults to np.quantile.\n        label_string (str): string to append to the parameter mapping, e.g. for fixed sources, defaults to None.\n\n    \"\"\"\n\n    dispersion_model: GaussianPlume = field(init=False, default=None)\n    coupling: np.ndarray = field(init=False)\n\n    sensor_object: SensorGroup = field(init=False, default=None)\n    meteorology: Meteorology = field(init=False, default=None)\n    gas_species: GasSpecies = field(init=False, default=None)\n\n    reversible_jump: bool = False\n    random_walk_step_size: np.ndarray = field(default_factory=lambda: np.array([1.0, 1.0, 0.1], ndmin=2).T)\n    site_limits: np.ndarray = None\n    rate_num_sources: int = 5\n    n_sources_max: int = 20\n    emission_proposal_std: float = 0.5\n\n    update_precision: bool = False\n    prior_precision_shape: Union[float, np.ndarray] = 1e-3\n    prior_precision_rate: Union[float, np.ndarray] = 1e-3\n    initial_precision: Union[float, np.ndarray] = 1.0\n    precision_scalar: np.ndarray = field(init=False)\n\n    all_source_locations: np.ndarray = field(init=False)\n    individual_source_labels: Optional[list] = None\n\n    coverage_detection: float = 0.1\n    coverage_test_source: float = 6.0\n\n    threshold_function: callable = lambda x: np.quantile(x, 0.95, axis=0)\n\n    label_string: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Post-initialisation of the class.\n\n        This function is called after the class has been initialised,\n        and is used to set up the mapping dictionary for the class by applying the\n        append_string function to the mapping dictionary.\n        \"\"\"\n        if self.label_string is not None:\n            self.append_string(self.label_string)\n\n    @property\n    def nof_sources(self):\n        \"\"\"Get number of sources in the source map.\"\"\"\n        return self.dispersion_model.source_map.nof_sources\n\n    @property\n    def coverage_threshold(self):\n        \"\"\"Compute coverage threshold from detection threshold and test source strength.\"\"\"\n        return self.coverage_test_source / self.coverage_detection\n\n    def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n        \"\"\"Set up the source model.\n\n        Extract required information from the sensor, meteorology and gas species objects:\n            - Attach coupling calculated using self.dispersion_model.\n            - (If self.reversible_jump == True) Attach objects to source model which will be used in RJMCMC sampler,\n                they will be required when we need to update the couplings when new source locations are proposed when\n                we move/birth/death.\n\n        Args:\n            sensor_object (SensorGroup): object containing sensor data.\n            meteorology (MeteorologyGroup): object containing meteorology data.\n            gas_species (GasSpecies): object containing gas species information.\n\n        \"\"\"\n        self.initialise_dispersion_model(sensor_object)\n        self.coupling = self.dispersion_model.compute_coupling(\n            sensor_object, meteorology, gas_species, output_stacked=True\n        )\n        self.screen_coverage()\n        if self.reversible_jump:\n            self.sensor_object = sensor_object\n            self.meteorology = meteorology\n            self.gas_species = gas_species\n\n    def initialise_dispersion_model(self, sensor_object: SensorGroup):\n        \"\"\"Initialise the dispersion model.\n\n        If a dispersion_model has already been attached to this instance, then this function takes no action.\n\n        If a dispersion_model has not already been attached to the instance, then this function adds a GaussianPlume\n        dispersion model, with a default source map that has limits set based on the sensor locations.\n\n        Args:\n            sensor_object (SensorGroup): object containing sensor data.\n\n        \"\"\"\n        if self.dispersion_model is None:\n            source_map = SourceMap()\n            sensor_locations = sensor_object.location.to_enu()\n            location_object = ENU(\n                ref_latitude=sensor_locations.ref_latitude,\n                ref_longitude=sensor_locations.ref_longitude,\n                ref_altitude=sensor_locations.ref_altitude,\n            )\n            source_map.generate_sources(\n                coordinate_object=location_object,\n                sourcemap_limits=np.array(\n                    [\n                        [np.min(sensor_locations.east), np.max(sensor_locations.east)],\n                        [np.min(sensor_locations.north), np.max(sensor_locations.north)],\n                        [np.min(sensor_locations.up), np.max(sensor_locations.up)],\n                    ]\n                ),\n                sourcemap_type=\"grid\",\n            )\n            self.dispersion_model = GaussianPlume(source_map)\n\n    def screen_coverage(self):\n        \"\"\"Screen the initial source map for coverage.\"\"\"\n        in_coverage_area = self.dispersion_model.compute_coverage(\n            self.coupling, coverage_threshold=self.coverage_threshold, threshold_function=self.threshold_function\n        )\n        self.coupling = self.coupling[:, in_coverage_area]\n        all_locations = self.dispersion_model.source_map.location.to_array()\n        screened_locations = all_locations[in_coverage_area, :]\n        self.dispersion_model.source_map.location.from_array(screened_locations)\n\n    def update_coupling_column(self, state: dict, update_column: int) -&gt; dict:\n        \"\"\"Update the coupling, based on changes to the source locations as part of inversion.\n\n        To be used in two different situations:\n            - movement of source locations (e.g. Metropolis Hastings, random walk).\n            - adding of new source locations (e.g. reversible jump birth move).\n        If [update_column &lt; A.shape[1]]: an existing column of the A matrix is updated.\n        If [update_column == A.shape[1]]: a new column is appended to the right-hand side of the A matrix\n        (corresponding to a new source).\n\n        A central assumption of this function is that the sensor information and meteorology information\n        have already been interpolated onto the same space/time points.\n\n        If an update_column is supplied, the coupling for that source location only is calculated to save on\n        computation time. If update_column is None, then we just re-compute the whole coupling matrix.\n\n        Args:\n            state (dict): dictionary containing state parameters.\n            update_column (int): index of the coupling column to be updated.\n\n        Returns:\n            state (dict): state dictionary containing updated coupling information.\n\n        \"\"\"\n        self.dispersion_model.source_map.location.from_array(state[self.map[\"source_location\"]][:, [update_column]].T)\n        new_coupling = self.dispersion_model.compute_coupling(\n            self.sensor_object, self.meteorology, self.gas_species, output_stacked=True, run_interpolation=False\n        )\n\n        if update_column == state[self.map[\"coupling_matrix\"]].shape[1]:\n            state[self.map[\"coupling_matrix\"]] = np.concatenate(\n                (state[self.map[\"coupling_matrix\"]], new_coupling), axis=1\n            )\n        elif update_column &lt; state[self.map[\"coupling_matrix\"]].shape[1]:\n            state[self.map[\"coupling_matrix\"]][:, [update_column]] = new_coupling\n        else:\n            raise ValueError(\"Invalid column specification for updating.\")\n        return state\n\n    def birth_function(self, current_state: dict, prop_state: dict) -&gt; Tuple[dict, float, float]:\n        \"\"\"Update MCMC state based on source birth proposal.\n\n        Proposed state updated as follows:\n            1- Add column to coupling matrix for new source location.\n            2- If required, adjust other components of the state which correspond to the sources.\n        The source emission rate vector will be adjusted using the standardised functionality\n        in the openMCMC package.\n\n        After the coupling has been updated, a coverage test is applied for the new source\n        location. If the max coupling is too small, a large contribution is added to the\n        log-proposal density for the new state, to force the sampler to reject it.\n\n        A central assumption of this function is that the sensor information and meteorology information\n        have already been interpolated onto the same space/time points.\n\n        This function assumes that the new source location has been added as the final column of\n        the source location matrix, and so will correspondingly append the new coupling column to the right\n        hand side of the current state coupling, and append an emission rate as the last element of the\n        current state emission rate vector.\n\n        Args:\n            current_state (dict): dictionary containing parameters of the current state.\n            prop_state (dict): dictionary containing the parameters of the proposed state.\n\n        Returns:\n            prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n            logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n                (i.e. log[p(proposed | current)])\n            logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n                (i.e. log[p(current | proposed)])\n\n        \"\"\"\n        prop_state = self.update_coupling_column(prop_state, int(prop_state[self.map[\"number_sources\"]]) - 1)\n        prop_state[self.map[\"allocation\"]] = np.concatenate(\n            (prop_state[self.map[\"allocation\"]], np.array([0], ndmin=2)), axis=0\n        )\n        in_cov_area = self.dispersion_model.compute_coverage(\n            prop_state[self.map[\"coupling_matrix\"]][:, -1],\n            coverage_threshold=self.coverage_threshold,\n            threshold_function=self.threshold_function,\n        )\n        if not in_cov_area:\n            logp_pr_g_cr = 1e10\n        else:\n            logp_pr_g_cr = 0.0\n        logp_cr_g_pr = 0.0\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def death_function(self, current_state: dict, prop_state: dict, deletion_index: int) -&gt; Tuple[dict, float, float]:\n        \"\"\"Update MCMC state based on source death proposal.\n\n        Proposed state updated as follows:\n            1- Remove column from coupling for deleted source.\n            2- If required, adjust other components of the state which correspond to the sources.\n        The source emission rate vector will be adjusted using the standardised functionality in the general_mcmc repo.\n\n        A central assumption of this function is that the sensor information and meteorology information have already\n        been interpolated onto the same space/time points.\n\n        Args:\n            current_state (dict): dictionary containing parameters of the current state.\n            prop_state (dict): dictionary containing the parameters of the proposed state.\n            deletion_index (int): index of the source to be deleted in the overall set of sources.\n\n        Returns:\n            prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n            logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n                (i.e. log[p(proposed | current)])\n            logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n                (i.e. log[p(current | proposed)])\n\n        \"\"\"\n        prop_state[self.map[\"coupling_matrix\"]] = np.delete(\n            prop_state[self.map[\"coupling_matrix\"]], obj=deletion_index, axis=1\n        )\n        prop_state[self.map[\"allocation\"]] = np.delete(prop_state[self.map[\"allocation\"]], obj=deletion_index, axis=0)\n        logp_pr_g_cr = 0.0\n        logp_cr_g_pr = 0.0\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def move_function(self, current_state: dict, update_column: int) -&gt; dict:\n        \"\"\"Re-compute the coupling after a source location move.\n\n        Function first updates the coupling column, and then checks whether the location passes a coverage test. If the\n        location does not have good enough coverage, the state reverts to the coupling from the current state.\n\n        Args:\n            current_state (dict): dictionary containing parameters of the current state.\n            update_column (int): index of the coupling column to be updated.\n\n        Returns:\n            dict: proposed state, with updated coupling matrix.\n\n        \"\"\"\n        prop_state = deepcopy(current_state)\n        prop_state = self.update_coupling_column(prop_state, update_column)\n        in_cov_area = self.dispersion_model.compute_coverage(\n            prop_state[self.map[\"coupling_matrix\"]][:, update_column],\n            coverage_threshold=self.coverage_threshold,\n            threshold_function=self.threshold_function,\n        )\n        if not in_cov_area:\n            prop_state = deepcopy(current_state)\n        return prop_state\n\n    def make_model(self, model: list) -&gt; list:\n        \"\"\"Take model list and append new elements from current model component.\n\n        Args:\n            model (list): Current list of model elements.\n\n        Returns:\n            list: model list updated with source-related distributions.\n\n        \"\"\"\n        model = self.make_allocation_model(model)\n        model = self.make_source_model(model)\n        if self.update_precision:\n            model.append(\n                Gamma(\n                    self.map[\"emission_rate_precision\"],\n                    shape=self.map[\"precision_prior_shape\"],\n                    rate=self.map[\"precision_prior_rate\"],\n                )\n            )\n        if self.reversible_jump:\n            model.append(\n                Uniform(\n                    response=self.map[\"source_location\"],\n                    domain_response_lower=self.site_limits[:, [0]],\n                    domain_response_upper=self.site_limits[:, [1]],\n                )\n            )\n            model.append(Poisson(response=self.map[\"number_sources\"], rate=self.map[\"number_source_rate\"]))\n        return model\n\n    def make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Take sampler list and append new elements from current model component.\n\n        Args:\n            model (Model): Full model list of distributions.\n            sampler_list (list): Current list of samplers.\n\n        Returns:\n            list: sampler list updated with source-related samplers.\n\n        \"\"\"\n        sampler_list = self.make_source_sampler(model, sampler_list)\n        sampler_list = self.make_allocation_sampler(model, sampler_list)\n        if self.update_precision:\n            sampler_list.append(NormalGamma(self.map[\"emission_rate_precision\"], model))\n        if self.reversible_jump:\n            sampler_list = self.make_sampler_rjmcmc(model, sampler_list)\n        return sampler_list\n\n    def make_state(self, state: dict) -&gt; dict:\n        \"\"\"Take state dictionary and append initial values from model component.\n\n        Args:\n            state (dict): current state vector.\n\n        Returns:\n            dict: current state vector with source-related parameters added.\n\n        \"\"\"\n        state = self.make_allocation_state(state)\n        state = self.make_source_state(state)\n        state[self.map[\"coupling_matrix\"]] = self.coupling\n        state[self.map[\"emission_rate_precision\"]] = np.array(self.initial_precision, ndmin=1)\n        if self.update_precision:\n            state[self.map[\"precision_prior_shape\"]] = np.ones_like(self.initial_precision) * self.prior_precision_shape\n            state[self.map[\"precision_prior_rate\"]] = np.ones_like(self.initial_precision) * self.prior_precision_rate\n        if self.reversible_jump:\n            state[self.map[\"source_location\"]] = self.dispersion_model.source_map.location.to_array().T\n            state[self.map[\"number_sources\"]] = state[self.map[\"source_location\"]].shape[1]\n            state[self.map[\"number_source_rate\"]] = self.rate_num_sources\n        return state\n\n    def make_sampler_rjmcmc(self, model: Model, sampler_list: list) -&gt; list:\n        \"\"\"Create the parts of the sampler related to the reversible jump MCMC scheme.\n\n        RJ MCMC scheme:\n            - create the RandomWalkLoop sampler object which updates the source locations one-at-a-time.\n            - create the ReversibleJump sampler which proposes birth/death moves to add/remove sources from the source\n                map.\n\n        Args:\n            model (Model): model object containing probability density objects for all uncertain\n                parameters.\n            sampler_list (list): list of existing samplers.\n\n        Returns:\n            sampler_list (list): list of samplers updated with samplers corresponding to RJMCMC routine.\n\n        \"\"\"\n        for sampler in sampler_list:\n            if sampler.param == self.map[\"source\"]:\n                sampler.max_variable_size = self.n_sources_max\n\n        sampler_list.append(\n            RandomWalkLoop(\n                self.map[\"source_location\"],\n                model,\n                step=self.random_walk_step_size,\n                max_variable_size=(3, self.n_sources_max),\n                domain_limits=self.site_limits,\n                state_update_function=self.move_function,\n            )\n        )\n        matching_params = {\n            \"variable\": self.map[\"source\"],\n            \"matrix\": self.map[\"coupling_matrix\"],\n            \"scale\": 1.0,\n            \"limits\": [0.0, 1e6],\n        }\n        sampler_list.append(\n            ReversibleJump(\n                self.map[\"number_sources\"],\n                model,\n                step=np.array([1.0], ndmin=2),\n                associated_params=self.map[\"source_location\"],\n                n_max=self.n_sources_max,\n                state_birth_function=self.birth_function,\n                state_death_function=self.death_function,\n                matching_params=matching_params,\n            )\n        )\n        return sampler_list\n\n    def from_mcmc(self, store: dict):\n        \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n        For the reversible jump case we extract all estimated source locations\n        per iteration. For the fixed sources case we grab the source locations\n        from the inputted sourcemap and repeat those for all iterations.\n\n        Args:\n            store (dict): mcmc result dictionary.\n\n        \"\"\"\n        self.from_mcmc_group(store)\n        self.from_mcmc_dist(store)\n        if self.individual_source_labels is None:\n            self.individual_source_labels = list(np.repeat(None, store[self.map[\"source\"]].shape[0]))\n\n        if self.update_precision:\n            self.precision_scalar = store[self.map[\"emission_rate_precision\"]]\n\n        if self.reversible_jump:\n            reference_latitude = self.dispersion_model.source_map.location.ref_latitude\n            reference_longitude = self.dispersion_model.source_map.location.ref_longitude\n            ref_altitude = self.dispersion_model.source_map.location.ref_altitude\n            self.all_source_locations = ENU(\n                ref_latitude=reference_latitude,\n                ref_longitude=reference_longitude,\n                ref_altitude=ref_altitude,\n                east=store[self.map[\"source_location\"]][0, :, :],\n                north=store[self.map[\"source_location\"]][1, :, :],\n                up=store[self.map[\"source_location\"]][2, :, :],\n            )\n\n        else:\n            location_temp = self.dispersion_model.source_map.location.to_enu()\n            location_temp.east = np.repeat(location_temp.east[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n            location_temp.north = np.repeat(location_temp.north[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n            location_temp.up = np.repeat(location_temp.up[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n            self.all_source_locations = location_temp\n\n    def plot_iterations(self, plot: \"Plot\", burn_in_value: int, y_axis_type: str = \"linear\") -&gt; \"Plot\":\n        \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n        Args:\n            burn_in_value (int): Burn in value to show in plot.\n            y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n            plot (Plot): Plot object to which this figure will be added in the figure dictionary.\n\n        Returns:\n            plot (Plot): Plot object to which the figures added in the figure dictionary with\n                keys 'estimated_values_plot'/'log_estimated_values_plot' and 'number_of_sources_plot'\n\n        \"\"\"\n        plot.plot_emission_rate_estimates(source_model_object=self, burn_in=burn_in_value, y_axis_type=y_axis_type)\n        plot.plot_single_trace(object_to_plot=self)\n        return plot\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.nof_sources","title":"<code>nof_sources</code>  <code>property</code>","text":"<p>Get number of sources in the source map.</p>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.coverage_threshold","title":"<code>coverage_threshold</code>  <code>property</code>","text":"<p>Compute coverage threshold from detection threshold and test source strength.</p>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialisation of the class.</p> <p>This function is called after the class has been initialised, and is used to set up the mapping dictionary for the class by applying the append_string function to the mapping dictionary.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post-initialisation of the class.\n\n    This function is called after the class has been initialised,\n    and is used to set up the mapping dictionary for the class by applying the\n    append_string function to the mapping dictionary.\n    \"\"\"\n    if self.label_string is not None:\n        self.append_string(self.label_string)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.initialise","title":"<code>initialise(sensor_object, meteorology, gas_species)</code>","text":"<p>Set up the source model.</p> <p>Extract required information from the sensor, meteorology and gas species objects:     - Attach coupling calculated using self.dispersion_model.     - (If self.reversible_jump == True) Attach objects to source model which will be used in RJMCMC sampler,         they will be required when we need to update the couplings when new source locations are proposed when         we move/birth/death.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>object containing sensor data.</p> required <code>meteorology</code> <code>MeteorologyGroup</code> <p>object containing meteorology data.</p> required <code>gas_species</code> <code>GasSpecies</code> <p>object containing gas species information.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def initialise(self, sensor_object: SensorGroup, meteorology: Meteorology, gas_species: GasSpecies):\n    \"\"\"Set up the source model.\n\n    Extract required information from the sensor, meteorology and gas species objects:\n        - Attach coupling calculated using self.dispersion_model.\n        - (If self.reversible_jump == True) Attach objects to source model which will be used in RJMCMC sampler,\n            they will be required when we need to update the couplings when new source locations are proposed when\n            we move/birth/death.\n\n    Args:\n        sensor_object (SensorGroup): object containing sensor data.\n        meteorology (MeteorologyGroup): object containing meteorology data.\n        gas_species (GasSpecies): object containing gas species information.\n\n    \"\"\"\n    self.initialise_dispersion_model(sensor_object)\n    self.coupling = self.dispersion_model.compute_coupling(\n        sensor_object, meteorology, gas_species, output_stacked=True\n    )\n    self.screen_coverage()\n    if self.reversible_jump:\n        self.sensor_object = sensor_object\n        self.meteorology = meteorology\n        self.gas_species = gas_species\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.initialise_dispersion_model","title":"<code>initialise_dispersion_model(sensor_object)</code>","text":"<p>Initialise the dispersion model.</p> <p>If a dispersion_model has already been attached to this instance, then this function takes no action.</p> <p>If a dispersion_model has not already been attached to the instance, then this function adds a GaussianPlume dispersion model, with a default source map that has limits set based on the sensor locations.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>SensorGroup</code> <p>object containing sensor data.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def initialise_dispersion_model(self, sensor_object: SensorGroup):\n    \"\"\"Initialise the dispersion model.\n\n    If a dispersion_model has already been attached to this instance, then this function takes no action.\n\n    If a dispersion_model has not already been attached to the instance, then this function adds a GaussianPlume\n    dispersion model, with a default source map that has limits set based on the sensor locations.\n\n    Args:\n        sensor_object (SensorGroup): object containing sensor data.\n\n    \"\"\"\n    if self.dispersion_model is None:\n        source_map = SourceMap()\n        sensor_locations = sensor_object.location.to_enu()\n        location_object = ENU(\n            ref_latitude=sensor_locations.ref_latitude,\n            ref_longitude=sensor_locations.ref_longitude,\n            ref_altitude=sensor_locations.ref_altitude,\n        )\n        source_map.generate_sources(\n            coordinate_object=location_object,\n            sourcemap_limits=np.array(\n                [\n                    [np.min(sensor_locations.east), np.max(sensor_locations.east)],\n                    [np.min(sensor_locations.north), np.max(sensor_locations.north)],\n                    [np.min(sensor_locations.up), np.max(sensor_locations.up)],\n                ]\n            ),\n            sourcemap_type=\"grid\",\n        )\n        self.dispersion_model = GaussianPlume(source_map)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.screen_coverage","title":"<code>screen_coverage()</code>","text":"<p>Screen the initial source map for coverage.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def screen_coverage(self):\n    \"\"\"Screen the initial source map for coverage.\"\"\"\n    in_coverage_area = self.dispersion_model.compute_coverage(\n        self.coupling, coverage_threshold=self.coverage_threshold, threshold_function=self.threshold_function\n    )\n    self.coupling = self.coupling[:, in_coverage_area]\n    all_locations = self.dispersion_model.source_map.location.to_array()\n    screened_locations = all_locations[in_coverage_area, :]\n    self.dispersion_model.source_map.location.from_array(screened_locations)\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.update_coupling_column","title":"<code>update_coupling_column(state, update_column)</code>","text":"<p>Update the coupling, based on changes to the source locations as part of inversion.</p> To be used in two different situations <ul> <li>movement of source locations (e.g. Metropolis Hastings, random walk).</li> <li>adding of new source locations (e.g. reversible jump birth move).</li> </ul> <p>If [update_column &lt; A.shape[1]]: an existing column of the A matrix is updated. If [update_column == A.shape[1]]: a new column is appended to the right-hand side of the A matrix (corresponding to a new source).</p> <p>A central assumption of this function is that the sensor information and meteorology information have already been interpolated onto the same space/time points.</p> <p>If an update_column is supplied, the coupling for that source location only is calculated to save on computation time. If update_column is None, then we just re-compute the whole coupling matrix.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary containing state parameters.</p> required <code>update_column</code> <code>int</code> <p>index of the coupling column to be updated.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>state dictionary containing updated coupling information.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def update_coupling_column(self, state: dict, update_column: int) -&gt; dict:\n    \"\"\"Update the coupling, based on changes to the source locations as part of inversion.\n\n    To be used in two different situations:\n        - movement of source locations (e.g. Metropolis Hastings, random walk).\n        - adding of new source locations (e.g. reversible jump birth move).\n    If [update_column &lt; A.shape[1]]: an existing column of the A matrix is updated.\n    If [update_column == A.shape[1]]: a new column is appended to the right-hand side of the A matrix\n    (corresponding to a new source).\n\n    A central assumption of this function is that the sensor information and meteorology information\n    have already been interpolated onto the same space/time points.\n\n    If an update_column is supplied, the coupling for that source location only is calculated to save on\n    computation time. If update_column is None, then we just re-compute the whole coupling matrix.\n\n    Args:\n        state (dict): dictionary containing state parameters.\n        update_column (int): index of the coupling column to be updated.\n\n    Returns:\n        state (dict): state dictionary containing updated coupling information.\n\n    \"\"\"\n    self.dispersion_model.source_map.location.from_array(state[self.map[\"source_location\"]][:, [update_column]].T)\n    new_coupling = self.dispersion_model.compute_coupling(\n        self.sensor_object, self.meteorology, self.gas_species, output_stacked=True, run_interpolation=False\n    )\n\n    if update_column == state[self.map[\"coupling_matrix\"]].shape[1]:\n        state[self.map[\"coupling_matrix\"]] = np.concatenate(\n            (state[self.map[\"coupling_matrix\"]], new_coupling), axis=1\n        )\n    elif update_column &lt; state[self.map[\"coupling_matrix\"]].shape[1]:\n        state[self.map[\"coupling_matrix\"]][:, [update_column]] = new_coupling\n    else:\n        raise ValueError(\"Invalid column specification for updating.\")\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.birth_function","title":"<code>birth_function(current_state, prop_state)</code>","text":"<p>Update MCMC state based on source birth proposal.</p> Proposed state updated as follows <p>1- Add column to coupling matrix for new source location. 2- If required, adjust other components of the state which correspond to the sources.</p> <p>The source emission rate vector will be adjusted using the standardised functionality in the openMCMC package.</p> <p>After the coupling has been updated, a coverage test is applied for the new source location. If the max coupling is too small, a large contribution is added to the log-proposal density for the new state, to force the sampler to reject it.</p> <p>A central assumption of this function is that the sensor information and meteorology information have already been interpolated onto the same space/time points.</p> <p>This function assumes that the new source location has been added as the final column of the source location matrix, and so will correspondingly append the new coupling column to the right hand side of the current state coupling, and append an emission rate as the last element of the current state emission rate vector.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing parameters of the current state.</p> required <code>prop_state</code> <code>dict</code> <p>dictionary containing the parameters of the proposed state.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state, with coupling matrix and source emission rate vector updated.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-transition density of the proposed state given the current state (i.e. log[p(proposed | current)])</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-transition density of the current state given the proposed state (i.e. log[p(current | proposed)])</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def birth_function(self, current_state: dict, prop_state: dict) -&gt; Tuple[dict, float, float]:\n    \"\"\"Update MCMC state based on source birth proposal.\n\n    Proposed state updated as follows:\n        1- Add column to coupling matrix for new source location.\n        2- If required, adjust other components of the state which correspond to the sources.\n    The source emission rate vector will be adjusted using the standardised functionality\n    in the openMCMC package.\n\n    After the coupling has been updated, a coverage test is applied for the new source\n    location. If the max coupling is too small, a large contribution is added to the\n    log-proposal density for the new state, to force the sampler to reject it.\n\n    A central assumption of this function is that the sensor information and meteorology information\n    have already been interpolated onto the same space/time points.\n\n    This function assumes that the new source location has been added as the final column of\n    the source location matrix, and so will correspondingly append the new coupling column to the right\n    hand side of the current state coupling, and append an emission rate as the last element of the\n    current state emission rate vector.\n\n    Args:\n        current_state (dict): dictionary containing parameters of the current state.\n        prop_state (dict): dictionary containing the parameters of the proposed state.\n\n    Returns:\n        prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n        logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n            (i.e. log[p(proposed | current)])\n        logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n            (i.e. log[p(current | proposed)])\n\n    \"\"\"\n    prop_state = self.update_coupling_column(prop_state, int(prop_state[self.map[\"number_sources\"]]) - 1)\n    prop_state[self.map[\"allocation\"]] = np.concatenate(\n        (prop_state[self.map[\"allocation\"]], np.array([0], ndmin=2)), axis=0\n    )\n    in_cov_area = self.dispersion_model.compute_coverage(\n        prop_state[self.map[\"coupling_matrix\"]][:, -1],\n        coverage_threshold=self.coverage_threshold,\n        threshold_function=self.threshold_function,\n    )\n    if not in_cov_area:\n        logp_pr_g_cr = 1e10\n    else:\n        logp_pr_g_cr = 0.0\n    logp_cr_g_pr = 0.0\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.death_function","title":"<code>death_function(current_state, prop_state, deletion_index)</code>","text":"<p>Update MCMC state based on source death proposal.</p> Proposed state updated as follows <p>1- Remove column from coupling for deleted source. 2- If required, adjust other components of the state which correspond to the sources.</p> <p>The source emission rate vector will be adjusted using the standardised functionality in the general_mcmc repo.</p> <p>A central assumption of this function is that the sensor information and meteorology information have already been interpolated onto the same space/time points.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing parameters of the current state.</p> required <code>prop_state</code> <code>dict</code> <p>dictionary containing the parameters of the proposed state.</p> required <code>deletion_index</code> <code>int</code> <p>index of the source to be deleted in the overall set of sources.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state, with coupling matrix and source emission rate vector updated.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-transition density of the proposed state given the current state (i.e. log[p(proposed | current)])</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-transition density of the current state given the proposed state (i.e. log[p(current | proposed)])</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def death_function(self, current_state: dict, prop_state: dict, deletion_index: int) -&gt; Tuple[dict, float, float]:\n    \"\"\"Update MCMC state based on source death proposal.\n\n    Proposed state updated as follows:\n        1- Remove column from coupling for deleted source.\n        2- If required, adjust other components of the state which correspond to the sources.\n    The source emission rate vector will be adjusted using the standardised functionality in the general_mcmc repo.\n\n    A central assumption of this function is that the sensor information and meteorology information have already\n    been interpolated onto the same space/time points.\n\n    Args:\n        current_state (dict): dictionary containing parameters of the current state.\n        prop_state (dict): dictionary containing the parameters of the proposed state.\n        deletion_index (int): index of the source to be deleted in the overall set of sources.\n\n    Returns:\n        prop_state (dict): proposed state, with coupling matrix and source emission rate vector updated.\n        logp_pr_g_cr (float): log-transition density of the proposed state given the current state\n            (i.e. log[p(proposed | current)])\n        logp_cr_g_pr (float): log-transition density of the current state given the proposed state\n            (i.e. log[p(current | proposed)])\n\n    \"\"\"\n    prop_state[self.map[\"coupling_matrix\"]] = np.delete(\n        prop_state[self.map[\"coupling_matrix\"]], obj=deletion_index, axis=1\n    )\n    prop_state[self.map[\"allocation\"]] = np.delete(prop_state[self.map[\"allocation\"]], obj=deletion_index, axis=0)\n    logp_pr_g_cr = 0.0\n    logp_cr_g_pr = 0.0\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.move_function","title":"<code>move_function(current_state, update_column)</code>","text":"<p>Re-compute the coupling after a source location move.</p> <p>Function first updates the coupling column, and then checks whether the location passes a coverage test. If the location does not have good enough coverage, the state reverts to the coupling from the current state.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing parameters of the current state.</p> required <code>update_column</code> <code>int</code> <p>index of the coupling column to be updated.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>proposed state, with updated coupling matrix.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def move_function(self, current_state: dict, update_column: int) -&gt; dict:\n    \"\"\"Re-compute the coupling after a source location move.\n\n    Function first updates the coupling column, and then checks whether the location passes a coverage test. If the\n    location does not have good enough coverage, the state reverts to the coupling from the current state.\n\n    Args:\n        current_state (dict): dictionary containing parameters of the current state.\n        update_column (int): index of the coupling column to be updated.\n\n    Returns:\n        dict: proposed state, with updated coupling matrix.\n\n    \"\"\"\n    prop_state = deepcopy(current_state)\n    prop_state = self.update_coupling_column(prop_state, update_column)\n    in_cov_area = self.dispersion_model.compute_coverage(\n        prop_state[self.map[\"coupling_matrix\"]][:, update_column],\n        coverage_threshold=self.coverage_threshold,\n        threshold_function=self.threshold_function,\n    )\n    if not in_cov_area:\n        prop_state = deepcopy(current_state)\n    return prop_state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_model","title":"<code>make_model(model)</code>","text":"<p>Take model list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>Current list of model elements.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>model list updated with source-related distributions.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_model(self, model: list) -&gt; list:\n    \"\"\"Take model list and append new elements from current model component.\n\n    Args:\n        model (list): Current list of model elements.\n\n    Returns:\n        list: model list updated with source-related distributions.\n\n    \"\"\"\n    model = self.make_allocation_model(model)\n    model = self.make_source_model(model)\n    if self.update_precision:\n        model.append(\n            Gamma(\n                self.map[\"emission_rate_precision\"],\n                shape=self.map[\"precision_prior_shape\"],\n                rate=self.map[\"precision_prior_rate\"],\n            )\n        )\n    if self.reversible_jump:\n        model.append(\n            Uniform(\n                response=self.map[\"source_location\"],\n                domain_response_lower=self.site_limits[:, [0]],\n                domain_response_upper=self.site_limits[:, [1]],\n            )\n        )\n        model.append(Poisson(response=self.map[\"number_sources\"], rate=self.map[\"number_source_rate\"]))\n    return model\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_sampler","title":"<code>make_sampler(model, sampler_list)</code>","text":"<p>Take sampler list and append new elements from current model component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Full model list of distributions.</p> required <code>sampler_list</code> <code>list</code> <p>Current list of samplers.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>sampler list updated with source-related samplers.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_sampler(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Take sampler list and append new elements from current model component.\n\n    Args:\n        model (Model): Full model list of distributions.\n        sampler_list (list): Current list of samplers.\n\n    Returns:\n        list: sampler list updated with source-related samplers.\n\n    \"\"\"\n    sampler_list = self.make_source_sampler(model, sampler_list)\n    sampler_list = self.make_allocation_sampler(model, sampler_list)\n    if self.update_precision:\n        sampler_list.append(NormalGamma(self.map[\"emission_rate_precision\"], model))\n    if self.reversible_jump:\n        sampler_list = self.make_sampler_rjmcmc(model, sampler_list)\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_state","title":"<code>make_state(state)</code>","text":"<p>Take state dictionary and append initial values from model component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state vector.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>current state vector with source-related parameters added.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_state(self, state: dict) -&gt; dict:\n    \"\"\"Take state dictionary and append initial values from model component.\n\n    Args:\n        state (dict): current state vector.\n\n    Returns:\n        dict: current state vector with source-related parameters added.\n\n    \"\"\"\n    state = self.make_allocation_state(state)\n    state = self.make_source_state(state)\n    state[self.map[\"coupling_matrix\"]] = self.coupling\n    state[self.map[\"emission_rate_precision\"]] = np.array(self.initial_precision, ndmin=1)\n    if self.update_precision:\n        state[self.map[\"precision_prior_shape\"]] = np.ones_like(self.initial_precision) * self.prior_precision_shape\n        state[self.map[\"precision_prior_rate\"]] = np.ones_like(self.initial_precision) * self.prior_precision_rate\n    if self.reversible_jump:\n        state[self.map[\"source_location\"]] = self.dispersion_model.source_map.location.to_array().T\n        state[self.map[\"number_sources\"]] = state[self.map[\"source_location\"]].shape[1]\n        state[self.map[\"number_source_rate\"]] = self.rate_num_sources\n    return state\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.make_sampler_rjmcmc","title":"<code>make_sampler_rjmcmc(model, sampler_list)</code>","text":"<p>Create the parts of the sampler related to the reversible jump MCMC scheme.</p> RJ MCMC scheme <ul> <li>create the RandomWalkLoop sampler object which updates the source locations one-at-a-time.</li> <li>create the ReversibleJump sampler which proposes birth/death moves to add/remove sources from the source     map.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>model object containing probability density objects for all uncertain parameters.</p> required <code>sampler_list</code> <code>list</code> <p>list of existing samplers.</p> required <p>Returns:</p> Name Type Description <code>sampler_list</code> <code>list</code> <p>list of samplers updated with samplers corresponding to RJMCMC routine.</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def make_sampler_rjmcmc(self, model: Model, sampler_list: list) -&gt; list:\n    \"\"\"Create the parts of the sampler related to the reversible jump MCMC scheme.\n\n    RJ MCMC scheme:\n        - create the RandomWalkLoop sampler object which updates the source locations one-at-a-time.\n        - create the ReversibleJump sampler which proposes birth/death moves to add/remove sources from the source\n            map.\n\n    Args:\n        model (Model): model object containing probability density objects for all uncertain\n            parameters.\n        sampler_list (list): list of existing samplers.\n\n    Returns:\n        sampler_list (list): list of samplers updated with samplers corresponding to RJMCMC routine.\n\n    \"\"\"\n    for sampler in sampler_list:\n        if sampler.param == self.map[\"source\"]:\n            sampler.max_variable_size = self.n_sources_max\n\n    sampler_list.append(\n        RandomWalkLoop(\n            self.map[\"source_location\"],\n            model,\n            step=self.random_walk_step_size,\n            max_variable_size=(3, self.n_sources_max),\n            domain_limits=self.site_limits,\n            state_update_function=self.move_function,\n        )\n    )\n    matching_params = {\n        \"variable\": self.map[\"source\"],\n        \"matrix\": self.map[\"coupling_matrix\"],\n        \"scale\": 1.0,\n        \"limits\": [0.0, 1e6],\n    }\n    sampler_list.append(\n        ReversibleJump(\n            self.map[\"number_sources\"],\n            model,\n            step=np.array([1.0], ndmin=2),\n            associated_params=self.map[\"source_location\"],\n            n_max=self.n_sources_max,\n            state_birth_function=self.birth_function,\n            state_death_function=self.death_function,\n            matching_params=matching_params,\n        )\n    )\n    return sampler_list\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.from_mcmc","title":"<code>from_mcmc(store)</code>","text":"<p>Extract results of mcmc from mcmc.store and attach to components.</p> <p>For the reversible jump case we extract all estimated source locations per iteration. For the fixed sources case we grab the source locations from the inputted sourcemap and repeat those for all iterations.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>mcmc result dictionary.</p> required Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def from_mcmc(self, store: dict):\n    \"\"\"Extract results of mcmc from mcmc.store and attach to components.\n\n    For the reversible jump case we extract all estimated source locations\n    per iteration. For the fixed sources case we grab the source locations\n    from the inputted sourcemap and repeat those for all iterations.\n\n    Args:\n        store (dict): mcmc result dictionary.\n\n    \"\"\"\n    self.from_mcmc_group(store)\n    self.from_mcmc_dist(store)\n    if self.individual_source_labels is None:\n        self.individual_source_labels = list(np.repeat(None, store[self.map[\"source\"]].shape[0]))\n\n    if self.update_precision:\n        self.precision_scalar = store[self.map[\"emission_rate_precision\"]]\n\n    if self.reversible_jump:\n        reference_latitude = self.dispersion_model.source_map.location.ref_latitude\n        reference_longitude = self.dispersion_model.source_map.location.ref_longitude\n        ref_altitude = self.dispersion_model.source_map.location.ref_altitude\n        self.all_source_locations = ENU(\n            ref_latitude=reference_latitude,\n            ref_longitude=reference_longitude,\n            ref_altitude=ref_altitude,\n            east=store[self.map[\"source_location\"]][0, :, :],\n            north=store[self.map[\"source_location\"]][1, :, :],\n            up=store[self.map[\"source_location\"]][2, :, :],\n        )\n\n    else:\n        location_temp = self.dispersion_model.source_map.location.to_enu()\n        location_temp.east = np.repeat(location_temp.east[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n        location_temp.north = np.repeat(location_temp.north[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n        location_temp.up = np.repeat(location_temp.up[:, np.newaxis], store[\"log_post\"].shape[0], axis=1)\n        self.all_source_locations = location_temp\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.SourceModel.plot_iterations","title":"<code>plot_iterations(plot, burn_in_value, y_axis_type='linear')</code>","text":"<p>Plot the emission rate estimates source model object against MCMC iteration.</p> <p>Parameters:</p> Name Type Description Default <code>burn_in_value</code> <code>int</code> <p>Burn in value to show in plot.</p> required <code>y_axis_type</code> <code>str</code> <p>String to indicate whether the y-axis should be linear of log scale.</p> <code>'linear'</code> <code>plot</code> <code>Plot</code> <p>Plot object to which this figure will be added in the figure dictionary.</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>Plot</code> <p>Plot object to which the figures added in the figure dictionary with keys 'estimated_values_plot'/'log_estimated_values_plot' and 'number_of_sources_plot'</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>def plot_iterations(self, plot: \"Plot\", burn_in_value: int, y_axis_type: str = \"linear\") -&gt; \"Plot\":\n    \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n    Args:\n        burn_in_value (int): Burn in value to show in plot.\n        y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n        plot (Plot): Plot object to which this figure will be added in the figure dictionary.\n\n    Returns:\n        plot (Plot): Plot object to which the figures added in the figure dictionary with\n            keys 'estimated_values_plot'/'log_estimated_values_plot' and 'number_of_sources_plot'\n\n    \"\"\"\n    plot.plot_emission_rate_estimates(source_model_object=self, burn_in=burn_in_value, y_axis_type=y_axis_type)\n    plot.plot_single_trace(object_to_plot=self)\n    return plot\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.Normal","title":"<code>Normal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceModel</code>, <code>NullGrouping</code>, <code>NormalResponse</code></p> <p>Normal model, with null allocation.</p> <p>(Truncated) Gaussian prior for emission rates, no grouping/allocation; no transformation applied to emission rate parameters.</p> Can be used in the following cases <ul> <li>Fixed set of sources (grid or specific locations), all with the same Gaussian prior distribution.</li> <li>Variable number of sources, with a common prior distribution, estimated using reversible jump MCMC.</li> <li>Fixed set of sources with a bespoke prior per source (using the allocation to map prior parameters onto     sources).</li> </ul> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass Normal(SourceModel, NullGrouping, NormalResponse):\n    \"\"\"Normal model, with null allocation.\n\n    (Truncated) Gaussian prior for emission rates, no grouping/allocation; no transformation applied to emission rate\n    parameters.\n\n    Can be used in the following cases:\n        - Fixed set of sources (grid or specific locations), all with the same Gaussian prior distribution.\n        - Variable number of sources, with a common prior distribution, estimated using reversible jump MCMC.\n        - Fixed set of sources with a bespoke prior per source (using the allocation to map prior parameters onto\n            sources).\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/component/source_model/#pyelq.component.source_model.NormalSlabAndSpike","title":"<code>NormalSlabAndSpike</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SourceModel</code>, <code>SlabAndSpike</code>, <code>NormalResponse</code></p> <p>Normal Slab and Spike model.</p> <p>(Truncated) Gaussian prior for emission rates, slab and spike prior, with allocation estimation; no transformation applied to emission rate parameters.</p> <p>Attributes:</p> Name Type Description <code>initial_precision</code> <code>ndarray</code> <p>initial precision parameter for a slab and spike case. shape=(2, 1).</p> <code>emission_rate_mean</code> <code>ndarray</code> <p>emission rate prior mean for a slab and spike case. shape=(2, 1).</p> Source code in <code>src/pyelq/component/source_model.py</code> <pre><code>@dataclass\nclass NormalSlabAndSpike(SourceModel, SlabAndSpike, NormalResponse):\n    \"\"\"Normal Slab and Spike model.\n\n    (Truncated) Gaussian prior for emission rates, slab and spike prior, with allocation estimation; no transformation\n    applied to emission rate parameters.\n\n    Attributes:\n        initial_precision (np.ndarray): initial precision parameter for a slab and spike case. shape=(2, 1).\n        emission_rate_mean (np.ndarray): emission rate prior mean for a slab and spike case. shape=(2, 1).\n\n    \"\"\"\n\n    initial_precision: np.ndarray = field(default_factory=lambda: np.array([1 / (10**2), 1 / (0.01**2)], ndmin=2).T)\n    emission_rate_mean: np.ndarray = field(default_factory=lambda: np.array([0, 0], ndmin=2).T)\n</code></pre>"},{"location":"pyelq/data_access/data_access/","title":"Data Access","text":""},{"location":"pyelq/data_access/data_access/#data-access","title":"Data access","text":"<p>Data access module.</p> <p>Superclass containing some common attributes and helper functions used in multiple data access classes</p>"},{"location":"pyelq/data_access/data_access/#pyelq.data_access.data_access.DataAccess","title":"<code>DataAccess</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>DataAccess superclass containing some common attributes and functionalities.</p> <p>This superclass is used to show the type of methods to implement when creating a new data access class. The data access classes are used to convert raw data into well-defined classes and objects which can be used by the rest of the package.</p> <p>Attributes:</p> Name Type Description <code>latitude_bounds</code> <code>tuple</code> <p>Tuple specifying (latitude_min, latitude_max)</p> <code>longitude_bounds</code> <code>tuple</code> <p>Tuple specifying (longitude_min, longitude_max)</p> <code>date_bounds</code> <code>tuple</code> <p>Tuple specifying (datetime_min, datetime_max)</p> Source code in <code>src/pyelq/data_access/data_access.py</code> <pre><code>@dataclass\nclass DataAccess(ABC):\n    \"\"\"DataAccess superclass containing some common attributes and functionalities.\n\n    This superclass is used to show the type of methods to implement when creating a new data access class. The data\n    access classes are used to convert raw data into well-defined classes and objects which can be used by the rest of\n    the package.\n\n    Attributes:\n        latitude_bounds (tuple, optional): Tuple specifying (latitude_min, latitude_max)\n        longitude_bounds (tuple, optional): Tuple specifying (longitude_min, longitude_max)\n        date_bounds (tuple, optional): Tuple specifying (datetime_min, datetime_max)\n\n    \"\"\"\n\n    latitude_bounds: tuple = (None, None)\n    longitude_bounds: tuple = (None, None)\n    date_bounds: tuple = (None, None)\n\n    @abstractmethod\n    def to_sensor(self, *args: Any, **kwargs: dict) -&gt; Union[Sensor, SensorGroup]:\n        \"\"\"Abstract method to convert raw data into a Sensor or SensorGroup object.\n\n        This method should be implemented to convert the raw data into a Sensor or SensorGroup object.\n\n        Args:\n            *args (Any): Variable length argument list of any type.\n            **kwargs (dict): Arbitrary keyword arguments\n\n        \"\"\"\n\n    @abstractmethod\n    def to_meteorology(self, *args: Any, **kwargs: dict) -&gt; Union[Meteorology, MeteorologyGroup]:\n        \"\"\"Abstract method to convert raw data into a Meteorology or MeteorologyGroup object.\n\n        This method should be implemented to convert the raw data into a Meteorology or MeteorologyGroup object.\n\n        Args:\n            *args (Any): Variable length argument list of any type.\n            **kwargs (dict): Arbitrary keyword arguments\n\n        \"\"\"\n\n    def _query_aoi(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Helper function to perform area of interest query on data.\n\n        Args:\n            data (pd.Dataframe): Pandas dataframe to perform the query on\n\n        \"\"\"\n        aoi_query_string = \"\"\n        if self.latitude_bounds[0] is not None:\n            aoi_query_string += f\" &amp; latitude&gt;={self.latitude_bounds[0]}\"\n        if self.latitude_bounds[1] is not None:\n            aoi_query_string += f\" &amp; latitude&lt;={self.latitude_bounds[1]}\"\n        if self.longitude_bounds[0] is not None:\n            aoi_query_string += f\" &amp; longitude&gt;={self.longitude_bounds[0]}\"\n        if self.longitude_bounds[1] is not None:\n            aoi_query_string += f\" &amp; longitude&lt;={self.longitude_bounds[1]}\"\n        if len(aoi_query_string) &gt; 0:\n            aoi_query_string = aoi_query_string[3:]\n            return data.query(aoi_query_string).copy()\n        return data\n\n    def _query_time(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Helper function to perform time query on data.\n\n        Args:\n            data (pd.Dataframe): Pandas dataframe to perform the query on\n\n        \"\"\"\n        time_query_string = \"\"\n        if self.date_bounds[0] is not None:\n            timestamp_min = dt.datetime.timestamp(self.date_bounds[0])\n            time_query_string += f\" &amp; timestamp&gt;={timestamp_min}\"\n        if self.date_bounds[1] is not None:\n            timestamp_max = dt.datetime.timestamp(self.date_bounds[1])\n            time_query_string += f\" &amp; timestamp&lt;={timestamp_max}\"\n        if len(time_query_string) &gt; 0:\n            time_query_string = time_query_string[3:]\n            return data.query(time_query_string).copy()\n        return data\n</code></pre>"},{"location":"pyelq/data_access/data_access/#pyelq.data_access.data_access.DataAccess.to_sensor","title":"<code>to_sensor(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to convert raw data into a Sensor or SensorGroup object.</p> <p>This method should be implemented to convert the raw data into a Sensor or SensorGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list of any type.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Arbitrary keyword arguments</p> <code>{}</code> Source code in <code>src/pyelq/data_access/data_access.py</code> <pre><code>@abstractmethod\ndef to_sensor(self, *args: Any, **kwargs: dict) -&gt; Union[Sensor, SensorGroup]:\n    \"\"\"Abstract method to convert raw data into a Sensor or SensorGroup object.\n\n    This method should be implemented to convert the raw data into a Sensor or SensorGroup object.\n\n    Args:\n        *args (Any): Variable length argument list of any type.\n        **kwargs (dict): Arbitrary keyword arguments\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/data_access/data_access/#pyelq.data_access.data_access.DataAccess.to_meteorology","title":"<code>to_meteorology(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to convert raw data into a Meteorology or MeteorologyGroup object.</p> <p>This method should be implemented to convert the raw data into a Meteorology or MeteorologyGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list of any type.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Arbitrary keyword arguments</p> <code>{}</code> Source code in <code>src/pyelq/data_access/data_access.py</code> <pre><code>@abstractmethod\ndef to_meteorology(self, *args: Any, **kwargs: dict) -&gt; Union[Meteorology, MeteorologyGroup]:\n    \"\"\"Abstract method to convert raw data into a Meteorology or MeteorologyGroup object.\n\n    This method should be implemented to convert the raw data into a Meteorology or MeteorologyGroup object.\n\n    Args:\n        *args (Any): Variable length argument list of any type.\n        **kwargs (dict): Arbitrary keyword arguments\n\n    \"\"\"\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/","title":"Dispersion Model","text":""},{"location":"pyelq/dispersion_model/gaussian_plume/#gaussian-plume","title":"Gaussian Plume","text":"<p>Gaussian Plume module.</p> <p>The class for the Gaussian Plume dispersion model used in pyELQ.</p> <p>The Mathematics of Atmospheric Dispersion Modeling, John M. Stockie, DOI. 10.1137/10080991X</p>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume","title":"<code>GaussianPlume</code>  <code>dataclass</code>","text":"<p>Defines the Gaussian plume dispersion model class.</p> <p>Attributes:</p> Name Type Description <code>source_map</code> <code>Sourcemap</code> <p>SourceMap object used for the dispersion model</p> <code>source_half_width</code> <code>float</code> <p>Source half width (radius) to be used in the Gaussian plume model (in meters)</p> <code>minimum_contribution</code> <code>float</code> <p>All elements in the Gaussian plume coupling smaller than this number will be set to 0. Helps to speed up matrix multiplications/matrix inverses, also helps with stability</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>@dataclass\nclass GaussianPlume:\n    \"\"\"Defines the Gaussian plume dispersion model class.\n\n    Attributes:\n        source_map (Sourcemap): SourceMap object used for the dispersion model\n        source_half_width (float): Source half width (radius) to be used in the Gaussian plume model (in meters)\n        minimum_contribution (float): All elements in the Gaussian plume coupling smaller than this number will be set\n            to 0. Helps to speed up matrix multiplications/matrix inverses, also helps with stability\n\n    \"\"\"\n\n    source_map: SourceMap\n    source_half_width: float = 1\n    minimum_contribution: float = 0\n\n    def compute_coupling(\n        self,\n        sensor_object: Union[SensorGroup, Sensor],\n        meteorology_object: Union[MeteorologyGroup, Meteorology],\n        gas_object: GasSpecies = None,\n        output_stacked: bool = False,\n        run_interpolation: bool = True,\n    ) -&gt; Union[list, np.ndarray, dict]:\n        \"\"\"Top level function to calculate the Gaussian plume coupling.\n\n        Calculates the coupling for either a single sensor object or a dictionary of sensor objects.\n\n        When both a SensorGroup and a MeteorologyGroup have been passed in, we assume they are consistent and contain\n        exactly the same keys for each item in both groups. Also assuming interpolation has been performed and time axes\n        are consistent, so we set run_interpolation to False\n\n        When you input a SensorGroup and a single Meteorology object we convert this object into a dictionary, so we\n        don't have to duplicate the same code.\n\n        Args:\n            sensor_object (Union[SensorGroup, Sensor]): Single sensor object or SensorGroup object which is used in the\n                calculation of the plume coupling.\n            meteorology_object (Union[MeteorologyGroup, Meteorology]): Meteorology object or MeteorologyGroup object\n                which is used in the calculation of the plume coupling.\n            gas_object (GasSpecies, optional): Optional input, a gas species object to correctly calculate the\n                gas density which is used in the conversion of the units of the Gaussian plume coupling\n            output_stacked (bool, optional): if true outputs as stacked np.array across sensors if not\n                outputs as dict\n            run_interpolation (bool, optional): logical indicating whether interpolation of the meteorological data to\n                the sensor/source is required. Defaults to True.\n\n        Returns:\n            plume_coupling (Union[list, np.ndarray, dict]): List of arrays, single array or dictionary containing the\n                plume coupling in hr/kg. When a single source object is passed in as input this function returns a list\n                or an array depending on the sensor type.\n                If a dictionary of sensor objects is passed in as input and output_stacked=False  this function returns\n                a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling\n                outputs for each sensor.\n                If a dictionary of sensor objects is passed in as input and output_stacked=True  this function returns\n                a np.array containing the stacked coupling matrices.\n\n        \"\"\"\n        if isinstance(sensor_object, SensorGroup):\n            output = {}\n            if isinstance(meteorology_object, Meteorology):\n                meteorology_object = dict.fromkeys(sensor_object.keys(), meteorology_object)\n            elif isinstance(meteorology_object, MeteorologyGroup):\n                run_interpolation = False\n\n            for sensor_key in sensor_object:\n                output[sensor_key] = self.compute_coupling_single_sensor(\n                    sensor_object=sensor_object[sensor_key],\n                    meteorology=meteorology_object[sensor_key],\n                    gas_object=gas_object,\n                    run_interpolation=run_interpolation,\n                )\n            if output_stacked:\n                output = np.concatenate(tuple(output.values()), axis=0)\n\n        elif isinstance(sensor_object, Sensor):\n            if isinstance(meteorology_object, MeteorologyGroup):\n                raise TypeError(\"Please provide a single Meteorology object when using a single Sensor object\")\n\n            output = self.compute_coupling_single_sensor(\n                sensor_object=sensor_object,\n                meteorology=meteorology_object,\n                gas_object=gas_object,\n                run_interpolation=run_interpolation,\n            )\n        else:\n            raise TypeError(\"Please provide either a Sensor or SensorGroup as input argument\")\n\n        return output\n\n    def compute_coupling_single_sensor(\n        self,\n        sensor_object: Sensor,\n        meteorology: Meteorology,\n        gas_object: GasSpecies = None,\n        run_interpolation: bool = True,\n    ) -&gt; Union[list, np.ndarray]:\n        \"\"\"Wrapper function to compute the gaussian plume coupling for a single sensor.\n\n        Wrapper is used to identify specific cases and calculate the Gaussian plume coupling accordingly.\n\n        When the sensor object contains the source_on attribute we set all coupling values to 0 for observations for\n        which source_on is False. Making sure the source_on is column array, aligning with the 1st dimension\n        (nof_observations) of the plume coupling array.\n\n        Args:\n            sensor_object (Sensor): Single sensor object which is used in the calculation of the plume coupling\n            meteorology (Meteorology): Meteorology object which is used in the calculation of the plume coupling\n            gas_object (GasSpecies, optional): Optionally input a gas species object to correctly calculate the\n                gas density which is used in the conversion of the units of the Gaussian plume coupling\n            run_interpolation (bool): logical indicating whether interpolation of the meteorological data to\n                the sensor/source is required. Default passed from compute_coupling.\n\n        Returns:\n            plume_coupling (Union[list, np.ndarray]): List of arrays or single array containing the plume coupling\n                in 1e6*[hr/kg]. Entries of the list are per source in the case of a satellite sensor, if a single array\n                is returned the coupling for each observation (first dimension) to each source (second dimension) is\n                provided.\n\n        \"\"\"\n        if not isinstance(sensor_object, Sensor):\n            raise NotImplementedError(\"Please provide a valid sensor type\")\n\n        (\n            gas_density,\n            u_interpolated,\n            v_interpolated,\n            wind_turbulence_horizontal,\n            wind_turbulence_vertical,\n        ) = self.interpolate_all_meteorology(\n            meteorology=meteorology,\n            sensor_object=sensor_object,\n            gas_object=gas_object,\n            run_interpolation=run_interpolation,\n        )\n\n        wind_speed = np.sqrt(u_interpolated**2 + v_interpolated**2)\n        theta = np.arctan2(v_interpolated, u_interpolated)\n\n        if isinstance(sensor_object, Satellite):\n            plume_coupling = self.compute_coupling_satellite(\n                sensor_object=sensor_object,\n                wind_speed=wind_speed,\n                theta=theta,\n                wind_turbulence_horizontal=wind_turbulence_horizontal,\n                wind_turbulence_vertical=wind_turbulence_vertical,\n                gas_density=gas_density,\n            )\n\n        else:\n            plume_coupling = self.compute_coupling_ground(\n                sensor_object=sensor_object,\n                wind_speed=wind_speed,\n                theta=theta,\n                wind_turbulence_horizontal=wind_turbulence_horizontal,\n                wind_turbulence_vertical=wind_turbulence_vertical,\n                gas_density=gas_density,\n            )\n\n        if sensor_object.source_on is not None:\n            plume_coupling = plume_coupling * sensor_object.source_on[:, None]\n\n        return plume_coupling\n\n    def compute_coupling_array(\n        self,\n        sensor_x: np.ndarray,\n        sensor_y: np.ndarray,\n        sensor_z: np.ndarray,\n        source_z: np.ndarray,\n        wind_speed: np.ndarray,\n        theta: np.ndarray,\n        wind_turbulence_horizontal: np.ndarray,\n        wind_turbulence_vertical: np.ndarray,\n        gas_density: Union[float, np.ndarray],\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the Gaussian plume coupling.\n\n        Most low level function to calculate the Gaussian plume coupling. Assuming input shapes are consistent but no\n        checking is done on this.\n\n        Setting sigma_vert to 1e-16 when it is identically zero (distance_x == 0) so we don't get a divide by 0 error\n        all the time.\n\n        Args:\n            sensor_x (np.ndarray): sensor x location relative to source [m].\n            sensor_y (np.ndarray): sensor y location relative to source [m].\n            sensor_z (np.ndarray): sensor z location relative to ground height [m].\n            source_z (np.ndarray): source z location relative to ground height [m].\n            wind_speed (np.ndarray): wind speed at source locations in [m/s].\n            theta (np.ndarray): Mathematical wind direction at source locations [radians]:\n                calculated as np.arctan2(v_component_wind, u_component_wind).\n            wind_turbulence_horizontal (np.ndarray): Horizontal wind turbulence [deg].\n            wind_turbulence_vertical (np.ndarray): Vertical wind turbulence [deg].\n            gas_density (Union[float, np.ndarray]): Gas density to use in coupling calculation [kg/m^3].\n\n        Returns:\n            plume_coupling (np.ndarray): Gaussian plume coupling in (1e6)*[hr/kg]: gives concentrations\n                in [ppm] when multiplied by sources in [kg/hr].\n\n        \"\"\"\n        cos_theta = np.cos(theta)\n        sin_theta = np.sin(theta)\n\n        distance_x = cos_theta * sensor_x + sin_theta * sensor_y\n        if np.all(distance_x &lt; 0):\n            return np.zeros_like(distance_x)\n\n        distance_y = -sin_theta * sensor_x + cos_theta * sensor_y\n\n        sigma_hor = np.tan(wind_turbulence_horizontal * (np.pi / 180)) * np.abs(distance_x) + self.source_half_width\n        sigma_vert = np.tan(wind_turbulence_vertical * (np.pi / 180)) * np.abs(distance_x)\n\n        sigma_vert[sigma_vert == 0] = 1e-16\n\n        plume_coupling = (\n            (1 / (2 * np.pi * wind_speed * sigma_hor * sigma_vert))\n            * np.exp(-0.5 * (distance_y / sigma_hor) ** 2)\n            * (\n                np.exp(-0.5 * (((sensor_z + source_z) / sigma_vert) ** 2))\n                + np.exp(-0.5 * (((sensor_z - source_z) / sigma_vert) ** 2))\n            )\n        )\n\n        plume_coupling = np.divide(np.multiply(plume_coupling, 1e6), (gas_density * 3600))\n        plume_coupling[np.logical_or(distance_x &lt; 0, plume_coupling &lt; self.minimum_contribution)] = 0\n\n        return plume_coupling\n\n    def calculate_gas_density(\n        self, meteorology: Meteorology, sensor_object: Sensor, gas_object: Union[GasSpecies, None]\n    ) -&gt; np.ndarray:\n        \"\"\"Helper function to calculate the gas density using ideal gas law.\n\n        https://en.wikipedia.org/wiki/Ideal_gas\n\n        When a gas object is passed as input we calculate the density according to that gas. We check if the\n        meteorology object has a temperature and/or pressure value and use those accordingly. Otherwise, we use Standard\n        Temperature and Pressure (STP).\n\n        We interpolate the temperature and pressure values to the source locations/times such that this is consistent\n        with the other calculations, i.e. we only do spatial interpolation when the sensor is a Satellite object\n        and temporal interpolation otherwise.\n\n        When no gas_object is passed in we just set the gas density value to 1.\n\n        Args:\n            meteorology (Meteorology): Meteorology object potentially containing temperature or pressure values\n            sensor_object (Sensor): Sensor object containing information about where to interpolate to\n            gas_object (Union[GasSpecies, None]): Gas species object which actually calculates the correct density\n\n        Returns:\n            gas_density (np.ndarray): Numpy array of shape [1 x nof_sources] (Satellite sensor)\n                or [nof_observations x 1] (otherwise) containing the gas density values to use\n\n        \"\"\"\n        if not isinstance(gas_object, GasSpecies):\n            if isinstance(sensor_object, Satellite):\n                return np.ones((1, self.source_map.nof_sources))\n            return np.ones((sensor_object.nof_observations, 1))\n\n        temperature_interpolated = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"temperature\", sensor_object=sensor_object\n        )\n        if temperature_interpolated is None:\n            temperature_interpolated = np.array([[273.15]])\n\n        pressure_interpolated = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"pressure\", sensor_object=sensor_object\n        )\n        if pressure_interpolated is None:\n            pressure_interpolated = np.array([[101.325]])\n\n        gas_density = gas_object.gas_density(temperature=temperature_interpolated, pressure=pressure_interpolated)\n\n        return gas_density\n\n    def interpolate_all_meteorology(\n        self, sensor_object: Sensor, meteorology: Meteorology, gas_object: GasSpecies, run_interpolation: bool\n    ):\n        \"\"\"Function which carries out interpolation of all meteorological information.\n\n        The flag run_interpolation determines whether the interpolation should be carried out. If this\n        is set to be False, the meteorological parameters are simply set to the values stored on the\n        meteorology object (i.e. we assume that the meteorology has already been interpolated). This\n        functionality is required to avoid wasted computation in the case of e.g. a reversible jump run.\n\n        Args:\n            sensor_object (Sensor): object containing locations/times onto which met information should\n                be interpolated.\n            meteorology (Meteorology): object containing meteorology information for interpolation.\n            gas_object (GasSpecies): object containing gas information.\n            run_interpolation (bool): logical indicating whether the meteorology information needs to be interpolated.\n\n        Returns:\n            gas_density (np.ndarray): numpy array of shape [n_data x 1] of gas densities.\n            u_interpolated (np.ndarray): numpy array of shape [n_data x 1] of northerly wind components.\n            v_interpolated (np.ndarray): numpy array of shape [n_data x 1] of easterly wind components.\n            wind_turbulence_horizontal (np.ndarray): numpy array of shape [n_data x 1] of horizontal turbulence\n                parameters.\n            wind_turbulence_vertical (np.ndarray): numpy array of shape [n_data x 1] of vertical turbulence\n                parameters.\n\n        \"\"\"\n        if run_interpolation:\n            gas_density = self.calculate_gas_density(\n                meteorology=meteorology, sensor_object=sensor_object, gas_object=gas_object\n            )\n            u_interpolated = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"u_component\", sensor_object=sensor_object\n            )\n            v_interpolated = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"v_component\", sensor_object=sensor_object\n            )\n            wind_turbulence_horizontal = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"wind_turbulence_horizontal\", sensor_object=sensor_object\n            )\n            wind_turbulence_vertical = self.interpolate_meteorology(\n                meteorology=meteorology, variable_name=\"wind_turbulence_vertical\", sensor_object=sensor_object\n            )\n        else:\n            gas_density = gas_object.gas_density(temperature=meteorology.temperature, pressure=meteorology.pressure)\n            gas_density = gas_density.reshape((gas_density.size, 1))\n            u_interpolated = meteorology.u_component.reshape((meteorology.u_component.size, 1))\n            v_interpolated = meteorology.v_component.reshape((meteorology.v_component.size, 1))\n            wind_turbulence_horizontal = meteorology.wind_turbulence_horizontal.reshape(\n                (meteorology.wind_turbulence_horizontal.size, 1)\n            )\n            wind_turbulence_vertical = meteorology.wind_turbulence_vertical.reshape(\n                (meteorology.wind_turbulence_vertical.size, 1)\n            )\n\n        return gas_density, u_interpolated, v_interpolated, wind_turbulence_horizontal, wind_turbulence_vertical\n\n    def interpolate_meteorology(\n        self, meteorology: Meteorology, variable_name: str, sensor_object: Sensor\n    ) -&gt; Union[np.ndarray, None]:\n        \"\"\"Helper function to interpolate meteorology variables.\n\n        This function interpolates meteorological variables to times in Sensor or Sources in sourcemap. It also\n        calculates the wind speed and mathematical angle between the u- and v-components which in turn gets used in the\n        calculation of the Gaussian plume.\n\n        When the input sensor object is a Satellite type we use spatial interpolation using the interpolation method\n        from the coordinate system class as this takes care of the coordinate systems.\n        When the input sensor object is of another time we use temporal interpolation (assumption is spatial uniformity\n        for all observations over a small(er) area).\n\n        Args:\n            meteorology (Meteorology): Meteorology object containing u- and v-components of wind including their\n                spatial location\n            variable_name (str): String name of an attribute in the meteorology input object which needs to be\n                interpolated\n            sensor_object (Sensor): Sensor object containing information about where to interpolate to\n\n        Returns:\n            variable_interpolated (np.ndarray): Interpolated values\n\n        \"\"\"\n        variable = getattr(meteorology, variable_name)\n        if variable is None:\n            return None\n\n        if isinstance(sensor_object, Satellite):\n            variable_interpolated = meteorology.location.interpolate(variable, self.source_map.location)\n            variable_interpolated = variable_interpolated.reshape(1, self.source_map.nof_sources)\n        else:\n            variable_interpolated = sti.interpolate(\n                time_in=meteorology.time, values_in=variable, time_out=sensor_object.time\n            )\n            variable_interpolated = variable_interpolated.reshape(sensor_object.nof_observations, 1)\n        return variable_interpolated\n\n    def compute_coupling_satellite(\n        self,\n        sensor_object: Sensor,\n        wind_speed: np.ndarray,\n        theta: np.ndarray,\n        wind_turbulence_horizontal: np.ndarray,\n        wind_turbulence_vertical: np.ndarray,\n        gas_density: np.ndarray,\n    ) -&gt; list:\n        \"\"\"Compute Gaussian plume coupling for satellite sensor.\n\n        When the sensor is a Satellite object we calculate the plume coupling per source. Given the large number of\n        sources and the possibility of using the inclusion radius and inclusion indices here and validity of a local\n        ENU system over large distances we loop over each source and calculate the coupling on a per-source basis.\n\n        If source_map.inclusion_n_obs is None, we do not do any filtering on observations and we want to include all\n        observations in the plume coupling calculations.\n\n        All np.ndarray inputs should have a shape of [1 x nof_sources]\n\n        Args:\n            sensor_object (Sensor): Sensor object used in plume coupling calculation\n            wind_speed (np.ndarray): Wind speed [m/s]\n            theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n            wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n            wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n            gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n        Returns:\n            plume_coupling (list): List of Gaussian plume coupling 1e6*[hr/kg] arrays. The list has a length of\n                nof_sources, each array has the shape [nof_observations x 1] or [inclusion_n_obs x 1] when\n                inclusion_idx is used.\n\n        \"\"\"\n        plume_coupling = []\n\n        source_map_location_lla = self.source_map.location.to_lla()\n        for current_source in range(self.source_map.nof_sources):\n            if self.source_map.inclusion_n_obs is None:\n                enu_sensor_array = sensor_object.location.to_enu(\n                    ref_latitude=source_map_location_lla.latitude[current_source],\n                    ref_longitude=source_map_location_lla.longitude[current_source],\n                    ref_altitude=0,\n                ).to_array()\n\n            else:\n                if self.source_map.inclusion_n_obs[current_source] == 0:\n                    plume_coupling.append(np.array([]))\n                    continue\n\n                enu_sensor_array = _create_enu_sensor_array(\n                    inclusion_idx=self.source_map.inclusion_idx[current_source],\n                    sensor_object=sensor_object,\n                    source_map_location_lla=source_map_location_lla,\n                    current_source=current_source,\n                )\n\n            temp_coupling = self.compute_coupling_array(\n                enu_sensor_array[:, [0]],\n                enu_sensor_array[:, [1]],\n                enu_sensor_array[:, [2]],\n                source_map_location_lla.altitude[current_source],\n                wind_speed[:, current_source],\n                theta[:, current_source],\n                wind_turbulence_horizontal[:, current_source],\n                wind_turbulence_vertical[:, current_source],\n                gas_density[:, current_source],\n            )\n\n            plume_coupling.append(temp_coupling)\n\n        return plume_coupling\n\n    def compute_coupling_ground(\n        self,\n        sensor_object: Sensor,\n        wind_speed: np.ndarray,\n        theta: np.ndarray,\n        wind_turbulence_horizontal: np.ndarray,\n        wind_turbulence_vertical: np.ndarray,\n        gas_density: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute Gaussian plume coupling for a ground sensor.\n\n        If the source map is already defined as ENU the reference location is maintained but the sensor is checked\n        to make sure the same reference location is used. Otherwise, when converting to ENU object for the sensor\n        observations we use a single source and altitude 0 as the reference location. This way our ENU system is a\n        system w.r.t. ground level which is required for the current implementation of the actual coupling calculation.\n\n        When the sensor is a Beam object we calculate the plume coupling for all sources to all beam knot locations at\n        once in the same ENU coordinate system and finally averaged over the beam knots to get the final output.\n\n        In general, we calculate the coupling from all sources to all sensor observation locations. In order to achieve\n        this we input the sensor array as column and source array as row vector in calculating relative x etc.,\n        with the beam knot locations being the third dimension. When the sensor is a single point Sensor or a Drone\n        sensor we effectively have one beam knot, making the mean operation at the end effectively a reshape operation\n        which gets rid of the third dimension.\n\n        All np.ndarray inputs should have a shape of [nof_observations x 1]\n\n        Args:\n            sensor_object (Sensor): Sensor object used in plume coupling calculation\n            wind_speed (np.ndarray): Wind speed [m/s]\n            theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n            wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n            wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n            gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n        Returns:\n            plume_coupling (np.ndarray): Gaussian plume coupling 1e6*[hr/kg] array. The array has the\n                shape [nof_observations x nof_sources]\n\n        \"\"\"\n        if not isinstance(self.source_map.location, ENU):\n            source_map_lla = self.source_map.location.to_lla()\n            source_map_enu = source_map_lla.to_enu(\n                ref_latitude=source_map_lla.latitude[0], ref_longitude=source_map_lla.longitude[0], ref_altitude=0\n            )\n        else:\n            source_map_enu = self.source_map.location\n\n        enu_source_array = source_map_enu.to_array()\n\n        if isinstance(sensor_object, Beam):\n            enu_sensor_array = sensor_object.make_beam_knots(\n                ref_latitude=source_map_enu.ref_latitude,\n                ref_longitude=source_map_enu.ref_longitude,\n                ref_altitude=source_map_enu.ref_altitude,\n            )\n            relative_x = np.subtract(enu_sensor_array[:, 0][None, None, :], enu_source_array[:, 0][None, :, None])\n            relative_y = np.subtract(enu_sensor_array[:, 1][None, None, :], enu_source_array[:, 1][None, :, None])\n            z_sensor = enu_sensor_array[:, 2][None, None, :]\n        else:\n            enu_sensor_array = sensor_object.location.to_enu(\n                ref_latitude=source_map_enu.ref_latitude,\n                ref_longitude=source_map_enu.ref_longitude,\n                ref_altitude=source_map_enu.ref_altitude,\n            ).to_array()\n            relative_x = np.subtract(enu_sensor_array[:, 0][:, None, None], enu_source_array[:, 0][None, :, None])\n            relative_y = np.subtract(enu_sensor_array[:, 1][:, None, None], enu_source_array[:, 1][None, :, None])\n            z_sensor = enu_sensor_array[:, 2][:, None, None]\n\n        z_source = enu_source_array[:, 2][None, :, None]\n\n        plume_coupling = self.compute_coupling_array(\n            relative_x,\n            relative_y,\n            z_sensor,\n            z_source,\n            wind_speed[:, :, None],\n            theta[:, :, None],\n            wind_turbulence_horizontal[:, :, None],\n            wind_turbulence_vertical[:, :, None],\n            gas_density[:, :, None],\n        )\n\n        plume_coupling = plume_coupling.mean(axis=2)\n\n        return plume_coupling\n\n    @staticmethod\n    def compute_coverage(\n        couplings: np.ndarray, threshold_function: Callable, coverage_threshold: float = 6, **kwargs\n    ) -&gt; Union[np.ndarray, dict]:\n        \"\"\"Returns a logical vector that indicates which sources in the couplings are, or are not, within the coverage.\n\n        The 'coverage' is the area inside which all sources are well covered by wind data. E.g. If wind exclusively\n        blows towards East, then all sources to the East of any sensor are 'invisible', and are not within the coverage.\n\n        Couplings are returned in hr/kg. Some threshold function defines the largest allowed coupling value. This is\n        used to calculate estimated emission rates in kg/hr. Any emissions which are greater than the value of\n        'coverage_threshold' are defined as not within the coverage.\n\n        Args:\n            couplings (np.ndarray): Array of coupling values. Dimensions: n_datapoints x n_sources.\n            threshold_function (Callable): Callable function which returns some single value that defines the\n                maximum or 'threshold' coupling. For example: np.quantile(., q=0.95)\n            coverage_threshold (float, optional): The threshold value of the estimated emission rate which is\n                considered to be within the coverage. Defaults to 6 kg/hr.\n            kwargs (dict, optional): Keyword arguments required for the threshold function.\n\n        Returns:\n            coverage (Union[np.ndarray, dict]): A logical array specifying which sources are within the coverage.\n\n        \"\"\"\n        coupling_threshold = threshold_function(couplings, **kwargs)\n        no_warning_threshold = np.where(coupling_threshold &lt;= 1e-100, 1, coupling_threshold)\n        no_warning_estimated_emission_rates = np.where(coupling_threshold &lt;= 1e-100, np.inf, 1 / no_warning_threshold)\n        coverage = no_warning_estimated_emission_rates &lt; coverage_threshold\n\n        return coverage\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling","title":"<code>compute_coupling(sensor_object, meteorology_object, gas_object=None, output_stacked=False, run_interpolation=True)</code>","text":"<p>Top level function to calculate the Gaussian plume coupling.</p> <p>Calculates the coupling for either a single sensor object or a dictionary of sensor objects.</p> <p>When both a SensorGroup and a MeteorologyGroup have been passed in, we assume they are consistent and contain exactly the same keys for each item in both groups. Also assuming interpolation has been performed and time axes are consistent, so we set run_interpolation to False</p> <p>When you input a SensorGroup and a single Meteorology object we convert this object into a dictionary, so we don't have to duplicate the same code.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Single sensor object or SensorGroup object which is used in the calculation of the plume coupling.</p> required <code>meteorology_object</code> <code>Union[MeteorologyGroup, Meteorology]</code> <p>Meteorology object or MeteorologyGroup object which is used in the calculation of the plume coupling.</p> required <code>gas_object</code> <code>GasSpecies</code> <p>Optional input, a gas species object to correctly calculate the gas density which is used in the conversion of the units of the Gaussian plume coupling</p> <code>None</code> <code>output_stacked</code> <code>bool</code> <p>if true outputs as stacked np.array across sensors if not outputs as dict</p> <code>False</code> <code>run_interpolation</code> <code>bool</code> <p>logical indicating whether interpolation of the meteorological data to the sensor/source is required. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>Union[list, ndarray, dict]</code> <p>List of arrays, single array or dictionary containing the plume coupling in hr/kg. When a single source object is passed in as input this function returns a list or an array depending on the sensor type. If a dictionary of sensor objects is passed in as input and output_stacked=False  this function returns a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling outputs for each sensor. If a dictionary of sensor objects is passed in as input and output_stacked=True  this function returns a np.array containing the stacked coupling matrices.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling(\n    self,\n    sensor_object: Union[SensorGroup, Sensor],\n    meteorology_object: Union[MeteorologyGroup, Meteorology],\n    gas_object: GasSpecies = None,\n    output_stacked: bool = False,\n    run_interpolation: bool = True,\n) -&gt; Union[list, np.ndarray, dict]:\n    \"\"\"Top level function to calculate the Gaussian plume coupling.\n\n    Calculates the coupling for either a single sensor object or a dictionary of sensor objects.\n\n    When both a SensorGroup and a MeteorologyGroup have been passed in, we assume they are consistent and contain\n    exactly the same keys for each item in both groups. Also assuming interpolation has been performed and time axes\n    are consistent, so we set run_interpolation to False\n\n    When you input a SensorGroup and a single Meteorology object we convert this object into a dictionary, so we\n    don't have to duplicate the same code.\n\n    Args:\n        sensor_object (Union[SensorGroup, Sensor]): Single sensor object or SensorGroup object which is used in the\n            calculation of the plume coupling.\n        meteorology_object (Union[MeteorologyGroup, Meteorology]): Meteorology object or MeteorologyGroup object\n            which is used in the calculation of the plume coupling.\n        gas_object (GasSpecies, optional): Optional input, a gas species object to correctly calculate the\n            gas density which is used in the conversion of the units of the Gaussian plume coupling\n        output_stacked (bool, optional): if true outputs as stacked np.array across sensors if not\n            outputs as dict\n        run_interpolation (bool, optional): logical indicating whether interpolation of the meteorological data to\n            the sensor/source is required. Defaults to True.\n\n    Returns:\n        plume_coupling (Union[list, np.ndarray, dict]): List of arrays, single array or dictionary containing the\n            plume coupling in hr/kg. When a single source object is passed in as input this function returns a list\n            or an array depending on the sensor type.\n            If a dictionary of sensor objects is passed in as input and output_stacked=False  this function returns\n            a dictionary consistent with the input dictionary keys, containing the corresponding plume coupling\n            outputs for each sensor.\n            If a dictionary of sensor objects is passed in as input and output_stacked=True  this function returns\n            a np.array containing the stacked coupling matrices.\n\n    \"\"\"\n    if isinstance(sensor_object, SensorGroup):\n        output = {}\n        if isinstance(meteorology_object, Meteorology):\n            meteorology_object = dict.fromkeys(sensor_object.keys(), meteorology_object)\n        elif isinstance(meteorology_object, MeteorologyGroup):\n            run_interpolation = False\n\n        for sensor_key in sensor_object:\n            output[sensor_key] = self.compute_coupling_single_sensor(\n                sensor_object=sensor_object[sensor_key],\n                meteorology=meteorology_object[sensor_key],\n                gas_object=gas_object,\n                run_interpolation=run_interpolation,\n            )\n        if output_stacked:\n            output = np.concatenate(tuple(output.values()), axis=0)\n\n    elif isinstance(sensor_object, Sensor):\n        if isinstance(meteorology_object, MeteorologyGroup):\n            raise TypeError(\"Please provide a single Meteorology object when using a single Sensor object\")\n\n        output = self.compute_coupling_single_sensor(\n            sensor_object=sensor_object,\n            meteorology=meteorology_object,\n            gas_object=gas_object,\n            run_interpolation=run_interpolation,\n        )\n    else:\n        raise TypeError(\"Please provide either a Sensor or SensorGroup as input argument\")\n\n    return output\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_single_sensor","title":"<code>compute_coupling_single_sensor(sensor_object, meteorology, gas_object=None, run_interpolation=True)</code>","text":"<p>Wrapper function to compute the gaussian plume coupling for a single sensor.</p> <p>Wrapper is used to identify specific cases and calculate the Gaussian plume coupling accordingly.</p> <p>When the sensor object contains the source_on attribute we set all coupling values to 0 for observations for which source_on is False. Making sure the source_on is column array, aligning with the 1st dimension (nof_observations) of the plume coupling array.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Single sensor object which is used in the calculation of the plume coupling</p> required <code>meteorology</code> <code>Meteorology</code> <p>Meteorology object which is used in the calculation of the plume coupling</p> required <code>gas_object</code> <code>GasSpecies</code> <p>Optionally input a gas species object to correctly calculate the gas density which is used in the conversion of the units of the Gaussian plume coupling</p> <code>None</code> <code>run_interpolation</code> <code>bool</code> <p>logical indicating whether interpolation of the meteorological data to the sensor/source is required. Default passed from compute_coupling.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>Union[list, ndarray]</code> <p>List of arrays or single array containing the plume coupling in 1e6*[hr/kg]. Entries of the list are per source in the case of a satellite sensor, if a single array is returned the coupling for each observation (first dimension) to each source (second dimension) is provided.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_single_sensor(\n    self,\n    sensor_object: Sensor,\n    meteorology: Meteorology,\n    gas_object: GasSpecies = None,\n    run_interpolation: bool = True,\n) -&gt; Union[list, np.ndarray]:\n    \"\"\"Wrapper function to compute the gaussian plume coupling for a single sensor.\n\n    Wrapper is used to identify specific cases and calculate the Gaussian plume coupling accordingly.\n\n    When the sensor object contains the source_on attribute we set all coupling values to 0 for observations for\n    which source_on is False. Making sure the source_on is column array, aligning with the 1st dimension\n    (nof_observations) of the plume coupling array.\n\n    Args:\n        sensor_object (Sensor): Single sensor object which is used in the calculation of the plume coupling\n        meteorology (Meteorology): Meteorology object which is used in the calculation of the plume coupling\n        gas_object (GasSpecies, optional): Optionally input a gas species object to correctly calculate the\n            gas density which is used in the conversion of the units of the Gaussian plume coupling\n        run_interpolation (bool): logical indicating whether interpolation of the meteorological data to\n            the sensor/source is required. Default passed from compute_coupling.\n\n    Returns:\n        plume_coupling (Union[list, np.ndarray]): List of arrays or single array containing the plume coupling\n            in 1e6*[hr/kg]. Entries of the list are per source in the case of a satellite sensor, if a single array\n            is returned the coupling for each observation (first dimension) to each source (second dimension) is\n            provided.\n\n    \"\"\"\n    if not isinstance(sensor_object, Sensor):\n        raise NotImplementedError(\"Please provide a valid sensor type\")\n\n    (\n        gas_density,\n        u_interpolated,\n        v_interpolated,\n        wind_turbulence_horizontal,\n        wind_turbulence_vertical,\n    ) = self.interpolate_all_meteorology(\n        meteorology=meteorology,\n        sensor_object=sensor_object,\n        gas_object=gas_object,\n        run_interpolation=run_interpolation,\n    )\n\n    wind_speed = np.sqrt(u_interpolated**2 + v_interpolated**2)\n    theta = np.arctan2(v_interpolated, u_interpolated)\n\n    if isinstance(sensor_object, Satellite):\n        plume_coupling = self.compute_coupling_satellite(\n            sensor_object=sensor_object,\n            wind_speed=wind_speed,\n            theta=theta,\n            wind_turbulence_horizontal=wind_turbulence_horizontal,\n            wind_turbulence_vertical=wind_turbulence_vertical,\n            gas_density=gas_density,\n        )\n\n    else:\n        plume_coupling = self.compute_coupling_ground(\n            sensor_object=sensor_object,\n            wind_speed=wind_speed,\n            theta=theta,\n            wind_turbulence_horizontal=wind_turbulence_horizontal,\n            wind_turbulence_vertical=wind_turbulence_vertical,\n            gas_density=gas_density,\n        )\n\n    if sensor_object.source_on is not None:\n        plume_coupling = plume_coupling * sensor_object.source_on[:, None]\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_array","title":"<code>compute_coupling_array(sensor_x, sensor_y, sensor_z, source_z, wind_speed, theta, wind_turbulence_horizontal, wind_turbulence_vertical, gas_density)</code>","text":"<p>Compute the Gaussian plume coupling.</p> <p>Most low level function to calculate the Gaussian plume coupling. Assuming input shapes are consistent but no checking is done on this.</p> <p>Setting sigma_vert to 1e-16 when it is identically zero (distance_x == 0) so we don't get a divide by 0 error all the time.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_x</code> <code>ndarray</code> <p>sensor x location relative to source [m].</p> required <code>sensor_y</code> <code>ndarray</code> <p>sensor y location relative to source [m].</p> required <code>sensor_z</code> <code>ndarray</code> <p>sensor z location relative to ground height [m].</p> required <code>source_z</code> <code>ndarray</code> <p>source z location relative to ground height [m].</p> required <code>wind_speed</code> <code>ndarray</code> <p>wind speed at source locations in [m/s].</p> required <code>theta</code> <code>ndarray</code> <p>Mathematical wind direction at source locations [radians]: calculated as np.arctan2(v_component_wind, u_component_wind).</p> required <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Horizontal wind turbulence [deg].</p> required <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Vertical wind turbulence [deg].</p> required <code>gas_density</code> <code>Union[float, ndarray]</code> <p>Gas density to use in coupling calculation [kg/m^3].</p> required <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>ndarray</code> <p>Gaussian plume coupling in (1e6)*[hr/kg]: gives concentrations in [ppm] when multiplied by sources in [kg/hr].</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_array(\n    self,\n    sensor_x: np.ndarray,\n    sensor_y: np.ndarray,\n    sensor_z: np.ndarray,\n    source_z: np.ndarray,\n    wind_speed: np.ndarray,\n    theta: np.ndarray,\n    wind_turbulence_horizontal: np.ndarray,\n    wind_turbulence_vertical: np.ndarray,\n    gas_density: Union[float, np.ndarray],\n) -&gt; np.ndarray:\n    \"\"\"Compute the Gaussian plume coupling.\n\n    Most low level function to calculate the Gaussian plume coupling. Assuming input shapes are consistent but no\n    checking is done on this.\n\n    Setting sigma_vert to 1e-16 when it is identically zero (distance_x == 0) so we don't get a divide by 0 error\n    all the time.\n\n    Args:\n        sensor_x (np.ndarray): sensor x location relative to source [m].\n        sensor_y (np.ndarray): sensor y location relative to source [m].\n        sensor_z (np.ndarray): sensor z location relative to ground height [m].\n        source_z (np.ndarray): source z location relative to ground height [m].\n        wind_speed (np.ndarray): wind speed at source locations in [m/s].\n        theta (np.ndarray): Mathematical wind direction at source locations [radians]:\n            calculated as np.arctan2(v_component_wind, u_component_wind).\n        wind_turbulence_horizontal (np.ndarray): Horizontal wind turbulence [deg].\n        wind_turbulence_vertical (np.ndarray): Vertical wind turbulence [deg].\n        gas_density (Union[float, np.ndarray]): Gas density to use in coupling calculation [kg/m^3].\n\n    Returns:\n        plume_coupling (np.ndarray): Gaussian plume coupling in (1e6)*[hr/kg]: gives concentrations\n            in [ppm] when multiplied by sources in [kg/hr].\n\n    \"\"\"\n    cos_theta = np.cos(theta)\n    sin_theta = np.sin(theta)\n\n    distance_x = cos_theta * sensor_x + sin_theta * sensor_y\n    if np.all(distance_x &lt; 0):\n        return np.zeros_like(distance_x)\n\n    distance_y = -sin_theta * sensor_x + cos_theta * sensor_y\n\n    sigma_hor = np.tan(wind_turbulence_horizontal * (np.pi / 180)) * np.abs(distance_x) + self.source_half_width\n    sigma_vert = np.tan(wind_turbulence_vertical * (np.pi / 180)) * np.abs(distance_x)\n\n    sigma_vert[sigma_vert == 0] = 1e-16\n\n    plume_coupling = (\n        (1 / (2 * np.pi * wind_speed * sigma_hor * sigma_vert))\n        * np.exp(-0.5 * (distance_y / sigma_hor) ** 2)\n        * (\n            np.exp(-0.5 * (((sensor_z + source_z) / sigma_vert) ** 2))\n            + np.exp(-0.5 * (((sensor_z - source_z) / sigma_vert) ** 2))\n        )\n    )\n\n    plume_coupling = np.divide(np.multiply(plume_coupling, 1e6), (gas_density * 3600))\n    plume_coupling[np.logical_or(distance_x &lt; 0, plume_coupling &lt; self.minimum_contribution)] = 0\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.calculate_gas_density","title":"<code>calculate_gas_density(meteorology, sensor_object, gas_object)</code>","text":"<p>Helper function to calculate the gas density using ideal gas law.</p> <p>https://en.wikipedia.org/wiki/Ideal_gas</p> <p>When a gas object is passed as input we calculate the density according to that gas. We check if the meteorology object has a temperature and/or pressure value and use those accordingly. Otherwise, we use Standard Temperature and Pressure (STP).</p> <p>We interpolate the temperature and pressure values to the source locations/times such that this is consistent with the other calculations, i.e. we only do spatial interpolation when the sensor is a Satellite object and temporal interpolation otherwise.</p> <p>When no gas_object is passed in we just set the gas density value to 1.</p> <p>Parameters:</p> Name Type Description Default <code>meteorology</code> <code>Meteorology</code> <p>Meteorology object potentially containing temperature or pressure values</p> required <code>sensor_object</code> <code>Sensor</code> <p>Sensor object containing information about where to interpolate to</p> required <code>gas_object</code> <code>Union[GasSpecies, None]</code> <p>Gas species object which actually calculates the correct density</p> required <p>Returns:</p> Name Type Description <code>gas_density</code> <code>ndarray</code> <p>Numpy array of shape [1 x nof_sources] (Satellite sensor) or [nof_observations x 1] (otherwise) containing the gas density values to use</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def calculate_gas_density(\n    self, meteorology: Meteorology, sensor_object: Sensor, gas_object: Union[GasSpecies, None]\n) -&gt; np.ndarray:\n    \"\"\"Helper function to calculate the gas density using ideal gas law.\n\n    https://en.wikipedia.org/wiki/Ideal_gas\n\n    When a gas object is passed as input we calculate the density according to that gas. We check if the\n    meteorology object has a temperature and/or pressure value and use those accordingly. Otherwise, we use Standard\n    Temperature and Pressure (STP).\n\n    We interpolate the temperature and pressure values to the source locations/times such that this is consistent\n    with the other calculations, i.e. we only do spatial interpolation when the sensor is a Satellite object\n    and temporal interpolation otherwise.\n\n    When no gas_object is passed in we just set the gas density value to 1.\n\n    Args:\n        meteorology (Meteorology): Meteorology object potentially containing temperature or pressure values\n        sensor_object (Sensor): Sensor object containing information about where to interpolate to\n        gas_object (Union[GasSpecies, None]): Gas species object which actually calculates the correct density\n\n    Returns:\n        gas_density (np.ndarray): Numpy array of shape [1 x nof_sources] (Satellite sensor)\n            or [nof_observations x 1] (otherwise) containing the gas density values to use\n\n    \"\"\"\n    if not isinstance(gas_object, GasSpecies):\n        if isinstance(sensor_object, Satellite):\n            return np.ones((1, self.source_map.nof_sources))\n        return np.ones((sensor_object.nof_observations, 1))\n\n    temperature_interpolated = self.interpolate_meteorology(\n        meteorology=meteorology, variable_name=\"temperature\", sensor_object=sensor_object\n    )\n    if temperature_interpolated is None:\n        temperature_interpolated = np.array([[273.15]])\n\n    pressure_interpolated = self.interpolate_meteorology(\n        meteorology=meteorology, variable_name=\"pressure\", sensor_object=sensor_object\n    )\n    if pressure_interpolated is None:\n        pressure_interpolated = np.array([[101.325]])\n\n    gas_density = gas_object.gas_density(temperature=temperature_interpolated, pressure=pressure_interpolated)\n\n    return gas_density\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.interpolate_all_meteorology","title":"<code>interpolate_all_meteorology(sensor_object, meteorology, gas_object, run_interpolation)</code>","text":"<p>Function which carries out interpolation of all meteorological information.</p> <p>The flag run_interpolation determines whether the interpolation should be carried out. If this is set to be False, the meteorological parameters are simply set to the values stored on the meteorology object (i.e. we assume that the meteorology has already been interpolated). This functionality is required to avoid wasted computation in the case of e.g. a reversible jump run.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>object containing locations/times onto which met information should be interpolated.</p> required <code>meteorology</code> <code>Meteorology</code> <p>object containing meteorology information for interpolation.</p> required <code>gas_object</code> <code>GasSpecies</code> <p>object containing gas information.</p> required <code>run_interpolation</code> <code>bool</code> <p>logical indicating whether the meteorology information needs to be interpolated.</p> required <p>Returns:</p> Name Type Description <code>gas_density</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of gas densities.</p> <code>u_interpolated</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of northerly wind components.</p> <code>v_interpolated</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of easterly wind components.</p> <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of horizontal turbulence parameters.</p> <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>numpy array of shape [n_data x 1] of vertical turbulence parameters.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def interpolate_all_meteorology(\n    self, sensor_object: Sensor, meteorology: Meteorology, gas_object: GasSpecies, run_interpolation: bool\n):\n    \"\"\"Function which carries out interpolation of all meteorological information.\n\n    The flag run_interpolation determines whether the interpolation should be carried out. If this\n    is set to be False, the meteorological parameters are simply set to the values stored on the\n    meteorology object (i.e. we assume that the meteorology has already been interpolated). This\n    functionality is required to avoid wasted computation in the case of e.g. a reversible jump run.\n\n    Args:\n        sensor_object (Sensor): object containing locations/times onto which met information should\n            be interpolated.\n        meteorology (Meteorology): object containing meteorology information for interpolation.\n        gas_object (GasSpecies): object containing gas information.\n        run_interpolation (bool): logical indicating whether the meteorology information needs to be interpolated.\n\n    Returns:\n        gas_density (np.ndarray): numpy array of shape [n_data x 1] of gas densities.\n        u_interpolated (np.ndarray): numpy array of shape [n_data x 1] of northerly wind components.\n        v_interpolated (np.ndarray): numpy array of shape [n_data x 1] of easterly wind components.\n        wind_turbulence_horizontal (np.ndarray): numpy array of shape [n_data x 1] of horizontal turbulence\n            parameters.\n        wind_turbulence_vertical (np.ndarray): numpy array of shape [n_data x 1] of vertical turbulence\n            parameters.\n\n    \"\"\"\n    if run_interpolation:\n        gas_density = self.calculate_gas_density(\n            meteorology=meteorology, sensor_object=sensor_object, gas_object=gas_object\n        )\n        u_interpolated = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"u_component\", sensor_object=sensor_object\n        )\n        v_interpolated = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"v_component\", sensor_object=sensor_object\n        )\n        wind_turbulence_horizontal = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"wind_turbulence_horizontal\", sensor_object=sensor_object\n        )\n        wind_turbulence_vertical = self.interpolate_meteorology(\n            meteorology=meteorology, variable_name=\"wind_turbulence_vertical\", sensor_object=sensor_object\n        )\n    else:\n        gas_density = gas_object.gas_density(temperature=meteorology.temperature, pressure=meteorology.pressure)\n        gas_density = gas_density.reshape((gas_density.size, 1))\n        u_interpolated = meteorology.u_component.reshape((meteorology.u_component.size, 1))\n        v_interpolated = meteorology.v_component.reshape((meteorology.v_component.size, 1))\n        wind_turbulence_horizontal = meteorology.wind_turbulence_horizontal.reshape(\n            (meteorology.wind_turbulence_horizontal.size, 1)\n        )\n        wind_turbulence_vertical = meteorology.wind_turbulence_vertical.reshape(\n            (meteorology.wind_turbulence_vertical.size, 1)\n        )\n\n    return gas_density, u_interpolated, v_interpolated, wind_turbulence_horizontal, wind_turbulence_vertical\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.interpolate_meteorology","title":"<code>interpolate_meteorology(meteorology, variable_name, sensor_object)</code>","text":"<p>Helper function to interpolate meteorology variables.</p> <p>This function interpolates meteorological variables to times in Sensor or Sources in sourcemap. It also calculates the wind speed and mathematical angle between the u- and v-components which in turn gets used in the calculation of the Gaussian plume.</p> <p>When the input sensor object is a Satellite type we use spatial interpolation using the interpolation method from the coordinate system class as this takes care of the coordinate systems. When the input sensor object is of another time we use temporal interpolation (assumption is spatial uniformity for all observations over a small(er) area).</p> <p>Parameters:</p> Name Type Description Default <code>meteorology</code> <code>Meteorology</code> <p>Meteorology object containing u- and v-components of wind including their spatial location</p> required <code>variable_name</code> <code>str</code> <p>String name of an attribute in the meteorology input object which needs to be interpolated</p> required <code>sensor_object</code> <code>Sensor</code> <p>Sensor object containing information about where to interpolate to</p> required <p>Returns:</p> Name Type Description <code>variable_interpolated</code> <code>ndarray</code> <p>Interpolated values</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def interpolate_meteorology(\n    self, meteorology: Meteorology, variable_name: str, sensor_object: Sensor\n) -&gt; Union[np.ndarray, None]:\n    \"\"\"Helper function to interpolate meteorology variables.\n\n    This function interpolates meteorological variables to times in Sensor or Sources in sourcemap. It also\n    calculates the wind speed and mathematical angle between the u- and v-components which in turn gets used in the\n    calculation of the Gaussian plume.\n\n    When the input sensor object is a Satellite type we use spatial interpolation using the interpolation method\n    from the coordinate system class as this takes care of the coordinate systems.\n    When the input sensor object is of another time we use temporal interpolation (assumption is spatial uniformity\n    for all observations over a small(er) area).\n\n    Args:\n        meteorology (Meteorology): Meteorology object containing u- and v-components of wind including their\n            spatial location\n        variable_name (str): String name of an attribute in the meteorology input object which needs to be\n            interpolated\n        sensor_object (Sensor): Sensor object containing information about where to interpolate to\n\n    Returns:\n        variable_interpolated (np.ndarray): Interpolated values\n\n    \"\"\"\n    variable = getattr(meteorology, variable_name)\n    if variable is None:\n        return None\n\n    if isinstance(sensor_object, Satellite):\n        variable_interpolated = meteorology.location.interpolate(variable, self.source_map.location)\n        variable_interpolated = variable_interpolated.reshape(1, self.source_map.nof_sources)\n    else:\n        variable_interpolated = sti.interpolate(\n            time_in=meteorology.time, values_in=variable, time_out=sensor_object.time\n        )\n        variable_interpolated = variable_interpolated.reshape(sensor_object.nof_observations, 1)\n    return variable_interpolated\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_satellite","title":"<code>compute_coupling_satellite(sensor_object, wind_speed, theta, wind_turbulence_horizontal, wind_turbulence_vertical, gas_density)</code>","text":"<p>Compute Gaussian plume coupling for satellite sensor.</p> <p>When the sensor is a Satellite object we calculate the plume coupling per source. Given the large number of sources and the possibility of using the inclusion radius and inclusion indices here and validity of a local ENU system over large distances we loop over each source and calculate the coupling on a per-source basis.</p> <p>If source_map.inclusion_n_obs is None, we do not do any filtering on observations and we want to include all observations in the plume coupling calculations.</p> <p>All np.ndarray inputs should have a shape of [1 x nof_sources]</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Sensor object used in plume coupling calculation</p> required <code>wind_speed</code> <code>ndarray</code> <p>Wind speed [m/s]</p> required <code>theta</code> <code>ndarray</code> <p>Mathematical angle between the u- and v-components of wind [radians]</p> required <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Parameter of the wind stability in horizontal direction [deg]</p> required <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Parameter of the wind stability in vertical direction [deg]</p> required <code>gas_density</code> <code>ndarray</code> <p>(np.ndarray): Numpy array containing the gas density values to use [kg/m^3]</p> required <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>list</code> <p>List of Gaussian plume coupling 1e6*[hr/kg] arrays. The list has a length of nof_sources, each array has the shape [nof_observations x 1] or [inclusion_n_obs x 1] when inclusion_idx is used.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_satellite(\n    self,\n    sensor_object: Sensor,\n    wind_speed: np.ndarray,\n    theta: np.ndarray,\n    wind_turbulence_horizontal: np.ndarray,\n    wind_turbulence_vertical: np.ndarray,\n    gas_density: np.ndarray,\n) -&gt; list:\n    \"\"\"Compute Gaussian plume coupling for satellite sensor.\n\n    When the sensor is a Satellite object we calculate the plume coupling per source. Given the large number of\n    sources and the possibility of using the inclusion radius and inclusion indices here and validity of a local\n    ENU system over large distances we loop over each source and calculate the coupling on a per-source basis.\n\n    If source_map.inclusion_n_obs is None, we do not do any filtering on observations and we want to include all\n    observations in the plume coupling calculations.\n\n    All np.ndarray inputs should have a shape of [1 x nof_sources]\n\n    Args:\n        sensor_object (Sensor): Sensor object used in plume coupling calculation\n        wind_speed (np.ndarray): Wind speed [m/s]\n        theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n        wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n        wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n        gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n    Returns:\n        plume_coupling (list): List of Gaussian plume coupling 1e6*[hr/kg] arrays. The list has a length of\n            nof_sources, each array has the shape [nof_observations x 1] or [inclusion_n_obs x 1] when\n            inclusion_idx is used.\n\n    \"\"\"\n    plume_coupling = []\n\n    source_map_location_lla = self.source_map.location.to_lla()\n    for current_source in range(self.source_map.nof_sources):\n        if self.source_map.inclusion_n_obs is None:\n            enu_sensor_array = sensor_object.location.to_enu(\n                ref_latitude=source_map_location_lla.latitude[current_source],\n                ref_longitude=source_map_location_lla.longitude[current_source],\n                ref_altitude=0,\n            ).to_array()\n\n        else:\n            if self.source_map.inclusion_n_obs[current_source] == 0:\n                plume_coupling.append(np.array([]))\n                continue\n\n            enu_sensor_array = _create_enu_sensor_array(\n                inclusion_idx=self.source_map.inclusion_idx[current_source],\n                sensor_object=sensor_object,\n                source_map_location_lla=source_map_location_lla,\n                current_source=current_source,\n            )\n\n        temp_coupling = self.compute_coupling_array(\n            enu_sensor_array[:, [0]],\n            enu_sensor_array[:, [1]],\n            enu_sensor_array[:, [2]],\n            source_map_location_lla.altitude[current_source],\n            wind_speed[:, current_source],\n            theta[:, current_source],\n            wind_turbulence_horizontal[:, current_source],\n            wind_turbulence_vertical[:, current_source],\n            gas_density[:, current_source],\n        )\n\n        plume_coupling.append(temp_coupling)\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coupling_ground","title":"<code>compute_coupling_ground(sensor_object, wind_speed, theta, wind_turbulence_horizontal, wind_turbulence_vertical, gas_density)</code>","text":"<p>Compute Gaussian plume coupling for a ground sensor.</p> <p>If the source map is already defined as ENU the reference location is maintained but the sensor is checked to make sure the same reference location is used. Otherwise, when converting to ENU object for the sensor observations we use a single source and altitude 0 as the reference location. This way our ENU system is a system w.r.t. ground level which is required for the current implementation of the actual coupling calculation.</p> <p>When the sensor is a Beam object we calculate the plume coupling for all sources to all beam knot locations at once in the same ENU coordinate system and finally averaged over the beam knots to get the final output.</p> <p>In general, we calculate the coupling from all sources to all sensor observation locations. In order to achieve this we input the sensor array as column and source array as row vector in calculating relative x etc., with the beam knot locations being the third dimension. When the sensor is a single point Sensor or a Drone sensor we effectively have one beam knot, making the mean operation at the end effectively a reshape operation which gets rid of the third dimension.</p> <p>All np.ndarray inputs should have a shape of [nof_observations x 1]</p> <p>Parameters:</p> Name Type Description Default <code>sensor_object</code> <code>Sensor</code> <p>Sensor object used in plume coupling calculation</p> required <code>wind_speed</code> <code>ndarray</code> <p>Wind speed [m/s]</p> required <code>theta</code> <code>ndarray</code> <p>Mathematical angle between the u- and v-components of wind [radians]</p> required <code>wind_turbulence_horizontal</code> <code>ndarray</code> <p>Parameter of the wind stability in horizontal direction [deg]</p> required <code>wind_turbulence_vertical</code> <code>ndarray</code> <p>Parameter of the wind stability in vertical direction [deg]</p> required <code>gas_density</code> <code>ndarray</code> <p>(np.ndarray): Numpy array containing the gas density values to use [kg/m^3]</p> required <p>Returns:</p> Name Type Description <code>plume_coupling</code> <code>ndarray</code> <p>Gaussian plume coupling 1e6*[hr/kg] array. The array has the shape [nof_observations x nof_sources]</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>def compute_coupling_ground(\n    self,\n    sensor_object: Sensor,\n    wind_speed: np.ndarray,\n    theta: np.ndarray,\n    wind_turbulence_horizontal: np.ndarray,\n    wind_turbulence_vertical: np.ndarray,\n    gas_density: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Compute Gaussian plume coupling for a ground sensor.\n\n    If the source map is already defined as ENU the reference location is maintained but the sensor is checked\n    to make sure the same reference location is used. Otherwise, when converting to ENU object for the sensor\n    observations we use a single source and altitude 0 as the reference location. This way our ENU system is a\n    system w.r.t. ground level which is required for the current implementation of the actual coupling calculation.\n\n    When the sensor is a Beam object we calculate the plume coupling for all sources to all beam knot locations at\n    once in the same ENU coordinate system and finally averaged over the beam knots to get the final output.\n\n    In general, we calculate the coupling from all sources to all sensor observation locations. In order to achieve\n    this we input the sensor array as column and source array as row vector in calculating relative x etc.,\n    with the beam knot locations being the third dimension. When the sensor is a single point Sensor or a Drone\n    sensor we effectively have one beam knot, making the mean operation at the end effectively a reshape operation\n    which gets rid of the third dimension.\n\n    All np.ndarray inputs should have a shape of [nof_observations x 1]\n\n    Args:\n        sensor_object (Sensor): Sensor object used in plume coupling calculation\n        wind_speed (np.ndarray): Wind speed [m/s]\n        theta (np.ndarray): Mathematical angle between the u- and v-components of wind [radians]\n        wind_turbulence_horizontal (np.ndarray): Parameter of the wind stability in horizontal direction [deg]\n        wind_turbulence_vertical (np.ndarray): Parameter of the wind stability in vertical direction [deg]\n        gas_density: (np.ndarray): Numpy array containing the gas density values to use [kg/m^3]\n\n    Returns:\n        plume_coupling (np.ndarray): Gaussian plume coupling 1e6*[hr/kg] array. The array has the\n            shape [nof_observations x nof_sources]\n\n    \"\"\"\n    if not isinstance(self.source_map.location, ENU):\n        source_map_lla = self.source_map.location.to_lla()\n        source_map_enu = source_map_lla.to_enu(\n            ref_latitude=source_map_lla.latitude[0], ref_longitude=source_map_lla.longitude[0], ref_altitude=0\n        )\n    else:\n        source_map_enu = self.source_map.location\n\n    enu_source_array = source_map_enu.to_array()\n\n    if isinstance(sensor_object, Beam):\n        enu_sensor_array = sensor_object.make_beam_knots(\n            ref_latitude=source_map_enu.ref_latitude,\n            ref_longitude=source_map_enu.ref_longitude,\n            ref_altitude=source_map_enu.ref_altitude,\n        )\n        relative_x = np.subtract(enu_sensor_array[:, 0][None, None, :], enu_source_array[:, 0][None, :, None])\n        relative_y = np.subtract(enu_sensor_array[:, 1][None, None, :], enu_source_array[:, 1][None, :, None])\n        z_sensor = enu_sensor_array[:, 2][None, None, :]\n    else:\n        enu_sensor_array = sensor_object.location.to_enu(\n            ref_latitude=source_map_enu.ref_latitude,\n            ref_longitude=source_map_enu.ref_longitude,\n            ref_altitude=source_map_enu.ref_altitude,\n        ).to_array()\n        relative_x = np.subtract(enu_sensor_array[:, 0][:, None, None], enu_source_array[:, 0][None, :, None])\n        relative_y = np.subtract(enu_sensor_array[:, 1][:, None, None], enu_source_array[:, 1][None, :, None])\n        z_sensor = enu_sensor_array[:, 2][:, None, None]\n\n    z_source = enu_source_array[:, 2][None, :, None]\n\n    plume_coupling = self.compute_coupling_array(\n        relative_x,\n        relative_y,\n        z_sensor,\n        z_source,\n        wind_speed[:, :, None],\n        theta[:, :, None],\n        wind_turbulence_horizontal[:, :, None],\n        wind_turbulence_vertical[:, :, None],\n        gas_density[:, :, None],\n    )\n\n    plume_coupling = plume_coupling.mean(axis=2)\n\n    return plume_coupling\n</code></pre>"},{"location":"pyelq/dispersion_model/gaussian_plume/#pyelq.dispersion_model.gaussian_plume.GaussianPlume.compute_coverage","title":"<code>compute_coverage(couplings, threshold_function, coverage_threshold=6, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Returns a logical vector that indicates which sources in the couplings are, or are not, within the coverage.</p> <p>The 'coverage' is the area inside which all sources are well covered by wind data. E.g. If wind exclusively blows towards East, then all sources to the East of any sensor are 'invisible', and are not within the coverage.</p> <p>Couplings are returned in hr/kg. Some threshold function defines the largest allowed coupling value. This is used to calculate estimated emission rates in kg/hr. Any emissions which are greater than the value of 'coverage_threshold' are defined as not within the coverage.</p> <p>Parameters:</p> Name Type Description Default <code>couplings</code> <code>ndarray</code> <p>Array of coupling values. Dimensions: n_datapoints x n_sources.</p> required <code>threshold_function</code> <code>Callable</code> <p>Callable function which returns some single value that defines the maximum or 'threshold' coupling. For example: np.quantile(., q=0.95)</p> required <code>coverage_threshold</code> <code>float</code> <p>The threshold value of the estimated emission rate which is considered to be within the coverage. Defaults to 6 kg/hr.</p> <code>6</code> <code>kwargs</code> <code>dict</code> <p>Keyword arguments required for the threshold function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>coverage</code> <code>Union[ndarray, dict]</code> <p>A logical array specifying which sources are within the coverage.</p> Source code in <code>src/pyelq/dispersion_model/gaussian_plume.py</code> <pre><code>@staticmethod\ndef compute_coverage(\n    couplings: np.ndarray, threshold_function: Callable, coverage_threshold: float = 6, **kwargs\n) -&gt; Union[np.ndarray, dict]:\n    \"\"\"Returns a logical vector that indicates which sources in the couplings are, or are not, within the coverage.\n\n    The 'coverage' is the area inside which all sources are well covered by wind data. E.g. If wind exclusively\n    blows towards East, then all sources to the East of any sensor are 'invisible', and are not within the coverage.\n\n    Couplings are returned in hr/kg. Some threshold function defines the largest allowed coupling value. This is\n    used to calculate estimated emission rates in kg/hr. Any emissions which are greater than the value of\n    'coverage_threshold' are defined as not within the coverage.\n\n    Args:\n        couplings (np.ndarray): Array of coupling values. Dimensions: n_datapoints x n_sources.\n        threshold_function (Callable): Callable function which returns some single value that defines the\n            maximum or 'threshold' coupling. For example: np.quantile(., q=0.95)\n        coverage_threshold (float, optional): The threshold value of the estimated emission rate which is\n            considered to be within the coverage. Defaults to 6 kg/hr.\n        kwargs (dict, optional): Keyword arguments required for the threshold function.\n\n    Returns:\n        coverage (Union[np.ndarray, dict]): A logical array specifying which sources are within the coverage.\n\n    \"\"\"\n    coupling_threshold = threshold_function(couplings, **kwargs)\n    no_warning_threshold = np.where(coupling_threshold &lt;= 1e-100, 1, coupling_threshold)\n    no_warning_estimated_emission_rates = np.where(coupling_threshold &lt;= 1e-100, np.inf, 1 / no_warning_threshold)\n    coverage = no_warning_estimated_emission_rates &lt; coverage_threshold\n\n    return coverage\n</code></pre>"},{"location":"pyelq/plotting/plot/","title":"Plotting","text":""},{"location":"pyelq/plotting/plot/#plot","title":"Plot","text":"<p>Plot module.</p> <p>Large module containing all the plotting code used to create various plots. Contains helper functions and the Plot class definition.</p>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot","title":"<code>Plot</code>  <code>dataclass</code>","text":"<p>Defines the plot class.</p> <p>Can be used to generate various figures from model components while storing general settings to get consistent figure appearance.</p> <p>Attributes:</p> Name Type Description <code>figure_dict</code> <code>dict</code> <p>Figure dictionary, used as storage using keys to identify the different figures.</p> <code>layout</code> <code>dict</code> <p>Layout template for plotly figures, used in all figures generated using this class instance.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>@dataclass\nclass Plot:\n    \"\"\"Defines the plot class.\n\n    Can be used to generate various figures from model components while storing general settings to get consistent\n    figure appearance.\n\n    Attributes:\n        figure_dict (dict): Figure dictionary, used as storage using keys to identify the different figures.\n        layout (dict, optional): Layout template for plotly figures, used in all figures generated using this class\n            instance.\n\n    \"\"\"\n\n    figure_dict: dict = field(default_factory=dict)\n    layout: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Using post init to set the default layout, not able to do this in attribute definition/initialization.\"\"\"\n        self.layout = {\n            \"layout\": go.Layout(\n                font={\"family\": \"Futura\", \"size\": 20},\n                title={\"x\": 0.5},\n                title_font={\"size\": 30},\n                xaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n                yaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n                legend={\n                    \"orientation\": \"v\",\n                    \"yanchor\": \"middle\",\n                    \"y\": 0.5,\n                    \"xanchor\": \"right\",\n                    \"x\": 1.2,\n                    \"font\": {\"size\": 14, \"color\": \"black\"},\n                },\n            )\n        }\n\n    def show_all(self, renderer=\"browser\"):\n        \"\"\"Show all the figures which are in the figure dictionary.\n\n        Args:\n            renderer (str, optional): Default renderer to use when showing the figures.\n\n        \"\"\"\n        for fig in self.figure_dict.values():\n            fig.show(renderer=renderer)\n\n    def plot_single_trace(self, object_to_plot: Union[Type[SlabAndSpike], SourceModel, MCMC], **kwargs: Any):\n        \"\"\"Plotting a trace of a single variable.\n\n        Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n        First it grabs all the specifics needed for the plot and then plots the trace.\n\n        Args:\n            object_to_plot (Union[Type[SlabAndSpike], SourceModel, MCMC]): The object from which to plot a variable\n            **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n                some specific plots but not applicable to all.\n\n        \"\"\"\n        plot_specifics = create_trace_specifics(object_to_plot=object_to_plot, **kwargs)\n\n        burn_in = kwargs.pop(\"burn_in\", 0)\n\n        fig = go.Figure()\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=plot_specifics[\"x_values\"],\n            y_values=plot_specifics[\"y_values\"],\n            color=plot_specifics[\"color\"],\n            name=plot_specifics[\"name\"],\n            burn_in=burn_in,\n        )\n\n        if burn_in &gt; 0:\n            fig.add_vline(\n                x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n            )\n        if isinstance(object_to_plot, SlabAndSpike) and isinstance(object_to_plot, SourceModel):\n            prior_num_sources_on = round(object_to_plot.emission_rate.shape[0] * object_to_plot.slab_probability, 2)\n\n            fig.add_hline(\n                y=prior_num_sources_on,\n                line_width=3,\n                line_dash=\"dash\",\n                line_color=\"black\",\n                annotation_text=f\"Prior sources 'on': {prior_num_sources_on}\",\n            )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.update_layout(title=plot_specifics[\"title_text\"])\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n        self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n\n    def plot_trace_per_sensor(\n        self,\n        object_to_plot: Union[ErrorModel, PerSensor, MCMC],\n        sensor_object: Union[SensorGroup, Sensor],\n        plot_type: str,\n        **kwargs: Any,\n    ):\n        \"\"\"Plotting a trace of a single variable per sensor.\n\n        Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n        First it grabs all the specifics needed for the plot and then plots the trace per sensor.\n\n        Args:\n            object_to_plot (Union[ErrorModel, PerSensor, MCMC]): The object which to plot a variable from\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n            plot_type (str): String specifying a line or box plot.\n            **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n                some specific plots but not applicable to all.\n\n        \"\"\"\n        if isinstance(sensor_object, Sensor):\n            temp = SensorGroup()\n            temp.add_sensor(sensor_object)\n            sensor_object = deepcopy(temp)\n        plot_specifics = create_plot_specifics(\n            object_to_plot=object_to_plot, sensor_object=sensor_object, plot_type=plot_type, **kwargs\n        )\n        burn_in = kwargs.pop(\"burn_in\", 0)\n\n        fig = go.Figure()\n        for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n            color_idx = sensor_idx % len(sensor_object.color_map)\n            color = sensor_object.color_map[color_idx]\n\n            if plot_specifics[\"plot_type\"] == \"line\":\n                fig = plot_single_scatter(\n                    fig=fig,\n                    x_values=plot_specifics[\"x_values\"],\n                    y_values=plot_specifics[\"y_values\"][sensor_idx, :],\n                    color=color,\n                    name=sensor_key,\n                    burn_in=burn_in,\n                )\n            elif plot_specifics[\"plot_type\"] == \"box\":\n                fig = plot_single_box(\n                    fig=fig,\n                    y_values=plot_specifics[\"y_values\"][sensor_idx, burn_in:].flatten(),\n                    color=color,\n                    name=sensor_key,\n                )\n\n        if burn_in &gt; 0 and plot_specifics[\"plot_type\"] == \"line\":\n            fig.add_vline(\n                x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n            )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.update_layout(title=plot_specifics[\"title_text\"])\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n        self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n\n    def plot_fitted_values_per_sensor(\n        self,\n        mcmc_object: MCMC,\n        sensor_object: Union[SensorGroup, Sensor],\n        background_model: TemporalBackground = None,\n        burn_in: int = 0,\n    ):\n        \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when inputted.\n\n        Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n        measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n        The created figure is stored in the figure_dict attribute.\n\n        Args:\n            mcmc_object (MCMC): MCMC object which contains the fitted values in the store attribute of the object.\n            sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n            background_model (TemporalBackground, optional): Background model containing the estimated background.\n            burn_in (int, optional): Number of burn-in iterations to discard before calculating the quantiles\n                and median. Defaults to 0.\n\n        \"\"\"\n        if \"y\" not in mcmc_object.store:\n            raise ValueError(\"Missing fitted values ('y') in mcmc_store_object\")\n\n        if isinstance(sensor_object, Sensor):\n            temp = SensorGroup()\n            temp.add_sensor(sensor_object)\n            sensor_object = deepcopy(temp)\n\n        y_values_overall = mcmc_object.store[\"y\"]\n        dict_key = \"fitted_values\"\n        title_text = \"Observations and Predicted Model Values Against Time\"\n        x_label = \"Time\"\n        y_label = \"Concentration (ppm)\"\n        fig = go.Figure()\n\n        for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n            plot_idx = np.array(sensor_object.sensor_index == sensor_idx)\n\n            x_values = sensor_object[sensor_key].time\n            y_values = y_values_overall[plot_idx, burn_in:]\n\n            color_idx = sensor_idx % len(sensor_object.color_map)\n            color = sensor_object.color_map[color_idx]\n\n            fig = plot_quantiles_from_array(\n                fig=fig, x_values=x_values, y_values=y_values, quantiles=[10, 90], color=color, name=sensor_key\n            )\n\n        if isinstance(background_model, TemporalBackground):\n            fig = plot_quantiles_from_array(\n                fig=fig,\n                x_values=background_model.time,\n                y_values=background_model.bg,\n                quantiles=[10, 90],\n                color=\"rgb(186, 186, 186)\",\n                name=\"Background\",\n            )\n\n            fig.for_each_trace(\n                lambda trace: (\n                    trace.update(showlegend=True, name=\"Background\") if trace.name == \"Median for Background\" else ()\n                ),\n            )\n\n        fig = sensor_object.plot_timeseries(fig=fig, color_map=sensor_object.color_map, mode=\"markers\")\n\n        fig.add_annotation(\n            x=1,\n            y=1.1,\n            yref=\"paper\",\n            xref=\"paper\",\n            xanchor=\"left\",\n            yanchor=\"top\",\n            font={\"size\": 12, \"color\": \"#000000\"},\n            align=\"left\",\n            showarrow=False,\n            borderwidth=2,\n            borderpad=10,\n            bgcolor=\"#ffffff\",\n            bordercolor=\"#000000\",\n            opacity=0.8,\n            text=(\n                \"&lt;b&gt;Point&lt;/b&gt;: Real observation&lt;br&gt;&lt;b&gt;Line&lt;/b&gt;: Predicted Value&lt;br&gt;&lt;b&gt;Shading&lt;/b&gt;: \" + \"Quantiles 10-90\"\n            ),\n        )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.update_layout(title=title_text)\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n\n        self.figure_dict[dict_key] = fig\n\n    def plot_emission_rate_estimates(self, source_model_object, y_axis_type=\"linear\", **kwargs: Any):\n        \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n        Based on the inputs it plots the results of the mcmc analysis, being the estimated emission rate values for\n        each source location together with the total emissions estimate, which is the sum over all source locations.\n\n        The created figure is stored in the figure_dict attribute.\n\n        After the loop over all sources we add an empty trace to have the legend entry and desired legend group\n        behaviour.\n\n        Args:\n            source_model_object (SourceModel): Source model object which contains the estimated emission rate estimates.\n            y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n            **kwargs (Any): Additional key word arguments, e.g. burn_in, dict_key, used in some specific plots but not\n                applicable to all.\n\n        \"\"\"\n        total_emissions = np.nansum(source_model_object.emission_rate, axis=0)\n        x_values = np.array(range(total_emissions.size))\n\n        burn_in = kwargs.pop(\"burn_in\", 0)\n\n        dict_key = \"estimated_values_plot\"\n        title_text = \"Estimated Values of Sources With Respect to MCMC Iterations\"\n        x_label = MCMC_ITERATION_NUMBER_LITERAL\n        y_label = \"Estimated Emission&lt;br&gt;Values (kg/hr)\"\n\n        fig = go.Figure()\n\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=x_values,\n            y_values=total_emissions,\n            color=\"rgb(239, 85, 59)\",\n            name=\"Total Site Emissions\",\n            burn_in=burn_in,\n            show_legend=True,\n        )\n\n        for source_idx in range(source_model_object.emission_rate.shape[0]):\n            y_values = source_model_object.emission_rate[source_idx, :]\n            if source_model_object.individual_source_labels[source_idx] is not None:\n                source_label = source_model_object.individual_source_labels[source_idx]\n            else:\n                source_label = f\"Source {source_idx}\"\n\n            fig = plot_single_scatter(\n                fig=fig,\n                x_values=x_values,\n                y_values=y_values,\n                color=RGB_LIGHT_BLUE,\n                name=source_label,\n                burn_in=burn_in,\n                show_legend=False,\n                legend_group=\"Source traces\",\n            )\n\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=np.array([None]),\n            y_values=np.array([None]),\n            color=RGB_LIGHT_BLUE,\n            name=\"Source traces\",\n            burn_in=0,\n            show_legend=True,\n        )\n\n        if burn_in &gt; 0:\n            fig.add_vline(\n                x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n            )\n\n        if self.layout is not None:\n            fig.update_layout(template=self.layout)\n\n        fig.add_annotation(\n            x=1.05,\n            y=1.05,\n            yref=\"paper\",\n            xref=\"paper\",\n            xanchor=\"left\",\n            yanchor=\"top\",\n            align=\"left\",\n            font={\"size\": 12, \"color\": \"#000000\"},\n            showarrow=False,\n            borderwidth=2,\n            borderpad=10,\n            bgcolor=\"#ffffff\",\n            bordercolor=\"#000000\",\n            opacity=0.8,\n            text=(\n                \"&lt;b&gt;Total Site Emissions&lt;/b&gt; are&lt;br&gt;the sum of all estimated&lt;br&gt;\"\n                \"emission rates at a given&lt;br&gt;iteration number.\"\n            ),\n        )\n\n        fig.update_layout(title=title_text)\n        fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n        fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n        if y_axis_type == \"log\":\n            fig.update_yaxes(type=\"log\")\n            dict_key = \"log_estimated_values_plot\"\n        elif y_axis_type != \"linear\":\n            raise ValueError(f\"Only linear or log y axis type is allowed, {y_axis_type} was currently specified.\")\n\n        self.figure_dict[dict_key] = fig\n\n    def create_empty_map_figure(self, dict_key: str = \"map_plot\") -&gt; None:\n        \"\"\"Creating an empty map figure to use when you want to add additional traces on a map.\n\n        Args:\n            dict_key (str, optional): String key for figure dictionary\n\n        \"\"\"\n        self.figure_dict[dict_key] = go.Figure(\n            data=go.Scattermap(),\n            layout={\n                \"map_style\": \"carto-positron\",\n                \"map_center_lat\": 0,\n                \"map_center_lon\": 0,\n                \"map_zoom\": 0,\n            },\n        )\n\n    def plot_values_on_map(\n        self, dict_key: str, coordinates: LLA, values: np.ndarray, aggregate_function: Callable = np.sum, **kwargs: Any\n    ):\n        \"\"\"Plot values on a map based on coordinates.\n\n        Args:\n            dict_key (str): Sting key to use in the figure dictionary\n            coordinates (LLA): LLA coordinates to use in plotting the values on the map\n            values (np.ndarray): Numpy array of values consistent with coordinates to plot on the map\n            aggregate_function (Callable, optional): Function which to apply on the data in each hexagonal bin to\n                aggregate the data and visualise the result.\n            **kwargs (Any): Additional keyword arguments for plotting behaviour (opacity, map_color_scale, num_hexagons,\n                show_positions)\n\n        \"\"\"\n        map_color_scale = kwargs.pop(\"map_color_scale\", \"YlOrRd\")\n        num_hexagons = kwargs.pop(\"num_hexagons\", None)\n        opacity = kwargs.pop(\"opacity\", 0.8)\n        show_positions = kwargs.pop(\"show_positions\", False)\n\n        latitude_check, _ = is_regularly_spaced(coordinates.latitude)\n        longitude_check, _ = is_regularly_spaced(coordinates.longitude)\n        if latitude_check and longitude_check:\n            self.create_empty_map_figure(dict_key=dict_key)\n            trace = plot_regular_grid(\n                coordinates=coordinates,\n                values=values,\n                opacity=opacity,\n                map_color_scale=map_color_scale,\n                tolerance=1e-7,\n                unit=\"\",\n            )\n            self.figure_dict[dict_key].add_trace(trace)\n        else:\n            fig = plot_hexagonal_grid(\n                coordinates=coordinates,\n                values=values,\n                opacity=opacity,\n                map_color_scale=map_color_scale,\n                num_hexagons=num_hexagons,\n                show_positions=show_positions,\n                aggregate_function=aggregate_function,\n            )\n            fig.update_layout(map_style=\"carto-positron\")\n            self.figure_dict[dict_key] = fig\n\n        center_longitude = np.mean(coordinates.longitude)\n        center_latitude = np.mean(coordinates.latitude)\n        self.figure_dict[dict_key].update_layout(\n            map={\"zoom\": 10, \"center\": {\"lon\": center_longitude, \"lat\": center_latitude}}\n        )\n\n        if self.layout is not None:\n            self.figure_dict[dict_key].update_layout(template=self.layout)\n\n    def plot_quantification_results_on_map(\n        self,\n        model_object: \"ELQModel\",\n        source_model_to_plot_key: str = None,\n        bin_size_x: float = 1,\n        bin_size_y: float = 1,\n        normalized_count_limit: float = 0.005,\n        burn_in: int = 0,\n        show_summary_results: bool = True,\n        show_fixed_source_locations: bool = True,\n    ):\n        \"\"\"Function to create a map with the quantification results of the model object.\n\n        This function takes the \"SourceModel\" object and calculates the statistics for the quantification results.\n        It then populates the figure dictionary with three different maps showing the normalized count,\n        median emission rate and the inter-quartile range of the emission rate estimates.\n\n        Args:\n            model_object (ELQModel): ELQModel object containing the quantification results\n            source_model_to_plot_key (str, optional): Key to use in the model_object.components dictionary to access\n              the SourceModel object. If None, defaults to \"sources_combined\".\n            bin_size_x (float, optional): Size of the bins in the x-direction. Defaults to 1.\n            bin_size_y (float, optional): Size of the bins in the y-direction. Defaults to 1.\n            normalized_count_limit (float, optional): Limit for the normalized count to show on the map.\n                Defaults to 0.005.\n            burn_in (int, optional): Number of burn-in iterations to discard before calculating the statistics.\n                Defaults to 0.\n            show_summary_results (bool, optional): Flag to show the summary results on the map. Defaults to True.\n            show_fixed_source_locations (bool, optional): Flag to show the fixed sources location when present in one\n                of the sourcemaps. Defaults to True.\n\n        \"\"\"\n        if source_model_to_plot_key is None:\n            source_model_to_plot_key = \"sources_combined\"\n\n        source_model = model_object.components[source_model_to_plot_key]\n        sensor_object = model_object.sensor_object\n\n        source_locations = source_model.all_source_locations\n        emission_rates = source_model.emission_rate\n\n        ref_latitude = source_locations.ref_latitude\n        ref_longitude = source_locations.ref_longitude\n        ref_altitude = source_locations.ref_altitude\n\n        datetime_min_string = sensor_object.time.min().strftime(\"%d-%b-%Y, %H:%M:%S\")\n        datetime_max_string = sensor_object.time.max().strftime(\"%d-%b-%Y, %H:%M:%S\")\n\n        result_weighted, _, normalized_count, count_boolean, enu_points, summary_result = (\n            calculate_rectangular_statistics(\n                emission_rates=emission_rates,\n                source_locations=source_locations,\n                bin_size_x=bin_size_x,\n                bin_size_y=bin_size_y,\n                burn_in=burn_in,\n                normalized_count_limit=normalized_count_limit,\n            )\n        )\n\n        polygons = create_lla_polygons_from_xy_points(\n            points_array=enu_points,\n            ref_latitude=ref_latitude,\n            ref_longitude=ref_longitude,\n            ref_altitude=ref_altitude,\n            boolean_mask=count_boolean,\n        )\n\n        if show_summary_results:\n            summary_trace = self.create_summary_trace(summary_result=summary_result)\n\n        self.create_empty_map_figure(dict_key=\"count_map\")\n        trace = plot_polygons_on_map(\n            polygons=polygons,\n            values=normalized_count[count_boolean].flatten(),\n            opacity=0.8,\n            name=\"normalized_count\",\n            colorbar={\"title\": \"Normalized Count\", \"orientation\": \"h\"},\n            map_color_scale=\"Bluered\",\n        )\n        self.figure_dict[\"count_map\"].add_trace(trace)\n        self.figure_dict[\"count_map\"].update_layout(\n            map_style=\"carto-positron\",\n            map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n            title=f\"Source location probability \"\n            f\"(&gt;={normalized_count_limit}) for \"\n            f\"{datetime_min_string} to {datetime_max_string}\",\n            font_family=\"Futura\",\n            font_size=15,\n        )\n        sensor_object.plot_sensor_location(self.figure_dict[\"count_map\"])\n        self.figure_dict[\"count_map\"].update_traces(showlegend=False)\n\n        adjusted_result_weights = result_weighted.copy()\n        adjusted_result_weights[adjusted_result_weights == 0] = np.nan\n\n        median_of_all_emissions = np.nanmedian(adjusted_result_weights, axis=2)\n\n        self.create_empty_map_figure(dict_key=\"median_map\")\n\n        trace = plot_polygons_on_map(\n            polygons=polygons,\n            values=median_of_all_emissions[count_boolean].flatten(),\n            opacity=0.8,\n            name=\"median_emission\",\n            colorbar={\"title\": \"Median Emission\", \"orientation\": \"h\"},\n            map_color_scale=\"Bluered\",\n        )\n        self.figure_dict[\"median_map\"].add_trace(trace)\n        self.figure_dict[\"median_map\"].update_layout(\n            map_style=\"carto-positron\",\n            map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n            title=f\"Median emission rate estimate for {datetime_min_string} to {datetime_max_string}\",\n            font_family=\"Futura\",\n            font_size=15,\n        )\n        sensor_object.plot_sensor_location(self.figure_dict[\"median_map\"])\n        self.figure_dict[\"median_map\"].update_traces(showlegend=False)\n\n        iqr_of_all_emissions = np.nanquantile(a=adjusted_result_weights, q=0.75, axis=2) - np.nanquantile(\n            a=adjusted_result_weights, q=0.25, axis=2\n        )\n        self.create_empty_map_figure(dict_key=\"iqr_map\")\n\n        trace = plot_polygons_on_map(\n            polygons=polygons,\n            values=iqr_of_all_emissions[count_boolean].flatten(),\n            opacity=0.8,\n            name=\"iqr_emission\",\n            colorbar={\"title\": \"IQR\", \"orientation\": \"h\"},\n            map_color_scale=\"Bluered\",\n        )\n        self.figure_dict[\"iqr_map\"].add_trace(trace)\n        self.figure_dict[\"iqr_map\"].update_layout(\n            map_style=\"carto-positron\",\n            map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n            title=f\"Inter Quartile range (25%-75%) of emission rate \"\n            f\"estimate for {datetime_min_string} to {datetime_max_string}\",\n            font_family=\"Futura\",\n            font_size=15,\n        )\n        sensor_object.plot_sensor_location(self.figure_dict[\"iqr_map\"])\n        self.figure_dict[\"iqr_map\"].update_traces(showlegend=False)\n\n        if show_fixed_source_locations:\n            for key, _ in model_object.components.items():\n                if bool(re.search(\"fixed\", key)):\n                    source_model_fixed = model_object.components[key]\n                    source_locations_fixed = source_model_fixed.all_source_locations\n                    source_location_fixed_lla = source_locations_fixed.to_lla()\n                    sources_lat = source_location_fixed_lla.latitude[:, 0]\n                    sources_lon = source_location_fixed_lla.longitude[:, 0]\n                    fixed_source_location_trace = go.Scattermap(\n                        mode=\"markers\",\n                        lon=sources_lon,\n                        lat=sources_lat,\n                        name=f\"Fixed source locations, {key}\",\n                        marker={\"size\": 10, \"opacity\": 0.8},\n                    )\n                    self.figure_dict[\"count_map\"].add_trace(fixed_source_location_trace)\n                    self.figure_dict[\"median_map\"].add_trace(fixed_source_location_trace)\n                    self.figure_dict[\"iqr_map\"].add_trace(fixed_source_location_trace)\n\n        if show_summary_results:\n            self.figure_dict[\"count_map\"].add_trace(summary_trace)\n            self.figure_dict[\"count_map\"].update_traces(showlegend=True)\n            self.figure_dict[\"median_map\"].add_trace(summary_trace)\n            self.figure_dict[\"median_map\"].update_traces(showlegend=True)\n            self.figure_dict[\"iqr_map\"].add_trace(summary_trace)\n            self.figure_dict[\"iqr_map\"].update_traces(showlegend=True)\n\n    def plot_coverage(\n        self,\n        coordinates: LLA,\n        couplings: np.ndarray,\n        threshold_function: Callable = np.max,\n        coverage_threshold: float = 6,\n        opacity: float = 0.8,\n        map_color_scale=\"jet\",\n    ):\n        \"\"\"Creates a coverage plot using the coverage function from Gaussian Plume.\n\n        Args:\n            coordinates (LLA object): A LLA coordinate object containing a set of locations.\n            couplings (np.array): The calculated values of coupling (The 'A matrix') for a set of wind data.\n            threshold_function (Callable, optional): Callable function which returns some single value that defines the\n                                         maximum or 'threshold' coupling. Examples: np.quantile(q=0.9),\n                                         np.max, np.mean. Defaults to np.max.\n            coverage_threshold (float, optional): The threshold value of the estimated emission rate which is\n                                                  considered to be within the coverage. Defaults to 6 kg/hr.\n            opacity (float): The opacity of the grid cells when they are plotted.\n            map_color_scale (str): The string which defines which plotly colour scale should be used when plotting\n                                   the values.\n\n        \"\"\"\n        coverage_values = GaussianPlume(source_map=None).compute_coverage(\n            couplings=couplings, threshold_function=threshold_function, coverage_threshold=coverage_threshold\n        )\n        self.plot_values_on_map(\n            dict_key=\"coverage_map\",\n            coordinates=coordinates,\n            values=coverage_values,\n            aggregate_function=np.max,\n            opacity=opacity,\n            map_color_scale=map_color_scale,\n        )\n\n    @staticmethod\n    def create_summary_trace(\n        summary_result: pd.DataFrame,\n    ) -&gt; go.Scattermap:\n        \"\"\"Helper function to create the summary information to plot on top of map type plots.\n\n        We use the summary result calculated through the support functions module to create a trace which contains\n        the summary information for each source location.\n\n        Args:\n            summary_result (pd.DataFrame): DataFrame containing the summary information for each source location.\n\n        Returns:\n            summary_trace (go.Scattermap): Trace with summary information to plot on top of map type plots.\n\n        \"\"\"\n        summary_text_values = [\n            f\"&lt;b&gt;Source ID&lt;/b&gt;: {value}&lt;br&gt;\"\n            f\"&lt;b&gt;(Lon, Lat, Alt)&lt;/b&gt; ([deg], [deg], [m]):&lt;br&gt;\"\n            f\"({summary_result.longitude[value]:.7f}, \"\n            f\"{summary_result.latitude[value]:.7f}, {summary_result.altitude[value]:.3f})&lt;br&gt;\"\n            f\"&lt;b&gt;Height&lt;/b&gt;: {summary_result.height[value]:.3f} [m]&lt;br&gt;\"\n            f\"&lt;b&gt;Median emission rate&lt;/b&gt;: {summary_result.median_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;2.5% quantile&lt;/b&gt;: {summary_result.quantile_025[value]:.3f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;97.5% quantile&lt;/b&gt;: {summary_result.quantile_975[value]:.3f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;IQR&lt;/b&gt;: {summary_result.iqr_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n            f\"&lt;b&gt;Blob present during&lt;/b&gt;: \"\n            f\"{summary_result.absolute_count_iterations[value]:.0f} iterations&lt;br&gt;\"\n            f\"&lt;b&gt;Blob likelihood&lt;/b&gt;: {summary_result.blob_likelihood[value]:.5f}&lt;br&gt;\"\n            for value in summary_result.index\n        ]\n\n        summary_trace = go.Scattermap(\n            lat=summary_result.latitude,\n            lon=summary_result.longitude,\n            mode=\"markers\",\n            marker=go.scattermap.Marker(size=14, color=\"black\"),\n            text=summary_text_values,\n            name=\"Summary\",\n            hoverinfo=\"text\",\n        )\n\n        return summary_trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Using post init to set the default layout, not able to do this in attribute definition/initialization.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Using post init to set the default layout, not able to do this in attribute definition/initialization.\"\"\"\n    self.layout = {\n        \"layout\": go.Layout(\n            font={\"family\": \"Futura\", \"size\": 20},\n            title={\"x\": 0.5},\n            title_font={\"size\": 30},\n            xaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n            yaxis={\"ticks\": \"outside\", \"showline\": True, \"linewidth\": 2},\n            legend={\n                \"orientation\": \"v\",\n                \"yanchor\": \"middle\",\n                \"y\": 0.5,\n                \"xanchor\": \"right\",\n                \"x\": 1.2,\n                \"font\": {\"size\": 14, \"color\": \"black\"},\n            },\n        )\n    }\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.show_all","title":"<code>show_all(renderer='browser')</code>","text":"<p>Show all the figures which are in the figure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>renderer</code> <code>str</code> <p>Default renderer to use when showing the figures.</p> <code>'browser'</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def show_all(self, renderer=\"browser\"):\n    \"\"\"Show all the figures which are in the figure dictionary.\n\n    Args:\n        renderer (str, optional): Default renderer to use when showing the figures.\n\n    \"\"\"\n    for fig in self.figure_dict.values():\n        fig.show(renderer=renderer)\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_single_trace","title":"<code>plot_single_trace(object_to_plot, **kwargs)</code>","text":"<p>Plotting a trace of a single variable.</p> <p>Depending on the object to plot it creates a figure which is stored in the figure_dict attribute. First it grabs all the specifics needed for the plot and then plots the trace.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[Type[SlabAndSpike], SourceModel, MCMC]</code> <p>The object from which to plot a variable</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_single_trace(self, object_to_plot: Union[Type[SlabAndSpike], SourceModel, MCMC], **kwargs: Any):\n    \"\"\"Plotting a trace of a single variable.\n\n    Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n    First it grabs all the specifics needed for the plot and then plots the trace.\n\n    Args:\n        object_to_plot (Union[Type[SlabAndSpike], SourceModel, MCMC]): The object from which to plot a variable\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n            some specific plots but not applicable to all.\n\n    \"\"\"\n    plot_specifics = create_trace_specifics(object_to_plot=object_to_plot, **kwargs)\n\n    burn_in = kwargs.pop(\"burn_in\", 0)\n\n    fig = go.Figure()\n    fig = plot_single_scatter(\n        fig=fig,\n        x_values=plot_specifics[\"x_values\"],\n        y_values=plot_specifics[\"y_values\"],\n        color=plot_specifics[\"color\"],\n        name=plot_specifics[\"name\"],\n        burn_in=burn_in,\n    )\n\n    if burn_in &gt; 0:\n        fig.add_vline(\n            x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n        )\n    if isinstance(object_to_plot, SlabAndSpike) and isinstance(object_to_plot, SourceModel):\n        prior_num_sources_on = round(object_to_plot.emission_rate.shape[0] * object_to_plot.slab_probability, 2)\n\n        fig.add_hline(\n            y=prior_num_sources_on,\n            line_width=3,\n            line_dash=\"dash\",\n            line_color=\"black\",\n            annotation_text=f\"Prior sources 'on': {prior_num_sources_on}\",\n        )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.update_layout(title=plot_specifics[\"title_text\"])\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n    self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_trace_per_sensor","title":"<code>plot_trace_per_sensor(object_to_plot, sensor_object, plot_type, **kwargs)</code>","text":"<p>Plotting a trace of a single variable per sensor.</p> <p>Depending on the object to plot it creates a figure which is stored in the figure_dict attribute. First it grabs all the specifics needed for the plot and then plots the trace per sensor.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[ErrorModel, PerSensor, MCMC]</code> <p>The object which to plot a variable from</p> required <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the object_to_plot</p> required <code>plot_type</code> <code>str</code> <p>String specifying a line or box plot.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_trace_per_sensor(\n    self,\n    object_to_plot: Union[ErrorModel, PerSensor, MCMC],\n    sensor_object: Union[SensorGroup, Sensor],\n    plot_type: str,\n    **kwargs: Any,\n):\n    \"\"\"Plotting a trace of a single variable per sensor.\n\n    Depending on the object to plot it creates a figure which is stored in the figure_dict attribute.\n    First it grabs all the specifics needed for the plot and then plots the trace per sensor.\n\n    Args:\n        object_to_plot (Union[ErrorModel, PerSensor, MCMC]): The object which to plot a variable from\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n        plot_type (str): String specifying a line or box plot.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, dict_key, used in\n            some specific plots but not applicable to all.\n\n    \"\"\"\n    if isinstance(sensor_object, Sensor):\n        temp = SensorGroup()\n        temp.add_sensor(sensor_object)\n        sensor_object = deepcopy(temp)\n    plot_specifics = create_plot_specifics(\n        object_to_plot=object_to_plot, sensor_object=sensor_object, plot_type=plot_type, **kwargs\n    )\n    burn_in = kwargs.pop(\"burn_in\", 0)\n\n    fig = go.Figure()\n    for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n        color_idx = sensor_idx % len(sensor_object.color_map)\n        color = sensor_object.color_map[color_idx]\n\n        if plot_specifics[\"plot_type\"] == \"line\":\n            fig = plot_single_scatter(\n                fig=fig,\n                x_values=plot_specifics[\"x_values\"],\n                y_values=plot_specifics[\"y_values\"][sensor_idx, :],\n                color=color,\n                name=sensor_key,\n                burn_in=burn_in,\n            )\n        elif plot_specifics[\"plot_type\"] == \"box\":\n            fig = plot_single_box(\n                fig=fig,\n                y_values=plot_specifics[\"y_values\"][sensor_idx, burn_in:].flatten(),\n                color=color,\n                name=sensor_key,\n            )\n\n    if burn_in &gt; 0 and plot_specifics[\"plot_type\"] == \"line\":\n        fig.add_vline(\n            x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n        )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.update_layout(title=plot_specifics[\"title_text\"])\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"x_label\"])\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=plot_specifics[\"y_label\"])\n\n    self.figure_dict[plot_specifics[\"dict_key\"]] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_fitted_values_per_sensor","title":"<code>plot_fitted_values_per_sensor(mcmc_object, sensor_object, background_model=None, burn_in=0)</code>","text":"<p>Plot the fitted values from the mcmc object against time, also shows the estimated background when inputted.</p> <p>Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.</p> <p>The created figure is stored in the figure_dict attribute.</p> <p>Parameters:</p> Name Type Description Default <code>mcmc_object</code> <code>MCMC</code> <p>MCMC object which contains the fitted values in the store attribute of the object.</p> required <code>sensor_object</code> <code>Union[SensorGroup, Sensor]</code> <p>Sensor object associated with the object_to_plot</p> required <code>background_model</code> <code>TemporalBackground</code> <p>Background model containing the estimated background.</p> <code>None</code> <code>burn_in</code> <code>int</code> <p>Number of burn-in iterations to discard before calculating the quantiles and median. Defaults to 0.</p> <code>0</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_fitted_values_per_sensor(\n    self,\n    mcmc_object: MCMC,\n    sensor_object: Union[SensorGroup, Sensor],\n    background_model: TemporalBackground = None,\n    burn_in: int = 0,\n):\n    \"\"\"Plot the fitted values from the mcmc object against time, also shows the estimated background when inputted.\n\n    Based on the inputs it plots the results of the mcmc analysis, being the fitted values of the concentration\n    measurements together with the 10th and 90th quantile lines to show the goodness of fit of the estimates.\n\n    The created figure is stored in the figure_dict attribute.\n\n    Args:\n        mcmc_object (MCMC): MCMC object which contains the fitted values in the store attribute of the object.\n        sensor_object (Union[SensorGroup, Sensor]): Sensor object associated with the object_to_plot\n        background_model (TemporalBackground, optional): Background model containing the estimated background.\n        burn_in (int, optional): Number of burn-in iterations to discard before calculating the quantiles\n            and median. Defaults to 0.\n\n    \"\"\"\n    if \"y\" not in mcmc_object.store:\n        raise ValueError(\"Missing fitted values ('y') in mcmc_store_object\")\n\n    if isinstance(sensor_object, Sensor):\n        temp = SensorGroup()\n        temp.add_sensor(sensor_object)\n        sensor_object = deepcopy(temp)\n\n    y_values_overall = mcmc_object.store[\"y\"]\n    dict_key = \"fitted_values\"\n    title_text = \"Observations and Predicted Model Values Against Time\"\n    x_label = \"Time\"\n    y_label = \"Concentration (ppm)\"\n    fig = go.Figure()\n\n    for sensor_idx, sensor_key in enumerate(sensor_object.keys()):\n        plot_idx = np.array(sensor_object.sensor_index == sensor_idx)\n\n        x_values = sensor_object[sensor_key].time\n        y_values = y_values_overall[plot_idx, burn_in:]\n\n        color_idx = sensor_idx % len(sensor_object.color_map)\n        color = sensor_object.color_map[color_idx]\n\n        fig = plot_quantiles_from_array(\n            fig=fig, x_values=x_values, y_values=y_values, quantiles=[10, 90], color=color, name=sensor_key\n        )\n\n    if isinstance(background_model, TemporalBackground):\n        fig = plot_quantiles_from_array(\n            fig=fig,\n            x_values=background_model.time,\n            y_values=background_model.bg,\n            quantiles=[10, 90],\n            color=\"rgb(186, 186, 186)\",\n            name=\"Background\",\n        )\n\n        fig.for_each_trace(\n            lambda trace: (\n                trace.update(showlegend=True, name=\"Background\") if trace.name == \"Median for Background\" else ()\n            ),\n        )\n\n    fig = sensor_object.plot_timeseries(fig=fig, color_map=sensor_object.color_map, mode=\"markers\")\n\n    fig.add_annotation(\n        x=1,\n        y=1.1,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"left\",\n        yanchor=\"top\",\n        font={\"size\": 12, \"color\": \"#000000\"},\n        align=\"left\",\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=(\n            \"&lt;b&gt;Point&lt;/b&gt;: Real observation&lt;br&gt;&lt;b&gt;Line&lt;/b&gt;: Predicted Value&lt;br&gt;&lt;b&gt;Shading&lt;/b&gt;: \" + \"Quantiles 10-90\"\n        ),\n    )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.update_layout(title=title_text)\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n\n    self.figure_dict[dict_key] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_emission_rate_estimates","title":"<code>plot_emission_rate_estimates(source_model_object, y_axis_type='linear', **kwargs)</code>","text":"<p>Plot the emission rate estimates source model object against MCMC iteration.</p> <p>Based on the inputs it plots the results of the mcmc analysis, being the estimated emission rate values for each source location together with the total emissions estimate, which is the sum over all source locations.</p> <p>The created figure is stored in the figure_dict attribute.</p> <p>After the loop over all sources we add an empty trace to have the legend entry and desired legend group behaviour.</p> <p>Parameters:</p> Name Type Description Default <code>source_model_object</code> <code>SourceModel</code> <p>Source model object which contains the estimated emission rate estimates.</p> required <code>y_axis_type</code> <code>str</code> <p>String to indicate whether the y-axis should be linear of log scale.</p> <code>'linear'</code> <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_emission_rate_estimates(self, source_model_object, y_axis_type=\"linear\", **kwargs: Any):\n    \"\"\"Plot the emission rate estimates source model object against MCMC iteration.\n\n    Based on the inputs it plots the results of the mcmc analysis, being the estimated emission rate values for\n    each source location together with the total emissions estimate, which is the sum over all source locations.\n\n    The created figure is stored in the figure_dict attribute.\n\n    After the loop over all sources we add an empty trace to have the legend entry and desired legend group\n    behaviour.\n\n    Args:\n        source_model_object (SourceModel): Source model object which contains the estimated emission rate estimates.\n        y_axis_type (str, optional): String to indicate whether the y-axis should be linear of log scale.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, dict_key, used in some specific plots but not\n            applicable to all.\n\n    \"\"\"\n    total_emissions = np.nansum(source_model_object.emission_rate, axis=0)\n    x_values = np.array(range(total_emissions.size))\n\n    burn_in = kwargs.pop(\"burn_in\", 0)\n\n    dict_key = \"estimated_values_plot\"\n    title_text = \"Estimated Values of Sources With Respect to MCMC Iterations\"\n    x_label = MCMC_ITERATION_NUMBER_LITERAL\n    y_label = \"Estimated Emission&lt;br&gt;Values (kg/hr)\"\n\n    fig = go.Figure()\n\n    fig = plot_single_scatter(\n        fig=fig,\n        x_values=x_values,\n        y_values=total_emissions,\n        color=\"rgb(239, 85, 59)\",\n        name=\"Total Site Emissions\",\n        burn_in=burn_in,\n        show_legend=True,\n    )\n\n    for source_idx in range(source_model_object.emission_rate.shape[0]):\n        y_values = source_model_object.emission_rate[source_idx, :]\n        if source_model_object.individual_source_labels[source_idx] is not None:\n            source_label = source_model_object.individual_source_labels[source_idx]\n        else:\n            source_label = f\"Source {source_idx}\"\n\n        fig = plot_single_scatter(\n            fig=fig,\n            x_values=x_values,\n            y_values=y_values,\n            color=RGB_LIGHT_BLUE,\n            name=source_label,\n            burn_in=burn_in,\n            show_legend=False,\n            legend_group=\"Source traces\",\n        )\n\n    fig = plot_single_scatter(\n        fig=fig,\n        x_values=np.array([None]),\n        y_values=np.array([None]),\n        color=RGB_LIGHT_BLUE,\n        name=\"Source traces\",\n        burn_in=0,\n        show_legend=True,\n    )\n\n    if burn_in &gt; 0:\n        fig.add_vline(\n            x=burn_in, line_width=3, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"\\tBurn in: {burn_in}\"\n        )\n\n    if self.layout is not None:\n        fig.update_layout(template=self.layout)\n\n    fig.add_annotation(\n        x=1.05,\n        y=1.05,\n        yref=\"paper\",\n        xref=\"paper\",\n        xanchor=\"left\",\n        yanchor=\"top\",\n        align=\"left\",\n        font={\"size\": 12, \"color\": \"#000000\"},\n        showarrow=False,\n        borderwidth=2,\n        borderpad=10,\n        bgcolor=\"#ffffff\",\n        bordercolor=\"#000000\",\n        opacity=0.8,\n        text=(\n            \"&lt;b&gt;Total Site Emissions&lt;/b&gt; are&lt;br&gt;the sum of all estimated&lt;br&gt;\"\n            \"emission rates at a given&lt;br&gt;iteration number.\"\n        ),\n    )\n\n    fig.update_layout(title=title_text)\n    fig.update_xaxes(title_standoff=20, automargin=True, title_text=x_label)\n    fig.update_yaxes(title_standoff=20, automargin=True, title_text=y_label)\n    if y_axis_type == \"log\":\n        fig.update_yaxes(type=\"log\")\n        dict_key = \"log_estimated_values_plot\"\n    elif y_axis_type != \"linear\":\n        raise ValueError(f\"Only linear or log y axis type is allowed, {y_axis_type} was currently specified.\")\n\n    self.figure_dict[dict_key] = fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.create_empty_map_figure","title":"<code>create_empty_map_figure(dict_key='map_plot')</code>","text":"<p>Creating an empty map figure to use when you want to add additional traces on a map.</p> <p>Parameters:</p> Name Type Description Default <code>dict_key</code> <code>str</code> <p>String key for figure dictionary</p> <code>'map_plot'</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def create_empty_map_figure(self, dict_key: str = \"map_plot\") -&gt; None:\n    \"\"\"Creating an empty map figure to use when you want to add additional traces on a map.\n\n    Args:\n        dict_key (str, optional): String key for figure dictionary\n\n    \"\"\"\n    self.figure_dict[dict_key] = go.Figure(\n        data=go.Scattermap(),\n        layout={\n            \"map_style\": \"carto-positron\",\n            \"map_center_lat\": 0,\n            \"map_center_lon\": 0,\n            \"map_zoom\": 0,\n        },\n    )\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_values_on_map","title":"<code>plot_values_on_map(dict_key, coordinates, values, aggregate_function=np.sum, **kwargs)</code>","text":"<p>Plot values on a map based on coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>dict_key</code> <code>str</code> <p>Sting key to use in the figure dictionary</p> required <code>coordinates</code> <code>LLA</code> <p>LLA coordinates to use in plotting the values on the map</p> required <code>values</code> <code>ndarray</code> <p>Numpy array of values consistent with coordinates to plot on the map</p> required <code>aggregate_function</code> <code>Callable</code> <p>Function which to apply on the data in each hexagonal bin to aggregate the data and visualise the result.</p> <code>sum</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plotting behaviour (opacity, map_color_scale, num_hexagons, show_positions)</p> <code>{}</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_values_on_map(\n    self, dict_key: str, coordinates: LLA, values: np.ndarray, aggregate_function: Callable = np.sum, **kwargs: Any\n):\n    \"\"\"Plot values on a map based on coordinates.\n\n    Args:\n        dict_key (str): Sting key to use in the figure dictionary\n        coordinates (LLA): LLA coordinates to use in plotting the values on the map\n        values (np.ndarray): Numpy array of values consistent with coordinates to plot on the map\n        aggregate_function (Callable, optional): Function which to apply on the data in each hexagonal bin to\n            aggregate the data and visualise the result.\n        **kwargs (Any): Additional keyword arguments for plotting behaviour (opacity, map_color_scale, num_hexagons,\n            show_positions)\n\n    \"\"\"\n    map_color_scale = kwargs.pop(\"map_color_scale\", \"YlOrRd\")\n    num_hexagons = kwargs.pop(\"num_hexagons\", None)\n    opacity = kwargs.pop(\"opacity\", 0.8)\n    show_positions = kwargs.pop(\"show_positions\", False)\n\n    latitude_check, _ = is_regularly_spaced(coordinates.latitude)\n    longitude_check, _ = is_regularly_spaced(coordinates.longitude)\n    if latitude_check and longitude_check:\n        self.create_empty_map_figure(dict_key=dict_key)\n        trace = plot_regular_grid(\n            coordinates=coordinates,\n            values=values,\n            opacity=opacity,\n            map_color_scale=map_color_scale,\n            tolerance=1e-7,\n            unit=\"\",\n        )\n        self.figure_dict[dict_key].add_trace(trace)\n    else:\n        fig = plot_hexagonal_grid(\n            coordinates=coordinates,\n            values=values,\n            opacity=opacity,\n            map_color_scale=map_color_scale,\n            num_hexagons=num_hexagons,\n            show_positions=show_positions,\n            aggregate_function=aggregate_function,\n        )\n        fig.update_layout(map_style=\"carto-positron\")\n        self.figure_dict[dict_key] = fig\n\n    center_longitude = np.mean(coordinates.longitude)\n    center_latitude = np.mean(coordinates.latitude)\n    self.figure_dict[dict_key].update_layout(\n        map={\"zoom\": 10, \"center\": {\"lon\": center_longitude, \"lat\": center_latitude}}\n    )\n\n    if self.layout is not None:\n        self.figure_dict[dict_key].update_layout(template=self.layout)\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_quantification_results_on_map","title":"<code>plot_quantification_results_on_map(model_object, source_model_to_plot_key=None, bin_size_x=1, bin_size_y=1, normalized_count_limit=0.005, burn_in=0, show_summary_results=True, show_fixed_source_locations=True)</code>","text":"<p>Function to create a map with the quantification results of the model object.</p> <p>This function takes the \"SourceModel\" object and calculates the statistics for the quantification results. It then populates the figure dictionary with three different maps showing the normalized count, median emission rate and the inter-quartile range of the emission rate estimates.</p> <p>Parameters:</p> Name Type Description Default <code>model_object</code> <code>ELQModel</code> <p>ELQModel object containing the quantification results</p> required <code>source_model_to_plot_key</code> <code>str</code> <p>Key to use in the model_object.components dictionary to access the SourceModel object. If None, defaults to \"sources_combined\".</p> <code>None</code> <code>bin_size_x</code> <code>float</code> <p>Size of the bins in the x-direction. Defaults to 1.</p> <code>1</code> <code>bin_size_y</code> <code>float</code> <p>Size of the bins in the y-direction. Defaults to 1.</p> <code>1</code> <code>normalized_count_limit</code> <code>float</code> <p>Limit for the normalized count to show on the map. Defaults to 0.005.</p> <code>0.005</code> <code>burn_in</code> <code>int</code> <p>Number of burn-in iterations to discard before calculating the statistics. Defaults to 0.</p> <code>0</code> <code>show_summary_results</code> <code>bool</code> <p>Flag to show the summary results on the map. Defaults to True.</p> <code>True</code> <code>show_fixed_source_locations</code> <code>bool</code> <p>Flag to show the fixed sources location when present in one of the sourcemaps. Defaults to True.</p> <code>True</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_quantification_results_on_map(\n    self,\n    model_object: \"ELQModel\",\n    source_model_to_plot_key: str = None,\n    bin_size_x: float = 1,\n    bin_size_y: float = 1,\n    normalized_count_limit: float = 0.005,\n    burn_in: int = 0,\n    show_summary_results: bool = True,\n    show_fixed_source_locations: bool = True,\n):\n    \"\"\"Function to create a map with the quantification results of the model object.\n\n    This function takes the \"SourceModel\" object and calculates the statistics for the quantification results.\n    It then populates the figure dictionary with three different maps showing the normalized count,\n    median emission rate and the inter-quartile range of the emission rate estimates.\n\n    Args:\n        model_object (ELQModel): ELQModel object containing the quantification results\n        source_model_to_plot_key (str, optional): Key to use in the model_object.components dictionary to access\n          the SourceModel object. If None, defaults to \"sources_combined\".\n        bin_size_x (float, optional): Size of the bins in the x-direction. Defaults to 1.\n        bin_size_y (float, optional): Size of the bins in the y-direction. Defaults to 1.\n        normalized_count_limit (float, optional): Limit for the normalized count to show on the map.\n            Defaults to 0.005.\n        burn_in (int, optional): Number of burn-in iterations to discard before calculating the statistics.\n            Defaults to 0.\n        show_summary_results (bool, optional): Flag to show the summary results on the map. Defaults to True.\n        show_fixed_source_locations (bool, optional): Flag to show the fixed sources location when present in one\n            of the sourcemaps. Defaults to True.\n\n    \"\"\"\n    if source_model_to_plot_key is None:\n        source_model_to_plot_key = \"sources_combined\"\n\n    source_model = model_object.components[source_model_to_plot_key]\n    sensor_object = model_object.sensor_object\n\n    source_locations = source_model.all_source_locations\n    emission_rates = source_model.emission_rate\n\n    ref_latitude = source_locations.ref_latitude\n    ref_longitude = source_locations.ref_longitude\n    ref_altitude = source_locations.ref_altitude\n\n    datetime_min_string = sensor_object.time.min().strftime(\"%d-%b-%Y, %H:%M:%S\")\n    datetime_max_string = sensor_object.time.max().strftime(\"%d-%b-%Y, %H:%M:%S\")\n\n    result_weighted, _, normalized_count, count_boolean, enu_points, summary_result = (\n        calculate_rectangular_statistics(\n            emission_rates=emission_rates,\n            source_locations=source_locations,\n            bin_size_x=bin_size_x,\n            bin_size_y=bin_size_y,\n            burn_in=burn_in,\n            normalized_count_limit=normalized_count_limit,\n        )\n    )\n\n    polygons = create_lla_polygons_from_xy_points(\n        points_array=enu_points,\n        ref_latitude=ref_latitude,\n        ref_longitude=ref_longitude,\n        ref_altitude=ref_altitude,\n        boolean_mask=count_boolean,\n    )\n\n    if show_summary_results:\n        summary_trace = self.create_summary_trace(summary_result=summary_result)\n\n    self.create_empty_map_figure(dict_key=\"count_map\")\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=normalized_count[count_boolean].flatten(),\n        opacity=0.8,\n        name=\"normalized_count\",\n        colorbar={\"title\": \"Normalized Count\", \"orientation\": \"h\"},\n        map_color_scale=\"Bluered\",\n    )\n    self.figure_dict[\"count_map\"].add_trace(trace)\n    self.figure_dict[\"count_map\"].update_layout(\n        map_style=\"carto-positron\",\n        map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n        title=f\"Source location probability \"\n        f\"(&gt;={normalized_count_limit}) for \"\n        f\"{datetime_min_string} to {datetime_max_string}\",\n        font_family=\"Futura\",\n        font_size=15,\n    )\n    sensor_object.plot_sensor_location(self.figure_dict[\"count_map\"])\n    self.figure_dict[\"count_map\"].update_traces(showlegend=False)\n\n    adjusted_result_weights = result_weighted.copy()\n    adjusted_result_weights[adjusted_result_weights == 0] = np.nan\n\n    median_of_all_emissions = np.nanmedian(adjusted_result_weights, axis=2)\n\n    self.create_empty_map_figure(dict_key=\"median_map\")\n\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=median_of_all_emissions[count_boolean].flatten(),\n        opacity=0.8,\n        name=\"median_emission\",\n        colorbar={\"title\": \"Median Emission\", \"orientation\": \"h\"},\n        map_color_scale=\"Bluered\",\n    )\n    self.figure_dict[\"median_map\"].add_trace(trace)\n    self.figure_dict[\"median_map\"].update_layout(\n        map_style=\"carto-positron\",\n        map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n        title=f\"Median emission rate estimate for {datetime_min_string} to {datetime_max_string}\",\n        font_family=\"Futura\",\n        font_size=15,\n    )\n    sensor_object.plot_sensor_location(self.figure_dict[\"median_map\"])\n    self.figure_dict[\"median_map\"].update_traces(showlegend=False)\n\n    iqr_of_all_emissions = np.nanquantile(a=adjusted_result_weights, q=0.75, axis=2) - np.nanquantile(\n        a=adjusted_result_weights, q=0.25, axis=2\n    )\n    self.create_empty_map_figure(dict_key=\"iqr_map\")\n\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=iqr_of_all_emissions[count_boolean].flatten(),\n        opacity=0.8,\n        name=\"iqr_emission\",\n        colorbar={\"title\": \"IQR\", \"orientation\": \"h\"},\n        map_color_scale=\"Bluered\",\n    )\n    self.figure_dict[\"iqr_map\"].add_trace(trace)\n    self.figure_dict[\"iqr_map\"].update_layout(\n        map_style=\"carto-positron\",\n        map={\"zoom\": 15, \"center\": {\"lon\": ref_longitude, \"lat\": ref_latitude}},\n        title=f\"Inter Quartile range (25%-75%) of emission rate \"\n        f\"estimate for {datetime_min_string} to {datetime_max_string}\",\n        font_family=\"Futura\",\n        font_size=15,\n    )\n    sensor_object.plot_sensor_location(self.figure_dict[\"iqr_map\"])\n    self.figure_dict[\"iqr_map\"].update_traces(showlegend=False)\n\n    if show_fixed_source_locations:\n        for key, _ in model_object.components.items():\n            if bool(re.search(\"fixed\", key)):\n                source_model_fixed = model_object.components[key]\n                source_locations_fixed = source_model_fixed.all_source_locations\n                source_location_fixed_lla = source_locations_fixed.to_lla()\n                sources_lat = source_location_fixed_lla.latitude[:, 0]\n                sources_lon = source_location_fixed_lla.longitude[:, 0]\n                fixed_source_location_trace = go.Scattermap(\n                    mode=\"markers\",\n                    lon=sources_lon,\n                    lat=sources_lat,\n                    name=f\"Fixed source locations, {key}\",\n                    marker={\"size\": 10, \"opacity\": 0.8},\n                )\n                self.figure_dict[\"count_map\"].add_trace(fixed_source_location_trace)\n                self.figure_dict[\"median_map\"].add_trace(fixed_source_location_trace)\n                self.figure_dict[\"iqr_map\"].add_trace(fixed_source_location_trace)\n\n    if show_summary_results:\n        self.figure_dict[\"count_map\"].add_trace(summary_trace)\n        self.figure_dict[\"count_map\"].update_traces(showlegend=True)\n        self.figure_dict[\"median_map\"].add_trace(summary_trace)\n        self.figure_dict[\"median_map\"].update_traces(showlegend=True)\n        self.figure_dict[\"iqr_map\"].add_trace(summary_trace)\n        self.figure_dict[\"iqr_map\"].update_traces(showlegend=True)\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.plot_coverage","title":"<code>plot_coverage(coordinates, couplings, threshold_function=np.max, coverage_threshold=6, opacity=0.8, map_color_scale='jet')</code>","text":"<p>Creates a coverage plot using the coverage function from Gaussian Plume.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>LLA object</code> <p>A LLA coordinate object containing a set of locations.</p> required <code>couplings</code> <code>array</code> <p>The calculated values of coupling (The 'A matrix') for a set of wind data.</p> required <code>threshold_function</code> <code>Callable</code> <p>Callable function which returns some single value that defines the                          maximum or 'threshold' coupling. Examples: np.quantile(q=0.9),                          np.max, np.mean. Defaults to np.max.</p> <code>max</code> <code>coverage_threshold</code> <code>float</code> <p>The threshold value of the estimated emission rate which is                                   considered to be within the coverage. Defaults to 6 kg/hr.</p> <code>6</code> <code>opacity</code> <code>float</code> <p>The opacity of the grid cells when they are plotted.</p> <code>0.8</code> <code>map_color_scale</code> <code>str</code> <p>The string which defines which plotly colour scale should be used when plotting                    the values.</p> <code>'jet'</code> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_coverage(\n    self,\n    coordinates: LLA,\n    couplings: np.ndarray,\n    threshold_function: Callable = np.max,\n    coverage_threshold: float = 6,\n    opacity: float = 0.8,\n    map_color_scale=\"jet\",\n):\n    \"\"\"Creates a coverage plot using the coverage function from Gaussian Plume.\n\n    Args:\n        coordinates (LLA object): A LLA coordinate object containing a set of locations.\n        couplings (np.array): The calculated values of coupling (The 'A matrix') for a set of wind data.\n        threshold_function (Callable, optional): Callable function which returns some single value that defines the\n                                     maximum or 'threshold' coupling. Examples: np.quantile(q=0.9),\n                                     np.max, np.mean. Defaults to np.max.\n        coverage_threshold (float, optional): The threshold value of the estimated emission rate which is\n                                              considered to be within the coverage. Defaults to 6 kg/hr.\n        opacity (float): The opacity of the grid cells when they are plotted.\n        map_color_scale (str): The string which defines which plotly colour scale should be used when plotting\n                               the values.\n\n    \"\"\"\n    coverage_values = GaussianPlume(source_map=None).compute_coverage(\n        couplings=couplings, threshold_function=threshold_function, coverage_threshold=coverage_threshold\n    )\n    self.plot_values_on_map(\n        dict_key=\"coverage_map\",\n        coordinates=coordinates,\n        values=coverage_values,\n        aggregate_function=np.max,\n        opacity=opacity,\n        map_color_scale=map_color_scale,\n    )\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.Plot.create_summary_trace","title":"<code>create_summary_trace(summary_result)</code>  <code>staticmethod</code>","text":"<p>Helper function to create the summary information to plot on top of map type plots.</p> <p>We use the summary result calculated through the support functions module to create a trace which contains the summary information for each source location.</p> <p>Parameters:</p> Name Type Description Default <code>summary_result</code> <code>DataFrame</code> <p>DataFrame containing the summary information for each source location.</p> required <p>Returns:</p> Name Type Description <code>summary_trace</code> <code>Scattermap</code> <p>Trace with summary information to plot on top of map type plots.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>@staticmethod\ndef create_summary_trace(\n    summary_result: pd.DataFrame,\n) -&gt; go.Scattermap:\n    \"\"\"Helper function to create the summary information to plot on top of map type plots.\n\n    We use the summary result calculated through the support functions module to create a trace which contains\n    the summary information for each source location.\n\n    Args:\n        summary_result (pd.DataFrame): DataFrame containing the summary information for each source location.\n\n    Returns:\n        summary_trace (go.Scattermap): Trace with summary information to plot on top of map type plots.\n\n    \"\"\"\n    summary_text_values = [\n        f\"&lt;b&gt;Source ID&lt;/b&gt;: {value}&lt;br&gt;\"\n        f\"&lt;b&gt;(Lon, Lat, Alt)&lt;/b&gt; ([deg], [deg], [m]):&lt;br&gt;\"\n        f\"({summary_result.longitude[value]:.7f}, \"\n        f\"{summary_result.latitude[value]:.7f}, {summary_result.altitude[value]:.3f})&lt;br&gt;\"\n        f\"&lt;b&gt;Height&lt;/b&gt;: {summary_result.height[value]:.3f} [m]&lt;br&gt;\"\n        f\"&lt;b&gt;Median emission rate&lt;/b&gt;: {summary_result.median_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;2.5% quantile&lt;/b&gt;: {summary_result.quantile_025[value]:.3f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;97.5% quantile&lt;/b&gt;: {summary_result.quantile_975[value]:.3f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;IQR&lt;/b&gt;: {summary_result.iqr_estimate[value]:.4f} [kg/hr]&lt;br&gt;\"\n        f\"&lt;b&gt;Blob present during&lt;/b&gt;: \"\n        f\"{summary_result.absolute_count_iterations[value]:.0f} iterations&lt;br&gt;\"\n        f\"&lt;b&gt;Blob likelihood&lt;/b&gt;: {summary_result.blob_likelihood[value]:.5f}&lt;br&gt;\"\n        for value in summary_result.index\n    ]\n\n    summary_trace = go.Scattermap(\n        lat=summary_result.latitude,\n        lon=summary_result.longitude,\n        mode=\"markers\",\n        marker=go.scattermap.Marker(size=14, color=\"black\"),\n        text=summary_text_values,\n        name=\"Summary\",\n        hoverinfo=\"text\",\n    )\n\n    return summary_trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.lighter_rgb","title":"<code>lighter_rgb(rbg_string)</code>","text":"<p>Takes in an RGB string and returns a lighter version of this colour.</p> <p>The colour is made lighter by increasing the magnitude of the RGB values by half of the difference between the original value and the number 255.</p> <p>Parameters:</p> Name Type Description Default <code>rbg_string</code> <code>str</code> <p>An RGB string.</p> required Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def lighter_rgb(rbg_string: str) -&gt; str:\n    \"\"\"Takes in an RGB string and returns a lighter version of this colour.\n\n    The colour is made lighter by increasing the magnitude of the RGB values by half of the difference between the\n    original value and the number 255.\n\n    Arguments:\n        rbg_string (str): An RGB string.\n\n    \"\"\"\n    rbg_string = rbg_string[4:-1]\n    rbg_string = rbg_string.replace(\" \", \"\")\n    colors = rbg_string.split(\",\")\n    colors_out = [np.nan, np.nan, np.nan]\n\n    for i, color in enumerate(colors):\n        color = int(color)\n        color = min(int(round(color + ((255 - color) * 0.5))), 255)\n        colors_out[i] = color\n\n    return f\"rgb({colors_out[0]}, {colors_out[1]}, {colors_out[2]})\"\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_quantiles_from_array","title":"<code>plot_quantiles_from_array(fig, x_values, y_values, quantiles, color, name=None)</code>","text":"<p>Plot quantiles over y-values against x-values.</p> <p>Assuming x-values have size N and y-values have size [N x M] where the second dimension is the dimension to calculate the quantiles over.</p> <p>Will plot the median of the y-values as a solid line and a filled area between the lower and upper specified quantile.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure to add the traces on.</p> required <code>x_values</code> <code>Union[ndarray, DatetimeArray]</code> <p>Numpy array containing the x-values to plot.</p> required <code>y_values</code> <code>ndarray</code> <p>Numpy array containing the y-values to calculate the quantiles for.</p> required <code>quantiles</code> <code>Union[tuple, list, ndarray]</code> <p>Values of upper and lower quantile to plot in range (0-100)</p> required <code>color</code> <code>str</code> <p>RGB string specifying color for quantile fill plot.</p> required <code>name</code> <code>str</code> <p>Optional string name to show in the legend.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure with the quantile filled traces and median trace added on it.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_quantiles_from_array(\n    fig: go.Figure,\n    x_values: Union[np.ndarray, pd.arrays.DatetimeArray],\n    y_values: np.ndarray,\n    quantiles: Union[tuple, list, np.ndarray],\n    color: str,\n    name: str = None,\n) -&gt; go.Figure:\n    \"\"\"Plot quantiles over y-values against x-values.\n\n    Assuming x-values have size N and y-values have size [N x M] where the second dimension is the dimension to\n    calculate the quantiles over.\n\n    Will plot the median of the y-values as a solid line and a filled area between the lower and upper specified\n    quantile.\n\n    Args:\n        fig (go.Figure): Plotly figure to add the traces on.\n        x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Numpy array containing the x-values to plot.\n        y_values (np.ndarray): Numpy array containing the y-values to calculate the quantiles for.\n        quantiles (Union[tuple, list, np.ndarray]): Values of upper and lower quantile to plot in range (0-100)\n        color (str): RGB string specifying color for quantile fill plot.\n        name (str, optional): Optional string name to show in the legend.\n\n    Returns:\n         fig (go.Figure): Plotly figure with the quantile filled traces and median trace added on it.\n\n    \"\"\"\n    color_fill = f\"rgba{color[3:-1]}, 0.3)\"\n\n    median_trace = go.Scatter(\n        x=x_values,\n        y=np.median(y_values, axis=1),\n        mode=\"lines\",\n        line={\"width\": 3, \"color\": color},\n        name=f\"Median for {name}\",\n        legendgroup=name,\n        showlegend=False,\n    )\n\n    lower_quantile_trace = go.Scatter(\n        x=x_values,\n        y=np.quantile(y_values, axis=1, q=quantiles[0] / 100),\n        mode=\"lines\",\n        line={\"width\": 0, \"color\": color_fill},\n        name=f\"{quantiles[0]}% quantile\",\n        legendgroup=name,\n        showlegend=False,\n    )\n\n    upper_quantile_trace = go.Scatter(\n        x=x_values,\n        y=np.quantile(y_values, axis=1, q=quantiles[1] / 100),\n        fill=\"tonexty\",\n        fillcolor=color_fill,\n        mode=\"lines\",\n        line={\"width\": 0, \"color\": color_fill},\n        name=f\"{quantiles[1]}% quantile\",\n        legendgroup=name,\n        showlegend=False,\n    )\n\n    fig.add_trace(median_trace)\n    fig.add_trace(lower_quantile_trace)\n    fig.add_trace(upper_quantile_trace)\n\n    return fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.create_trace_specifics","title":"<code>create_trace_specifics(object_to_plot, **kwargs)</code>","text":"<p>Specification of different traces of single variables.</p> <p>Provides all details for plots where we want to plot a single variable as a line plot. Based on the object_to_plot we select the correct plot to show.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[Type[SlabAndSpike], SourceModel, MCMC]</code> <p>Object which we want to plot a single variable from</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the following key/values: x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot. y_values (np.ndarray): Numpy array containing the y-values to use in plotting. dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the Plot     class. title_text (str): String title of the plot. x_label (str): String label of x-axis. y_label (str) : String label of y-axis. name (str): String name to show in the legend. color (str): RGB string specifying color for plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When no specifics are defined for the inputted object to plot.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def create_trace_specifics(object_to_plot: Union[Type[SlabAndSpike], SourceModel, MCMC], **kwargs: Any) -&gt; dict:\n    \"\"\"Specification of different traces of single variables.\n\n    Provides all details for plots where we want to plot a single variable as a line plot. Based on the object_to_plot\n    we select the correct plot to show.\n\n    Args:\n        object_to_plot (Union[Type[SlabAndSpike], SourceModel, MCMC]): Object which we want to plot a single\n            variable from\n        **kwargs (Any): Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not\n            applicable to all.\n\n    Returns:\n        dict: A dictionary with the following key/values:\n            x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot.\n            y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n            dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the Plot\n                class.\n            title_text (str): String title of the plot.\n            x_label (str): String label of x-axis.\n            y_label (str) : String label of y-axis.\n            name (str): String name to show in the legend.\n            color (str): RGB string specifying color for plot.\n\n    Raises:\n        ValueError: When no specifics are defined for the inputted object to plot.\n\n    \"\"\"\n    if isinstance(object_to_plot, SourceModel):\n        dict_key = kwargs.pop(\"dict_key\", \"number_of_sources_plot\")\n        title_text = \"Number of Sources 'on' against MCMC iterations\"\n        x_label = MCMC_ITERATION_NUMBER_LITERAL\n        y_label = \"Number of Sources 'on'\"\n        y_values = object_to_plot.number_on_sources\n        x_values = np.array(range(y_values.size))\n        color = \"rgb(248, 156, 116)\"\n        name = \"Number of Sources 'on'\"\n\n    elif isinstance(object_to_plot, MCMC):\n        dict_key = kwargs.pop(\"dict_key\", \"log_posterior_plot\")\n        title_text = \"Log posterior values against MCMC iterations\"\n        x_label = MCMC_ITERATION_NUMBER_LITERAL\n        y_label = \"Log Posterior&lt;br&gt;Value\"\n        y_values = object_to_plot.store[\"log_post\"].flatten()\n        x_values = np.array(range(y_values.size))\n        color = RGB_LIGHT_BLUE\n        name = \"Log Posterior\"\n\n        if \"burn_in\" not in kwargs:\n            warnings.warn(\"Burn in is not specified for the Log Posterior plot, are you sure this is correct?\")\n\n    else:\n        raise ValueError(\"No values to plot\")\n\n    return {\n        \"x_values\": x_values,\n        \"y_values\": y_values,\n        \"dict_key\": dict_key,\n        \"title_text\": title_text,\n        \"x_label\": x_label,\n        \"y_label\": y_label,\n        \"name\": name,\n        \"color\": color,\n    }\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.create_plot_specifics","title":"<code>create_plot_specifics(object_to_plot, sensor_object, plot_type='', **kwargs)</code>","text":"<p>Specification of different traces where we want to plot a trace for each sensor.</p> <p>Provides all details for plots where we want to plot a single variable for each sensor as a line or box plot. Based on the object_to_plot we select the correct plot to show.</p> <p>When plotting the MCMC Observations and Predicted Model Values Against Time plot we are assuming time axis is the same for all sensors w.r.t. the fitted values from the MCMC store attribute, so we are only using the time axis from the first sensor.</p> <p>Parameters:</p> Name Type Description Default <code>object_to_plot</code> <code>Union[ErrorModel, PerSensor, MCMC]</code> <p>Object which we want to plot a single variable from</p> required <code>sensor_object</code> <code>SensorGroup</code> <p>SensorGroup object associated with the object_to_plot</p> required <code>plot_type</code> <code>str</code> <p>String specifying either a line or a box plot.</p> <code>''</code> <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not applicable to all.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the following key/values: x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot. y_values (np.ndarray): Numpy array containing the y-values to use in plotting. dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the     Plot class. title_text (str): String title of the plot. x_label (str): String label of x-axis. y_label (str): String label of y-axis. plot_type (str): Type of plot which needs to be generated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When no specifics are defined for the inputted object to plot.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def create_plot_specifics(\n    object_to_plot: Union[ErrorModel, PerSensor, MCMC], sensor_object: SensorGroup, plot_type: str = \"\", **kwargs: Any\n) -&gt; dict:\n    \"\"\"Specification of different traces where we want to plot a trace for each sensor.\n\n    Provides all details for plots where we want to plot a single variable for each sensor as a line or box plot.\n    Based on the object_to_plot we select the correct plot to show.\n\n    When plotting the MCMC Observations and Predicted Model Values Against Time plot we are assuming time axis is the\n    same for all sensors w.r.t. the fitted values from the MCMC store attribute, so we are only using the time axis\n    from the first sensor.\n\n    Args:\n        object_to_plot (Union[ErrorModel, PerSensor, MCMC]): Object which we want to plot a single variable from\n        sensor_object (SensorGroup): SensorGroup object associated with the object_to_plot\n        plot_type (str, optional): String specifying either a line or a box plot.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in or dict_key, used in some specific plots but not\n            applicable to all.\n\n    Returns:\n        dict: A dictionary with the following key/values:\n            x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): Array containing the x-values to plot.\n            y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n            dict_key (str): String key associated with this plot to be used in the figure_dict attribute of the\n                Plot class.\n            title_text (str): String title of the plot.\n            x_label (str): String label of x-axis.\n            y_label (str): String label of y-axis.\n            plot_type (str): Type of plot which needs to be generated.\n\n    Raises:\n        ValueError: When no specifics are defined for the inputted object to plot.\n\n    \"\"\"\n    if isinstance(object_to_plot, ErrorModel):\n        y_values = np.sqrt(1 / object_to_plot.precision)\n        x_values = np.array(range(y_values.shape[1]))\n\n        if plot_type == \"line\":\n            dict_key = kwargs.pop(\"dict_key\", \"error_model_iterations\")\n            title_text = \"Estimated Error Model Values\"\n            x_label = MCMC_ITERATION_NUMBER_LITERAL\n            y_label = \"Estimated Error Model&lt;br&gt;Standard Deviation (ppm)\"\n\n        elif plot_type == \"box\":\n            dict_key = kwargs.pop(\"dict_key\", \"error_model_distributions\")\n            title_text = \"Distributions of Estimated Error Model Values After Burn-In\"\n            x_label = \"Sensor\"\n            y_label = \"Estimated Error Model&lt;br&gt;Standard Deviation (ppm)\"\n\n        else:\n            raise ValueError(\"Only line and box are allowed for the plot_type argument for ErrorModel\")\n\n        if \"burn_in\" not in kwargs:\n            warnings.warn(\"Burn in is not specified for the ErrorModel plot, are you sure this is correct?\")\n\n    elif isinstance(object_to_plot, PerSensor):\n        offset_sensor_name = list(sensor_object.values())[0].label\n        y_values = object_to_plot.offset\n        nan_row = np.tile(np.nan, (1, y_values.shape[1]))\n        y_values = np.concatenate((nan_row, y_values), axis=0)\n        x_values = np.array(range(y_values.shape[1]))\n\n        if plot_type == \"line\":\n            dict_key = kwargs.pop(\"dict_key\", \"offset_iterations\")\n            title_text = f\"Estimated Value of Offset w.r.t. {offset_sensor_name}\"\n            x_label = MCMC_ITERATION_NUMBER_LITERAL\n            y_label = \"Estimated Offset&lt;br&gt;Value (ppm)\"\n\n        elif plot_type == \"box\":\n            dict_key = kwargs.pop(\"dict_key\", \"offset_distributions\")\n            title_text = f\"Distributions of Estimated Offset Values w.r.t. {offset_sensor_name} After Burn-In\"\n            x_label = \"Sensor\"\n            y_label = \"Estimated Offset&lt;br&gt;Value (ppm)\"\n\n        else:\n            raise ValueError(\"Only line and box are allowed for the plot_type argument for PerSensor OffsetModel\")\n\n        if \"burn_in\" not in kwargs:\n            warnings.warn(\"Burn in is not specified for the PerSensor OffsetModel plot, are you sure this is correct?\")\n\n    elif isinstance(object_to_plot, MCMC):\n        y_values = object_to_plot.store[\"y\"]\n        x_values = list(sensor_object.values())[0].time\n        dict_key = kwargs.pop(\"dict_key\", \"fitted_values\")\n        title_text = \"Observations and Predicted Model Values Against Time\"\n        x_label = \"Time\"\n        y_label = \"Concentration (ppm)\"\n        plot_type = \"line\"\n\n    else:\n        raise ValueError(\"No values to plot\")\n\n    return {\n        \"x_values\": x_values,\n        \"y_values\": y_values,\n        \"dict_key\": dict_key,\n        \"title_text\": title_text,\n        \"x_label\": x_label,\n        \"y_label\": y_label,\n        \"plot_type\": plot_type,\n    }\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_single_scatter","title":"<code>plot_single_scatter(fig, x_values, y_values, color, name, **kwargs)</code>","text":"<p>Plots a single scatter trace on the supplied figure object.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure to add the trace to.</p> required <code>x_values</code> <code>Union[ndarray, DatetimeArray]</code> <p>X values to plot</p> required <code>y_values</code> <code>ndarray</code> <p>Numpy array containing the y-values to use in plotting.</p> required <code>color</code> <code>str</code> <p>RGB color string to use for this trace.</p> required <code>name</code> <code>str</code> <p>String name to show in the legend.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments, e.g. burn_in, legend_group, show_legend, used in some specific plots but not applicable to all.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure with the trace added to it.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_single_scatter(\n    fig: go.Figure,\n    x_values: Union[np.ndarray, pd.arrays.DatetimeArray],\n    y_values: np.ndarray,\n    color: str,\n    name: str,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Plots a single scatter trace on the supplied figure object.\n\n    Args:\n        fig (go.Figure): Plotly figure to add the trace to.\n        x_values (Union[np.ndarray, pd.arrays.DatetimeArray]): X values to plot\n        y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n        color (str): RGB color string to use for this trace.\n        name (str): String name to show in the legend.\n        **kwargs (Any): Additional key word arguments, e.g. burn_in, legend_group, show_legend, used in some specific\n            plots but not applicable to all.\n\n    Returns:\n        fig (go.Figure): Plotly figure with the trace added to it.\n\n    \"\"\"\n    burn_in = kwargs.pop(\"burn_in\", 0)\n    legend_group = kwargs.pop(\"legend_group\", name)\n    show_legend = kwargs.pop(\"show_legend\", True)\n    if burn_in &gt; 0:\n        fig.add_trace(\n            go.Scatter(\n                x=x_values[: burn_in + 1],\n                y=y_values[: burn_in + 1],\n                name=name,\n                mode=\"lines\",\n                line={\"width\": 3, \"color\": lighter_rgb(color)},\n                legendgroup=legend_group,\n                showlegend=False,\n            )\n        )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x_values[burn_in:],\n            y=y_values[burn_in:],\n            name=name,\n            mode=\"lines\",\n            line={\"width\": 3, \"color\": color},\n            legendgroup=legend_group,\n            showlegend=show_legend,\n        )\n    )\n\n    return fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_single_box","title":"<code>plot_single_box(fig, y_values, color, name)</code>","text":"<p>Plot a single box plot trace on the plot figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure to add the trace to.</p> required <code>y_values</code> <code>ndarray</code> <p>Numpy array containing the y-values to use in plotting.</p> required <code>color</code> <code>str</code> <p>RGB color string to use for this trace.</p> required <code>name</code> <code>str</code> <p>String name to show in the legend.</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure with the trace added to it.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_single_box(fig: go.Figure, y_values: np.ndarray, color: str, name: str) -&gt; go.Figure:\n    \"\"\"Plot a single box plot trace on the plot figure.\n\n    Args:\n        fig (go.Figure): Plotly figure to add the trace to.\n        y_values (np.ndarray): Numpy array containing the y-values to use in plotting.\n        color (str): RGB color string to use for this trace.\n        name (str): String name to show in the legend.\n\n    Returns:\n        fig (go.Figure): Plotly figure with the trace added to it.\n\n    \"\"\"\n    fig.add_trace(go.Box(y=y_values, name=name, legendgroup=name, marker={\"color\": color}))\n\n    return fig\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_polygons_on_map","title":"<code>plot_polygons_on_map(polygons, values, opacity, map_color_scale, **kwargs)</code>","text":"<p>Plot a set of polygons on a map.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[ndarray, list]</code> <p>Numpy array or list containing the polygons to plot.</p> required <code>values</code> <code>ndarray</code> <p>Numpy array consistent with polygons containing the value which is                  used in coloring the polygons on the map.</p> required <code>opacity</code> <code>float</code> <p>Float between 0 and 1 specifying the opacity of the polygon fill color.</p> required <code>map_color_scale</code> <code>str</code> <p>The string which defines which plotly color scale.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional key word arguments which can be passed on the go.Choroplethmap object (will override the default values as specified in this function)</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>trace</code> <code>Choroplethmap</code> <p>go.Choroplethmap trace with the colored polygons which can be added to a go.Figure object.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_polygons_on_map(\n    polygons: Union[np.ndarray, list], values: np.ndarray, opacity: float, map_color_scale: str, **kwargs: Any\n) -&gt; go.Choroplethmap:\n    \"\"\"Plot a set of polygons on a map.\n\n    Args:\n        polygons (Union[np.ndarray, list]): Numpy array or list containing the polygons to plot.\n        values (np.ndarray): Numpy array consistent with polygons containing the value which is\n                             used in coloring the polygons on the map.\n        opacity (float): Float between 0 and 1 specifying the opacity of the polygon fill color.\n        map_color_scale (str): The string which defines which plotly color scale.\n        **kwargs (Any): Additional key word arguments which can be passed on the go.Choroplethmap object\n            (will override the default values as specified in this function)\n\n    Returns:\n        trace: go.Choroplethmap trace with the colored polygons which can be added to a go.Figure object.\n\n    \"\"\"\n    polygon_id = list(range(values.shape[0]))\n    feature_collection = FeatureCollection([Feature(geometry=polygons[idx], id_value=idx) for idx in polygon_id])\n    text_box = [\n        f\"&lt;b&gt;Polygon ID&lt;/b&gt;: {counter:d}&lt;br&gt;&lt;b&gt;Center (lon, lat)&lt;/b&gt;: \"\n        f\"({polygons[counter].centroid.coords[0][0]:.4f}, {polygons[counter].centroid.coords[0][1]:.4f})&lt;br&gt;\"\n        f\"&lt;b&gt;Value&lt;/b&gt;: {values[counter]:f}&lt;br&gt;\"\n        for counter in polygon_id\n    ]\n\n    trace_options = {\n        \"geojson\": feature_collection,\n        \"featureidkey\": \"id_value\",\n        \"locations\": polygon_id,\n        \"z\": values,\n        \"marker\": {\"line\": {\"width\": 0}, \"opacity\": opacity},\n        \"hoverinfo\": \"text\",\n        \"text\": text_box,\n        \"name\": \"Values\",\n        \"colorscale\": map_color_scale,\n        \"colorbar\": {\"title\": \"Values\"},\n        \"showlegend\": True,\n    }\n\n    for key, value in kwargs.items():\n        trace_options[key] = value\n\n    trace = go.Choroplethmap(**trace_options)\n\n    return trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_regular_grid","title":"<code>plot_regular_grid(coordinates, values, opacity, map_color_scale, tolerance=1e-07, unit='kg/hr', name='Values')</code>","text":"<p>Plots a regular grid of LLA data onto a map.</p> <p>So long as the input array is regularly spaced, the value of the spacing is found. A set of rectangles are defined where the centre of the rectangle is the LLA coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>LLA object</code> <p>A LLA coordinate object containing a set of locations.</p> required <code>values</code> <code>array</code> <p>A set of values that correspond to locations specified in the coordinates.</p> required <code>opacity</code> <code>float</code> <p>The opacity of the grid cells when they are plotted.</p> required <code>map_color_scale</code> <code>str</code> <p>The string which defines which plotly color scale should be used when plotting the values.</p> required <code>tolerance</code> <code>float</code> <p>Absolute value above which the difference between values is considered significant.                          Used to calculate the regular grid of coordinate values. Defaults to 1e-7.</p> <code>1e-07</code> <code>unit</code> <code>str</code> <p>The unit to be added to the colorscale. Defaults to kg/hr.</p> <code>'kg/hr'</code> <code>name</code> <code>str</code> <p>Name for the trace to be used in the color bar as well</p> <code>'Values'</code> <p>Returns:</p> Name Type Description <code>trace</code> <code>Choroplethmap</code> <p>Trace with the colored polygons which can be added to a go.Figure object.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_regular_grid(\n    coordinates: LLA,\n    values: np.ndarray,\n    opacity: float,\n    map_color_scale: str,\n    tolerance: float = 1e-7,\n    unit: str = \"kg/hr\",\n    name=\"Values\",\n) -&gt; go.Choroplethmap:\n    \"\"\"Plots a regular grid of LLA data onto a map.\n\n    So long as the input array is regularly spaced, the value of the spacing is found. A set of rectangles are defined\n    where the centre of the rectangle is the LLA coordinate.\n\n    Args:\n        coordinates (LLA object): A LLA coordinate object containing a set of locations.\n        values (np.array): A set of values that correspond to locations specified in the coordinates.\n        opacity (float): The opacity of the grid cells when they are plotted.\n        map_color_scale (str): The string which defines which plotly color scale should be used when plotting\n            the values.\n        tolerance (float, optional): Absolute value above which the difference between values is considered significant.\n                                     Used to calculate the regular grid of coordinate values. Defaults to 1e-7.\n        unit (str, optional): The unit to be added to the colorscale. Defaults to kg/hr.\n        name (str, optional): Name for the trace to be used in the color bar as well\n\n    Returns:\n        trace (go.Choroplethmap): Trace with the colored polygons which can be added to a go.Figure object.\n\n    \"\"\"\n    _, gridsize_lat = is_regularly_spaced(coordinates.latitude, tolerance=tolerance)\n    _, gridsize_lon = is_regularly_spaced(coordinates.longitude, tolerance=tolerance)\n\n    polygons = [\n        geometry.box(\n            coordinates.longitude[idx] - gridsize_lon / 2,\n            coordinates.latitude[idx] - gridsize_lat / 2,\n            coordinates.longitude[idx] + gridsize_lon / 2,\n            coordinates.latitude[idx] + gridsize_lat / 2,\n        )\n        for idx in range(coordinates.nof_observations)\n    ]\n\n    trace = plot_polygons_on_map(\n        polygons=polygons,\n        values=values,\n        opacity=opacity,\n        name=name,\n        colorbar={\"title\": name + \"&lt;br&gt;\" + unit},\n        map_color_scale=map_color_scale,\n    )\n\n    return trace\n</code></pre>"},{"location":"pyelq/plotting/plot/#pyelq.plotting.plot.plot_hexagonal_grid","title":"<code>plot_hexagonal_grid(coordinates, values, opacity, map_color_scale, num_hexagons, show_positions, aggregate_function=np.sum)</code>","text":"<p>Plots a set of values into hexagonal bins with respect to the location of the values.</p> <p>Any data points that fall within the area of a hexagon are used to perform aggregation and bin the data. See: https://plotly.com/python-api-reference/generated/plotly.figure_factory.create_hexbin_mapbox.html</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>LLA object</code> <p>A LLA coordinate object containing a set of locations.</p> required <code>values</code> <code>array</code> <p>A set of values that correspond to locations specified in the coordinates.</p> required <code>opacity</code> <code>float</code> <p>The opacity of the hexagons when they are plotted.</p> required <code>map_color_scale</code> <code>str</code> <p>Colour scale for plotting values.</p> required <code>num_hexagons</code> <code>Union[int, None]</code> <p>The number of hexagons which define the horizontal axis of the plot.</p> required <code>show_positions</code> <code>bool</code> <p>A flag to determine whether the original data should be shown alongside the binning hexagons.</p> required <code>aggregate_function</code> <code>Callable</code> <p>Function which to apply on the data in each hexagonal bin to aggregate the data and visualise the result.</p> <code>sum</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A plotly go figure representing the data which was submitted to this function.</p> Source code in <code>src/pyelq/plotting/plot.py</code> <pre><code>def plot_hexagonal_grid(\n    coordinates: LLA,\n    values: np.ndarray,\n    opacity: float,\n    map_color_scale: str,\n    num_hexagons: Union[int, None],\n    show_positions: bool,\n    aggregate_function: Callable = np.sum,\n):\n    \"\"\"Plots a set of values into hexagonal bins with respect to the location of the values.\n\n    Any data points that fall within the area of a hexagon are used to perform aggregation and bin the data.\n    See: https://plotly.com/python-api-reference/generated/plotly.figure_factory.create_hexbin_mapbox.html\n\n    Args:\n        coordinates (LLA object): A LLA coordinate object containing a set of locations.\n        values (np.array): A set of values that correspond to locations specified in the coordinates.\n        opacity (float): The opacity of the hexagons when they are plotted.\n        map_color_scale (str): Colour scale for plotting values.\n        num_hexagons (Union[int, None]): The number of hexagons which define the *horizontal* axis of the plot.\n        show_positions (bool): A flag to determine whether the original data should be shown alongside\n            the binning hexagons.\n        aggregate_function (Callable, optional): Function which to apply on the data in each hexagonal bin to aggregate\n            the data and visualise the result.\n\n    Returns:\n        (go.Figure): A plotly go figure representing the data which was submitted to this function.\n\n    \"\"\"\n    if num_hexagons is None:\n        num_hexagons = max(1, np.ceil((np.max(coordinates.longitude) - np.min(coordinates.longitude)) / 0.25))\n\n    coordinates = coordinates.to_lla()\n\n    hex_plot = ff.create_hexbin_mapbox(\n        lat=coordinates.latitude,\n        lon=coordinates.longitude,\n        color=values,\n        nx_hexagon=num_hexagons,\n        opacity=opacity,\n        agg_func=aggregate_function,\n        color_continuous_scale=map_color_scale,\n        show_original_data=show_positions,\n        original_data_marker={\"color\": \"black\"},\n    )\n\n    return hex_plot\n</code></pre>"},{"location":"pyelq/sensor/beam/","title":"Beam","text":""},{"location":"pyelq/sensor/beam/#beam","title":"Beam","text":"<p>Beam module.</p> <p>Subclass of Sensor. Used for beam sensors</p>"},{"location":"pyelq/sensor/beam/#pyelq.sensor.beam.Beam","title":"<code>Beam</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Sensor</code></p> <p>Defines Beam sensor class.</p> <p>Location attribute from superclass is assumed to be a Coordinate class object containing 2 locations, the first of the sensor and the second of the retro.</p> <p>Attributes:</p> Name Type Description <code>n_beam_knots</code> <code>int</code> <p>Number of beam knots to evaluate along a single beam</p> Source code in <code>src/pyelq/sensor/beam.py</code> <pre><code>@dataclass\nclass Beam(Sensor):\n    \"\"\"Defines Beam sensor class.\n\n    Location attribute from superclass is assumed to be a Coordinate class object containing 2 locations, the first of\n    the sensor and the second of the retro.\n\n    Attributes:\n        n_beam_knots (int, optional): Number of beam knots to evaluate along a single beam\n\n    \"\"\"\n\n    n_beam_knots: int = 50\n\n    @property\n    def midpoint(self) -&gt; np.ndarray:\n        \"\"\"np.ndarray: Midpoint of the beam.\"\"\"\n        return np.mean(self.location.to_array(), axis=0)\n\n    def make_beam_knots(self, ref_latitude, ref_longitude, ref_altitude=0) -&gt; np.ndarray:\n        \"\"\"Create beam knot locations.\n\n        Creates beam knot locations based on location attribute and n_beam_knot attribute.\n        Results in an array of beam knot locations of shape [n_beam_knots x 3]. Have to provide a reference point in\n        order to create the beam knots in a local frame, spaced in meters\n\n        Args:\n            ref_latitude (float): Reference latitude in degrees\n            ref_longitude (float): Reference longitude in degrees\n            ref_altitude (float, optional): Reference altitude in meters\n\n        \"\"\"\n        temp_location = self.location.to_enu(\n            ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude\n        ).to_array()\n        beam_knot_array = np.linspace(temp_location[0, :], temp_location[1, :], num=self.n_beam_knots, endpoint=True)\n        return beam_knot_array\n</code></pre>"},{"location":"pyelq/sensor/beam/#pyelq.sensor.beam.Beam.midpoint","title":"<code>midpoint</code>  <code>property</code>","text":"<p>np.ndarray: Midpoint of the beam.</p>"},{"location":"pyelq/sensor/beam/#pyelq.sensor.beam.Beam.make_beam_knots","title":"<code>make_beam_knots(ref_latitude, ref_longitude, ref_altitude=0)</code>","text":"<p>Create beam knot locations.</p> <p>Creates beam knot locations based on location attribute and n_beam_knot attribute. Results in an array of beam knot locations of shape [n_beam_knots x 3]. Have to provide a reference point in order to create the beam knots in a local frame, spaced in meters</p> <p>Parameters:</p> Name Type Description Default <code>ref_latitude</code> <code>float</code> <p>Reference latitude in degrees</p> required <code>ref_longitude</code> <code>float</code> <p>Reference longitude in degrees</p> required <code>ref_altitude</code> <code>float</code> <p>Reference altitude in meters</p> <code>0</code> Source code in <code>src/pyelq/sensor/beam.py</code> <pre><code>def make_beam_knots(self, ref_latitude, ref_longitude, ref_altitude=0) -&gt; np.ndarray:\n    \"\"\"Create beam knot locations.\n\n    Creates beam knot locations based on location attribute and n_beam_knot attribute.\n    Results in an array of beam knot locations of shape [n_beam_knots x 3]. Have to provide a reference point in\n    order to create the beam knots in a local frame, spaced in meters\n\n    Args:\n        ref_latitude (float): Reference latitude in degrees\n        ref_longitude (float): Reference longitude in degrees\n        ref_altitude (float, optional): Reference altitude in meters\n\n    \"\"\"\n    temp_location = self.location.to_enu(\n        ref_latitude=ref_latitude, ref_longitude=ref_longitude, ref_altitude=ref_altitude\n    ).to_array()\n    beam_knot_array = np.linspace(temp_location[0, :], temp_location[1, :], num=self.n_beam_knots, endpoint=True)\n    return beam_knot_array\n</code></pre>"},{"location":"pyelq/sensor/satellite/","title":"Satellite","text":""},{"location":"pyelq/sensor/satellite/#satellite","title":"Satellite","text":"<p>Satellite module.</p> <p>Subclass of Sensor. Mainly used to accommodate satellite sensor TROPOMI. See: http://www.tropomi.eu/data-products/methane : http: //www.tropomi.eu/data-products/methane and http://www.tropomi.eu/data-products/nitrogen-dioxide</p>"},{"location":"pyelq/sensor/satellite/#pyelq.sensor.satellite.Satellite","title":"<code>Satellite</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Sensor</code></p> <p>Defines Satellite sensor class.</p> <p>Attributes:</p> Name Type Description <code>qa_value</code> <code>ndarray</code> <p>Array containing quality values associated with the observations.</p> <code>precision</code> <code>ndarray</code> <p>Array containing precision values associated with the observations.</p> <code>precision_kernel</code> <code>ndarray</code> <p>Array containing precision kernel values associated with the observations.</p> <code>ground_pixel</code> <code>ndarray</code> <p>Array containing ground pixels values associated with the observations. Ground pixels are indicating the dimension perpendicular to the flight direction.</p> <code>scanline</code> <code>ndarray</code> <p>Array containing scanline values associated with the observations. Scanlines are indicating the dimension in the direction of flight.</p> <code>orbit</code> <code>ndarray</code> <p>Array containing orbit values associated with the observations.</p> <code>pixel_bounds</code> <code>ndarray</code> <p>Array containing Polygon features which define the pixel bounds.</p> Source code in <code>src/pyelq/sensor/satellite.py</code> <pre><code>@dataclass\nclass Satellite(Sensor):\n    \"\"\"Defines Satellite sensor class.\n\n    Attributes:\n        qa_value (np.ndarray, optional): Array containing quality values associated with the observations.\n        precision (np.ndarray, optional): Array containing precision values associated with the observations.\n        precision_kernel (np.ndarray, optional): Array containing precision kernel values associated with the\n            observations.\n        ground_pixel (np.ndarray, optional): Array containing ground pixels values associated with the observations.\n            Ground pixels are indicating the dimension perpendicular to the flight direction.\n        scanline (np.ndarray, optional): Array containing scanline values associated with the observations.\n            Scanlines are indicating the dimension in the direction of flight.\n        orbit (np.ndarray, optional): Array containing orbit values associated with the observations.\n        pixel_bounds (np.ndarray, optional): Array containing Polygon features which define the pixel bounds.\n\n    \"\"\"\n\n    qa_value: np.ndarray = field(init=False)\n    precision: np.ndarray = field(init=False)\n    precision_kernel: np.ndarray = field(init=False)\n    ground_pixel: np.ndarray = field(init=False)\n    scanline: np.ndarray = field(init=False)\n    orbit: np.ndarray = field(init=False, default=None)\n    pixel_bounds: np.ndarray = field(init=False)\n\n    def get_orbits(self) -&gt; np.ndarray:\n        \"\"\"Gets the unique orbits which are present in the data.\n\n        Raises:\n            ValueError: When orbits attribute is None\n\n        Returns:\n            np.ndarray: Unique orbits present in the data.\n\n        \"\"\"\n        if self.orbit is None:\n            raise ValueError(\"Orbits attribute is None\")\n        return np.unique(self.orbit)\n</code></pre>"},{"location":"pyelq/sensor/satellite/#pyelq.sensor.satellite.Satellite.get_orbits","title":"<code>get_orbits()</code>","text":"<p>Gets the unique orbits which are present in the data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When orbits attribute is None</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Unique orbits present in the data.</p> Source code in <code>src/pyelq/sensor/satellite.py</code> <pre><code>def get_orbits(self) -&gt; np.ndarray:\n    \"\"\"Gets the unique orbits which are present in the data.\n\n    Raises:\n        ValueError: When orbits attribute is None\n\n    Returns:\n        np.ndarray: Unique orbits present in the data.\n\n    \"\"\"\n    if self.orbit is None:\n        raise ValueError(\"Orbits attribute is None\")\n    return np.unique(self.orbit)\n</code></pre>"},{"location":"pyelq/sensor/sensor/","title":"Overview","text":""},{"location":"pyelq/sensor/sensor/#sensor-classes","title":"Sensor classes","text":"<p>An overview of the sensor classes:</p> <ul> <li> <p>Beam</p> </li> <li> <p>Satellite</p> </li> </ul>"},{"location":"pyelq/sensor/sensor/#sensor-superclass","title":"Sensor superclass","text":"<p>Sensor module.</p> <p>The superclass for the sensor classes. This module provides the higher level Sensor and SensorGroup classes. The Sensor class is a single sensor, the SensorGroup is a dictionary of Sensors. The SensorGroup class is created to deal with the properties over all sensors together.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor","title":"<code>Sensor</code>  <code>dataclass</code>","text":"<p>Defines the properties and methods of the sensor class.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str</code> <p>String label for sensor</p> <code>time</code> <code>DatetimeArray</code> <p>Array containing time values associated with concentration reading</p> <code>location</code> <code>Coordinate</code> <p>Coordinate object specifying the observation locations</p> <code>concentration</code> <code>ndarray</code> <p>Array containing concentration values associated with time reading</p> <code>source_on</code> <code>ndarray</code> <p>Array of size nof_observations containing boolean values indicating whether a source is on or off for each observation, i.e. we are assuming the sensor can/can't see a source</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>@dataclass\nclass Sensor:\n    \"\"\"Defines the properties and methods of the sensor class.\n\n    Attributes:\n        label (str, optional): String label for sensor\n        time (pandas.arrays.DatetimeArray, optional): Array containing time values associated with concentration\n            reading\n        location (Coordinate, optional): Coordinate object specifying the observation locations\n        concentration (np.ndarray, optional): Array containing concentration values associated with time reading\n        source_on (np.ndarray, optional): Array of size nof_observations containing boolean values indicating\n            whether a source is on or off for each observation, i.e. we are assuming the sensor can/can't see a source\n\n    \"\"\"\n\n    label: str = field(init=False)\n    time: DatetimeArray = field(init=False, default=None)\n    location: Coordinate = field(init=False)\n    concentration: np.ndarray = field(default_factory=lambda: np.array([]))\n    source_on: np.ndarray = field(init=False, default=None)\n\n    @property\n    def nof_observations(self) -&gt; int:\n        \"\"\"Int: Number of observations contained in concentration array.\"\"\"\n        return self.concentration.size\n\n    def plot_sensor_location(self, fig: go.Figure, color=None) -&gt; go.Figure:\n        \"\"\"Plotting the sensor location.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color (`optional`): When specified, the color to be used\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor location trace added to it\n\n        \"\"\"\n        lla_object = self.location.to_lla()\n\n        marker_dict = {\"size\": 10, \"opacity\": 0.8}\n        if color is not None:\n            marker_dict[\"color\"] = color\n\n        fig.add_trace(\n            go.Scattermap(\n                mode=\"markers+lines\",\n                lat=np.array(lla_object.latitude),\n                lon=np.array(lla_object.longitude),\n                marker=marker_dict,\n                line={\"width\": 3},\n                name=self.label,\n            )\n        )\n        return fig\n\n    def plot_timeseries(self, fig: go.Figure, color=None, mode: str = \"markers\") -&gt; go.Figure:\n        \"\"\"Timeseries plot of the sensor concentration observations.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color (`optional`): When specified, the color to be used\n            mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor concentration timeseries trace added to it\n\n        \"\"\"\n        marker_dict = {\"size\": 5, \"opacity\": 1}\n        if color is not None:\n            marker_dict[\"color\"] = color\n\n        fig.add_trace(\n            go.Scatter(\n                x=self.time,\n                y=self.concentration.flatten(),\n                mode=mode,\n                marker=marker_dict,\n                name=self.label,\n                legendgroup=self.label,\n            )\n        )\n\n        return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.plot_sensor_location","title":"<code>plot_sensor_location(fig, color=None)</code>","text":"<p>Plotting the sensor location.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color</code> <code>`optional`</code> <p>When specified, the color to be used</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor location trace added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_sensor_location(self, fig: go.Figure, color=None) -&gt; go.Figure:\n    \"\"\"Plotting the sensor location.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color (`optional`): When specified, the color to be used\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor location trace added to it\n\n    \"\"\"\n    lla_object = self.location.to_lla()\n\n    marker_dict = {\"size\": 10, \"opacity\": 0.8}\n    if color is not None:\n        marker_dict[\"color\"] = color\n\n    fig.add_trace(\n        go.Scattermap(\n            mode=\"markers+lines\",\n            lat=np.array(lla_object.latitude),\n            lon=np.array(lla_object.longitude),\n            marker=marker_dict,\n            line={\"width\": 3},\n            name=self.label,\n        )\n    )\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.Sensor.plot_timeseries","title":"<code>plot_timeseries(fig, color=None, mode='markers')</code>","text":"<p>Timeseries plot of the sensor concentration observations.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color</code> <code>`optional`</code> <p>When specified, the color to be used</p> <code>None</code> <code>mode</code> <code>str</code> <p>Mode used for plotting, i.e. markers, lines or markers+lines</p> <code>'markers'</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor concentration timeseries trace added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_timeseries(self, fig: go.Figure, color=None, mode: str = \"markers\") -&gt; go.Figure:\n    \"\"\"Timeseries plot of the sensor concentration observations.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color (`optional`): When specified, the color to be used\n        mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor concentration timeseries trace added to it\n\n    \"\"\"\n    marker_dict = {\"size\": 5, \"opacity\": 1}\n    if color is not None:\n        marker_dict[\"color\"] = color\n\n    fig.add_trace(\n        go.Scatter(\n            x=self.time,\n            y=self.concentration.flatten(),\n            mode=mode,\n            marker=marker_dict,\n            name=self.label,\n            legendgroup=self.label,\n        )\n    )\n\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup","title":"<code>SensorGroup</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict</code></p> <p>A dictionary containing multiple Sensors.</p> <p>This class is used when we want to combine a collection of sensors and be able to store/access overall properties.</p> <p>Attributes:</p> Name Type Description <code>color_map</code> <code>list</code> <p>Default colormap to use for plotting</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>@dataclass\nclass SensorGroup(dict):\n    \"\"\"A dictionary containing multiple Sensors.\n\n    This class is used when we want to combine a collection of sensors and be able to store/access overall properties.\n\n    Attributes:\n        color_map (list, optional): Default colormap to use for plotting\n\n    \"\"\"\n\n    color_map: list = field(default_factory=list, init=False)\n\n    def __post_init__(self):\n        self.color_map = px.colors.qualitative.Pastel\n\n    @property\n    def nof_observations(self) -&gt; int:\n        \"\"\"Int: The total number of observations across all the sensors.\"\"\"\n        return int(np.sum([sensor.nof_observations for sensor in self.values()], axis=None))\n\n    @property\n    def concentration(self) -&gt; np.ndarray:\n        \"\"\"np.ndarray: Column vector of concentration values across all sensors, unwrapped per sensor.\"\"\"\n        return np.concatenate([sensor.concentration.flatten() for sensor in self.values()], axis=0)\n\n    @property\n    def time(self) -&gt; pd.arrays.DatetimeArray:\n        \"\"\"DatetimeArray: Column vector of time values across all sensors.\"\"\"\n        return pd.array(np.concatenate([sensor.time for sensor in self.values()]), dtype=\"datetime64[ns]\")\n\n    @property\n    def location(self) -&gt; Coordinate:\n        \"\"\"Coordinate: Coordinate object containing observation locations from all sensors in the group.\"\"\"\n        location_object = deepcopy(list(self.values())[0].location)\n        if isinstance(location_object, ENU):\n            attr_list = [\"east\", \"north\", \"up\"]\n        elif isinstance(location_object, LLA):\n            attr_list = [\"latitude\", \"longitude\", \"altitude\"]\n        elif isinstance(location_object, ECEF):\n            attr_list = [\"x\", \"y\", \"z\"]\n        else:\n            raise TypeError(\n                f\"Location object should be either ENU, LLA or ECEF, while currently it is{type(location_object)}\"\n            )\n        for attr in attr_list:\n            setattr(\n                location_object,\n                attr,\n                np.concatenate([np.array(getattr(sensor.location, attr), ndmin=1) for sensor in self.values()], axis=0),\n            )\n        return location_object\n\n    @property\n    def sensor_index(self) -&gt; np.ndarray:\n        \"\"\"np.ndarray: Column vector of integer indices linking concentration observation to a particular sensor.\"\"\"\n        return np.concatenate(\n            [np.ones(sensor.nof_observations, dtype=int) * i for i, sensor in enumerate(self.values())]\n        )\n\n    @property\n    def source_on(self) -&gt; np.ndarray:\n        \"\"\"Column vector of booleans indicating whether sources are expected to be on, unwrapped over sensors.\n\n        Assumes source is on when None is specified for a specific sensor.\n\n        Returns:\n            np.ndarray: Source on attribute, unwrapped over sensors.\n\n        \"\"\"\n        overall_idx = np.array([])\n        for curr_key in list(self.keys()):\n            if self[curr_key].source_on is None:\n                temp_idx = np.ones(self[curr_key].nof_observations).astype(bool)\n            else:\n                temp_idx = self[curr_key].source_on\n\n            overall_idx = np.concatenate([overall_idx, temp_idx])\n        return overall_idx.astype(bool)\n\n    @property\n    def nof_sensors(self) -&gt; int:\n        \"\"\"Int: Number of sensors contained in the SensorGroup.\"\"\"\n        return len(self)\n\n    def add_sensor(self, sensor: Sensor):\n        \"\"\"Add a sensor to the SensorGroup.\"\"\"\n        self[sensor.label] = sensor\n\n    def plot_sensor_location(self, fig: go.Figure, color_map: list = None) -&gt; go.Figure:\n        \"\"\"Plotting of the locations of all sensors in the SensorGroup.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n                the colors\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor location traces added to it\n\n        \"\"\"\n        if color_map is None:\n            color_map = self.color_map\n\n        for i, sensor in enumerate(self.values()):\n            color_idx = i % len(color_map)\n            fig = sensor.plot_sensor_location(fig, color=color_map[color_idx])\n\n        return fig\n\n    def plot_timeseries(self, fig: go.Figure, color_map: list = None, mode: str = \"markers\") -&gt; go.Figure:\n        \"\"\"Plotting of the concentration timeseries of all sensors in the SensorGroup.\n\n        Args:\n            fig (go.Figure): Plotly figure object to add the trace to\n            color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n                the colors\n            mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n        Returns:\n            fig (go.Figure): Plotly figure object with sensor concentration time series traces added to it\n\n        \"\"\"\n        if color_map is None:\n            color_map = self.color_map\n\n        for i, sensor in enumerate(self.values()):\n            color_idx = i % len(color_map)\n            fig = sensor.plot_timeseries(fig, color=color_map[color_idx], mode=mode)\n\n        return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.nof_observations","title":"<code>nof_observations</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.concentration","title":"<code>concentration</code>  <code>property</code>","text":"<p>np.ndarray: Column vector of concentration values across all sensors, unwrapped per sensor.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.time","title":"<code>time</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.location","title":"<code>location</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.sensor_index","title":"<code>sensor_index</code>  <code>property</code>","text":"<p>np.ndarray: Column vector of integer indices linking concentration observation to a particular sensor.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.source_on","title":"<code>source_on</code>  <code>property</code>","text":"<p>Column vector of booleans indicating whether sources are expected to be on, unwrapped over sensors.</p> <p>Assumes source is on when None is specified for a specific sensor.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Source on attribute, unwrapped over sensors.</p>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.nof_sensors","title":"<code>nof_sensors</code>  <code>property</code>","text":""},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.add_sensor","title":"<code>add_sensor(sensor)</code>","text":"<p>Add a sensor to the SensorGroup.</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def add_sensor(self, sensor: Sensor):\n    \"\"\"Add a sensor to the SensorGroup.\"\"\"\n    self[sensor.label] = sensor\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.plot_sensor_location","title":"<code>plot_sensor_location(fig, color_map=None)</code>","text":"<p>Plotting of the locations of all sensors in the SensorGroup.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color_map</code> <code>list</code> <p>When specified, the colormap to be used, plotting will cycle through the colors</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor location traces added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_sensor_location(self, fig: go.Figure, color_map: list = None) -&gt; go.Figure:\n    \"\"\"Plotting of the locations of all sensors in the SensorGroup.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n            the colors\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor location traces added to it\n\n    \"\"\"\n    if color_map is None:\n        color_map = self.color_map\n\n    for i, sensor in enumerate(self.values()):\n        color_idx = i % len(color_map)\n        fig = sensor.plot_sensor_location(fig, color=color_map[color_idx])\n\n    return fig\n</code></pre>"},{"location":"pyelq/sensor/sensor/#pyelq.sensor.sensor.SensorGroup.plot_timeseries","title":"<code>plot_timeseries(fig, color_map=None, mode='markers')</code>","text":"<p>Plotting of the concentration timeseries of all sensors in the SensorGroup.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to add the trace to</p> required <code>color_map</code> <code>list</code> <p>When specified, the colormap to be used, plotting will cycle through the colors</p> <code>None</code> <code>mode</code> <code>str</code> <p>Mode used for plotting, i.e. markers, lines or markers+lines</p> <code>'markers'</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Plotly figure object with sensor concentration time series traces added to it</p> Source code in <code>src/pyelq/sensor/sensor.py</code> <pre><code>def plot_timeseries(self, fig: go.Figure, color_map: list = None, mode: str = \"markers\") -&gt; go.Figure:\n    \"\"\"Plotting of the concentration timeseries of all sensors in the SensorGroup.\n\n    Args:\n        fig (go.Figure): Plotly figure object to add the trace to\n        color_map (list, optional): When specified, the colormap to be used, plotting will cycle through\n            the colors\n        mode (str, optional): Mode used for plotting, i.e. markers, lines or markers+lines\n\n    Returns:\n        fig (go.Figure): Plotly figure object with sensor concentration time series traces added to it\n\n    \"\"\"\n    if color_map is None:\n        color_map = self.color_map\n\n    for i, sensor in enumerate(self.values()):\n        color_idx = i % len(color_map)\n        fig = sensor.plot_timeseries(fig, color=color_map[color_idx], mode=mode)\n\n    return fig\n</code></pre>"},{"location":"pyelq/support_functions/spatio_temporal_interpolation/","title":"Support Functions","text":""},{"location":"pyelq/support_functions/spatio_temporal_interpolation/#spatio-temporal-interpolation","title":"Spatio-Temporal Interpolation","text":"<p>Spatio-temporal interpolation module.</p> <p>Support function to perform interpolation in various ways</p>"},{"location":"pyelq/support_functions/spatio_temporal_interpolation/#pyelq.support_functions.spatio_temporal_interpolation.interpolate","title":"<code>interpolate(location_in=None, time_in=None, values_in=None, location_out=None, time_out=None, **kwargs)</code>","text":"<p>Interpolates data based on input.</p> <p>Interpolation using scipy.griddata function. Which in turn uses linear barycentric interpolation.</p> <p>It is assumed that the shape of location_in, time_in and values_in is consistent</p> <p>When time_out has the same size as number of rows of location_out, it is assumed these are aligned and be treated as consistent, hence the output will be a column vector. If this is not the case an interpolation will be performed for all combinations of rows in location out with times of time_out and output wil be shaped as [nof_location_values x dimension]</p> <p>If location_out == None, we only perform temporal (1D) interpolation. If time_out == None we only perform spatial  interpolation</p> <p>If linear interpolation is not possible for spatio or spatiotemporal interpolation, we use nearest neighbor interpolation, a warning will be displayed</p> <p>Parameters:</p> Name Type Description Default <code>location_in</code> <code>ndarray</code> <p>Array of size [nof_values x dimension] with locations to interpolate from</p> <code>None</code> <code>time_in</code> <code>Union[ndarray, DatetimeArray]</code> <p>Array of size [nof_values x 1] with timestamps or some form of time values (seconds) to interpolate from</p> <code>None</code> <code>values_in</code> <code>ndarray</code> <p>Array of size [nof_values x 1] with values to interpolate from</p> <code>None</code> <code>location_out</code> <code>ndarray</code> <p>Array of size [nof_location_values x dimension] with locations to interpolate to</p> <code>None</code> <code>time_out</code> <code>Union[ndarray, DatetimeArray]</code> <p>Array of size [nof_time_values x 1] with timestamps or some form of time values (seconds) to interpolate to</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Other keyword arguments which get passed into the griddata interpolation function</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>result</code> <code>ndarray</code> <p>Array of size [nof_location_values x nof_time_values] with interpolated values</p> Source code in <code>src/pyelq/support_functions/spatio_temporal_interpolation.py</code> <pre><code>def interpolate(\n    location_in: np.ndarray = None,\n    time_in: Union[np.ndarray, pd.arrays.DatetimeArray] = None,\n    values_in: np.ndarray = None,\n    location_out: np.ndarray = None,\n    time_out: Union[np.ndarray, pd.arrays.DatetimeArray] = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Interpolates data based on input.\n\n    Interpolation using scipy.griddata function. Which in turn uses linear barycentric interpolation.\n\n    It is assumed that the shape of location_in, time_in and values_in is consistent\n\n    When time_out has the same size as number of rows of location_out, it is assumed these are aligned and be treated as\n    consistent, hence the output will be a column vector.\n    If this is not the case an interpolation will be performed for all combinations of rows in location out with times\n    of time_out and output wil be shaped as [nof_location_values x dimension]\n\n    If location_out == None, we only perform temporal (1D) interpolation.\n    If time_out == None we only perform spatial  interpolation\n\n    If linear interpolation is not possible for spatio or spatiotemporal interpolation, we use nearest neighbor\n    interpolation, a warning will be displayed\n\n    Args:\n        location_in (np.ndarray): Array of size [nof_values x dimension] with locations to interpolate from\n        time_in (Union[np.ndarray, pd.arrays.DatetimeArray]): Array of size [nof_values x 1] with timestamps or some\n            form of time values (seconds) to interpolate from\n        values_in (np.ndarray): Array of size [nof_values x 1] with values to interpolate from\n        location_out (np.ndarray): Array of size [nof_location_values x dimension] with locations to interpolate to\n        time_out (Union[np.ndarray, pd.arrays.DatetimeArray]): Array of size [nof_time_values x 1] with\n            timestamps or some form of time values (seconds) to interpolate to\n        **kwargs (dict): Other keyword arguments which get passed into the griddata interpolation function\n\n    Returns:\n        result (np.ndarray): Array of size [nof_location_values x nof_time_values] with interpolated values\n\n    \"\"\"\n    _sense_check_interpolate_inputs(\n        location_in=location_in, time_in=time_in, values_in=values_in, location_out=location_out, time_out=time_out\n    )\n\n    if (\n        time_out is not None\n        and isinstance(time_out, pd.arrays.DatetimeArray)\n        and isinstance(time_in, pd.arrays.DatetimeArray)\n    ):\n        min_time_out = np.amin(time_out)\n        time_out = (time_out - min_time_out).total_seconds()\n        time_in = (time_in - min_time_out).total_seconds()\n\n    if location_out is None:\n        return _griddata(points_in=time_in, values=values_in, points_out=time_out, **kwargs)\n\n    if time_out is None:\n        return _griddata(points_in=location_in, values=values_in, points_out=location_out, **kwargs)\n\n    if location_in.shape[0] != time_in.size:\n        raise ValueError(\"Location and time are do not have consistent sizes\")\n\n    if location_out.shape[0] != time_out.size:\n        location_temp = np.tile(location_out, (time_out.size, 1))\n        time_temp = np.repeat(time_out.squeeze(), location_out.shape[0])\n        out_array = np.column_stack((location_temp, time_temp))\n    else:\n        out_array = np.column_stack((location_out, time_out))\n\n    in_array = np.column_stack((location_in, time_in))\n\n    result = _griddata(points_in=in_array, values=values_in, points_out=out_array, **kwargs)\n\n    if location_out.shape[0] != time_out.size:\n        result = result.reshape((location_out.shape[0], time_out.size), order=\"C\")\n\n    return result\n</code></pre>"},{"location":"pyelq/support_functions/spatio_temporal_interpolation/#pyelq.support_functions.spatio_temporal_interpolation.temporal_resampling","title":"<code>temporal_resampling(time_in, values_in, time_bin_edges, aggregate_function='mean', side='center')</code>","text":"<p>Resamples data into a set of time bins.</p> <p>Checks which values of time_in are withing 2 consecutive values of time_bin_edges and performs the aggregate function on the corresponding values from values_in. time_in values outside the time_bin_edges are ignored. Empty bins will be assigned a 'NaN' value.</p> <p>When 'time_in' is a sequence of time stamps, a DatetimeArray should be used. Otherwise, a np.ndarray should be used.</p> <p>Parameters:</p> Name Type Description Default <code>time_in</code> <code>Union[ndarray, DatetimeArray]</code> <p>A vector of times which correspond to values_in.</p> required <code>values_in</code> <code>ndarray</code> <p>A vector of the values to be resampled.</p> required <code>time_bin_edges</code> <code>Union[ndarray, DatetimeArray]</code> <p>A vector of times which define the edges of the                                                          bins into which the data will be resampled.</p> required <code>aggregate_function</code> <code>str</code> <p>The function which is used to aggregate the data after it has been                                 sorted into bins. Defaults to mean.</p> <code>'mean'</code> <code>side</code> <code>str</code> <p>Which side of the time bins should be used to generate times_out. Possible values are:                   'left', 'center', and 'right'. Defaults to 'center'.</p> <code>'center'</code> <p>Returns:</p> Name Type Description <code>time_out</code> <code>Union[ndarray, DatetimeArray]</code> <p>Vector-like object containing the times of the resampled                                                    values consistent with time_in dtype and side input                                                    argument.</p> <code>values_out</code> <code>ndarray</code> <p>A vector of resampled values, according to the time bins and the aggregate function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input arguments are not of the correct type or shape, this error is raised.</p> Source code in <code>src/pyelq/support_functions/spatio_temporal_interpolation.py</code> <pre><code>def temporal_resampling(\n    time_in: Union[np.ndarray, pd.arrays.DatetimeArray],\n    values_in: np.ndarray,\n    time_bin_edges: Union[np.ndarray, pd.arrays.DatetimeArray],\n    aggregate_function: str = \"mean\",\n    side: str = \"center\",\n) -&gt; Tuple[Union[np.ndarray, pd.arrays.DatetimeArray], np.ndarray]:\n    \"\"\"Resamples data into a set of time bins.\n\n    Checks which values of time_in are withing 2 consecutive values of time_bin_edges and performs the aggregate\n    function on the corresponding values from values_in. time_in values outside the time_bin_edges are ignored.\n    Empty bins will be assigned a 'NaN' value.\n\n    When 'time_in' is a sequence of time stamps, a DatetimeArray should be used. Otherwise, a np.ndarray should be used.\n\n    Args:\n        time_in (Union[np.ndarray, pd.arrays.DatetimeArray]): A vector of times which correspond to values_in.\n        values_in (np.ndarray): A vector of the values to be resampled.\n        time_bin_edges (Union[np.ndarray, pd.arrays.DatetimeArray]): A vector of times which define the edges of the\n                                                                     bins into which the data will be resampled.\n        aggregate_function (str, optional): The function which is used to aggregate the data after it has been\n                                            sorted into bins. Defaults to mean.\n        side (str, optional): Which side of the time bins should be used to generate times_out. Possible values are:\n                              'left', 'center', and 'right'. Defaults to 'center'.\n\n    Returns:\n        time_out (Union[np.ndarray, pd.arrays.DatetimeArray]): Vector-like object containing the times of the resampled\n                                                               values consistent with time_in dtype and side input\n                                                               argument.\n        values_out (np.ndarray): A vector of resampled values, according to the time bins and the aggregate function.\n\n    Raises:\n        ValueError: If any of the input arguments are not of the correct type or shape, this error is raised.\n\n    \"\"\"\n    if not isinstance(time_bin_edges, type(time_in)) or values_in.size != time_in.size:\n        raise ValueError(\"Arguments 'time_in', 'time_bin_edges' and/or 'values_in' are not of consistent type or size.\")\n\n    if not isinstance(aggregate_function, str):\n        raise ValueError(\"The supplied 'aggregate_function' is not a string.\")\n\n    if side == \"center\":\n        time_out = np.diff(time_bin_edges) / 2 + time_bin_edges[:-1]\n    elif side == \"left\":\n        time_out = time_bin_edges[:-1]\n    elif side == \"right\":\n        time_out = time_bin_edges[1:]\n    else:\n        raise ValueError(f\"The 'side' argument must be 'left', 'center', or 'right', but received '{side}'.\")\n\n    zero_value = 0\n    if isinstance(time_bin_edges, pd.arrays.DatetimeArray):\n        zero_value = np.array(0).astype(\"&lt;m8[ns]\")\n\n    if not np.all(np.diff(time_bin_edges) &gt; zero_value):\n        raise ValueError(\"Argument 'time_bin_edges' does not monotonically increase.\")\n\n    if np.any(time_in &lt; time_bin_edges[0]) or np.any(time_in &gt; time_bin_edges[-1]):\n        warnings.warn(\"Values in time_in are outside of range of time_bin_edges. These values will be ignored.\")\n\n    index = np.searchsorted(time_bin_edges, time_in, side=\"left\")\n    grouped_vals = pd.Series(values_in).groupby(index).agg(aggregate_function)\n    grouped_vals = grouped_vals.drop(index=[0, time_bin_edges.size], errors=\"ignore\").sort_index()\n\n    values_out = np.full(time_out.shape, np.nan)\n    values_out[grouped_vals.index - 1] = grouped_vals.to_numpy()\n\n    return time_out, values_out\n</code></pre>"}]}